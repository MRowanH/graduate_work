{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOnEvXlcqaMGvpH3TMjyQe9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prompt"],"metadata":{"id":"fNIou4VAMF0Q"}},{"cell_type":"markdown","source":["## Code Instructions"],"metadata":{"id":"AdgnbqhUMPFK"}},{"cell_type":"markdown","source":["Please click here and follow the article. (https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70)\n","\n","This article is for text summarization using python.\n","\n","Please implement their code first. Then, try to apply their code to your data (5 long or short articles). You might notice that their code did not have the text cleaning except the stop-words. Please refer to the text cleaning methods used in ICE-1 and add appropriate text cleaning methods to the text summarization code. Then, apply the modified code to your data again.\n"],"metadata":{"id":"7hLIZa79MHW1"}},{"cell_type":"markdown","source":["## Questions - Answered at the end of the notebook."],"metadata":{"id":"7WQVVjE9MSKN"}},{"cell_type":"markdown","source":["1. What are the two main strategies used in text summarization?\n","\n","2. Which feature is used in the text summarization code? Explain how to calculate it.\n","\n","3. What is the similarity measurement method used in this code?\n","\n","4. We know in ICE-1, TF-IDF is used as the text feature. Can we use it in this code? \n","\n","5. Compare the outputs above. Are they the same or not? Please analyze the comparison result."],"metadata":{"id":"M80vqFjnMU9U"}},{"cell_type":"markdown","source":["# Imports, Installs, and Downloads"],"metadata":{"id":"_vqcGrK9TZw-"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"0DhUEzhdklCj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"efQVwqNmGxXC","executionInfo":{"status":"ok","timestamp":1663206784256,"user_tz":300,"elapsed":1360,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","import numpy as np\n","import networkx as nx\n","import re\n","import requests\n","from bs4 import BeautifulSoup"]},{"cell_type":"markdown","source":["## Downloads"],"metadata":{"id":"QP4nXRANkmnR"}},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_DrN-L8IU22","executionInfo":{"status":"ok","timestamp":1663206784446,"user_tz":300,"elapsed":194,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"03d4fa18-74c8-40db-8066-f1d6cb07ff0f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lyJa2kMknx0","executionInfo":{"status":"ok","timestamp":1663206784447,"user_tz":300,"elapsed":9,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"3890e80f-aae8-4d2b-e69f-247a6056d6b9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdEfdOSwkrSy","executionInfo":{"status":"ok","timestamp":1663206784447,"user_tz":300,"elapsed":7,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"c1948d6e-05a4-4a85-91d3-5946cfd01e25"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g23pc5_JlqSn","executionInfo":{"status":"ok","timestamp":1663206784666,"user_tz":300,"elapsed":223,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"c4cd68ec-f1be-47b7-e10d-022535ad46a9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Get Data"],"metadata":{"id":"LE3vFDd4LcsG"}},{"cell_type":"markdown","source":["## Load Text"],"metadata":{"id":"Tp3gQ2r5YuEE"}},{"cell_type":"markdown","source":["Note: All articles are from medium.com and towardsdatascience.com, but the text had been manually put into variables so that loading files isn't necessary. I would have scraped directly from webpages but most sits now protect against that."],"metadata":{"id":"MAwHosbuXYdU"}},{"cell_type":"code","source":["data = {}"],"metadata":{"id":"0kGKBiIoWy0N","executionInfo":{"status":"ok","timestamp":1663206784667,"user_tz":300,"elapsed":6,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### A1 - GPT-what? A non-technical guide to OpenAI’s groundbreaking new NLP model"],"metadata":{"id":"5YcOB73CZkfE"}},{"cell_type":"code","source":["a1 = \"\"\"Hype over GPT-3 reached an all-time high on Twitter over the weekend and many are calling the technological development a groundbreaking inflection point for future AI research. In this article I explore what GPT is, what it means for AI development, and where we might be headed from here.\n","OpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools. Those with early API access through OpenAI’s beta program went to Twitter to showcase impressive early tools built using GPT-3 technology:\n","\n","\n","\n","\n","For non-engineers, this may look like magic, but there is a lot to be unpacked here. In this article I will provide a brief overview of GPT and what it can be used for.\n","\n","What is OpenAI and GPT-3?\n","\n","OpenAI is an AI research laboratory founded in 2015 by Elon Musk, Sam Altman, and others with the mission of creating AI that benefits all of humanity. The company recently received $1 billion of additional funding from Microsoft in 2019 and is considered a leader in AI research and development.\n","\n","Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). Normally, this can be extremely time consuming and expensive. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task.\n","\n","For example, suppose you would like to learn a new language — German. Initially, you will still think about your sentences in English, then translate and rearrange words to come up with the German equivalent. The reality is, you are still indirectly applying learnings about sentence structure, language, and communication from the previous language even though the actual words and grammar are different. This is why learning new languages is typically easier if you already know another language.\n","\n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. In this great walkthrough, Francois Chollet compared the effectiveness of an AI model trained from scratch to one built from a pre-trained model. His results showed that the latter had 15% greater predictive accuracy after training both with the same amount of training data.\n","\n","In 2018, OpenAI presented convincing research showing that this strategy (pairing supervised learning with unsupervised pre-training) is particularly very effective in NLP tasks. They first produced a generative pre-trained model (“GPT”) using “a diverse corpus of unlabeled text” (i.e. over 7,000 unique unpublished books from a variety of genres), essentially creating a model that “understood” English and language. Next, this pre-trained model could be further fine-tuned and trained to perform specific tasks using supervised learning. As an analogy, this would be like teaching someone English, then training him or her for the specific task of reading and classifying resumes of acceptable and unacceptable candidates for hiring.\n","\n","GPT-3 is the latest iteration of the GPT model and was first described in May 2020. It contains 175 billion parameters compared to the 1.5 billion in GPT-2 (117x increase) and training it consumed several thousand petaflop/s-days of computing power. GPT-3 is fed with much more data and tuned with more parameters than GPT-2, and as a result, it has produced some amazing NLP capabilities so far. The volume of data and computing resources required makes it impossible for many organizations to recreate this, but luckily they won’t have to since OpenAI plans to release access via API in the future.\n","\n","Critical reception\n","\n","Admittedly, GPT-3 didn’t get much attention until last week’s viral tweets by Sharif Shameem and others (above). They demonstrated that GPT-3 could be used to create websites based on plain English instructions, envisioning a new era of no-code technologies where people can create apps by simply describing them in words. Early adopter Kevin Lacker tested the model with a Turing test and saw amazing results. GPT-3 performed exceptionally well in the initial Q&A and displayed many aspects of “common sense” that AI systems traditionally struggle with.\n","\n","However, the model is far from perfect. Max Woolf performed a critical analysis noting several issues such as model latency, implementation issues, and concerning biases in the data that need to be re-considered. Several users have reported these issues on Twitter as well:\n","\n","\n","\n","OpenAI’s blog discusses some of the key drawbacks of the model, most notably that GPT’s entire understanding of the world is based on the texts it was trained on. Case in point: it was trained in October 2019 and therefore does not know about COVID-19. It is unclear how these texts were chosen and what oversight was performed (or required) in this process.\n","\n","Additionally, the enormous computing resources required to produce and maintain these models raise serious questions about the environmental impact of AI technologies. Although often overlooked, both hardware and software usage significantly contribute to depletion of energy resources, excessive waste generation, and excessive mining of rare earth minerals with the associated negative impacts to human health.\n","\n","To quell concerns, OpenAI has repeatedly stated its mission to produce AI for the good of humanity and aims to stop access to its API if misuse is detected. Even in it’s beta access form, it asks candidates to describe their intentions with the technology and the benefits and risks to society.\n","\n","\n","Where do we go from here?\n","\n","Without a doubt, GPT-3 still represents a major milestone in AI development. Many early users have built impressive apps that accurately process natural language and produce amazing results. In summary:\n","\n","GPT-3 is a major improvement upon GPT-2 and features far greater accuracy for better use cases. This is a significant step forward for AI development, impressively accomplished in just a two-year time frame\n","Early tools that have been built on GPT-3 show great promise for commercial usability such as: no-code platforms that allow you to build apps by describing then; advanced search platforms using plain English; and better data analytics tools that make data gathering and processing much faster\n","OpenAI announced plans to release a commercial API, which will enable organizations to build products powered by GPT-3 at scale. However, many questions remain about how exactly this will be executed — pricing, SLA, model latency, etc.\n","Users have pointed out several issues that need to be addressed before widespread commercial use. Inherent biases in the model, questions around fairness and ethics, and concerns about misuse (fake news, bots, etc.) need to be thought through and oversight might be necessary\n","OpenAI is openly committed to creating AI for the benefit of humanity, but still, monitoring for misuse at scale will be difficult to achieve. This raises a broader question about the necessity of government involvement to protect the rights of individuals\n","All said, I’m extremely excited to see which new technologies are built on GPT-3 and how OpenAI continues to improve on its model. Increased attention and funding in NLP and GPT-3 might be enough to ward off fears from many critics that an AI winter might be coming (myself included). Despite the shortfalls of the model, I am hoping that everyone can be optimistic about a future where humans and machines will communicate with each other in a unified language and the ability to create tools using technology will be accessible to billions of more people.\n","\"\"\"\n","data.update({\"a1\": \"https://towardsdatascience.com/gpt-what-why-this-groundbreaking-model-is-driving-the-future-of-ai-and-nlp-e38fcf891172\"})"],"metadata":{"id":"XGH0XFsiWrY9","executionInfo":{"status":"ok","timestamp":1663206784667,"user_tz":300,"elapsed":5,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### A2 - How Transformers Work"],"metadata":{"id":"4BKMrNDIZqA4"}},{"cell_type":"code","source":["a2 = \"\"\"If you liked this post and want to learn how machine learning algorithms work, how did they arise, and where are they going, I recommend the following:\n","\n","Making Things Think: How AI and Deep Learning Power the Products We Use - Holloway\n","It is the obvious which is so difficult to see most of the time. People say 'It's as plain as the nose on your face.'…\n","www.holloway.com\n","\n","Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar — their program to defeat a top professional Starcraft player.\n","\n","Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..\n","\n","\n","Sequence transduction. The input is represented in green, the model is represented in blue, and the output is represented in purple. GIF from 3\n","For models to perform sequence transduction, it is necessary to have some sort of memory. For example let’s say that we are translating the following sentence to another language (French):\n","\n","“The Transformers” are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history”\n","\n","In this example, the word “the band” in the second sentence refers to the band “The Transformers” introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the “The Transformers” band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences.\n","\n","For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let’s go over these two architectures and their drawbacks.\n","\n","Recurrent Neural Networks\n","Recurrent Neural Networks have loops in them, allowing information to persist.\n","\n","\n","The input is represented as x_t\n","In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next.\n","\n","The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop:\n","\n","\n","An unrolled recurrent neural network\n","This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information.\n","\n","The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.\n","\n","\n","GIF from 3\n","The problem of long-term dependencies\n","Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence “the clouds in the sky”, we don’t need further context. It’s pretty obvious that the next word is going to be sky.\n","\n","In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence.\n","\n","\n","Image from 6\n","But there are cases where we need more context. For example, let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text.\n","\n","\n","Image from 6\n","RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.\n","\n","In theory, RNNs could learn this long-term dependencies. In practice, they don’t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.\n","\n","Long-Short Term Memory (LSTM)\n","When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.\n","\n","RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.\n","\n","LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.\n","\n","Internally, a LSTM looks like the following:\n","\n","\n","Image from 6\n","Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won’t go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher’s blog post:\n","\n","Understanding LSTM Networks -- colah's blog\n","These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that…\n","colah.github.io\n","\n","With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating.\n","\n","The problem with LSTMs\n","The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don’t do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.\n","\n","That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems:\n","\n","Sequential computation inhibits parallelization\n","No explicit modeling of long and short range dependencies\n","“Distance” between positions is linear\n","Attention\n","To solve some of these problems, researchers created a technique for paying attention to specific words.\n","\n","When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so.\n","\n","Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.\n","\n","To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.\n","\n","\n","The green step is called the encoding stage and the purple step is the decoding stage. GIF from 3\n","The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.\n","\n","For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.\n","\n","\n","GIF from 3\n","The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every “word” of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to.\n","\n","For example, when translating the sentence “Je suis étudiant” to English, requires that the decoding step looks at different words when translating it.\n","\n","\n","This gif shows how the weight that is given to each hidden state when translating the sentence “Je suis étudiant” to English. The darker the color is, the more weight is associated to each word. GIF from 3\n","Or for example, when you translate the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” from French to English, and how much attention it is paid to each input.\n","\n","\n","Translating the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” to English. Image from 3\n","But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.\n","\n","Convolutional Neural Networks\n","Convolutional Neural Networks help solve these problems. With them we can\n","\n","Trivial to parallelize (per layer)\n","Exploits local dependencies\n","Distance between positions is logarithmic\n","Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks.\n","\n","\n","Wavenet, model is a Convolutional Neural Network (CNN). Image from 10\n","The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the “distance” between the output word and any input for a CNN is in the order of log(N) — that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N.\n","\n","The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why Transformers were created, they are a combination of both CNNs with attention.\n","\n","Transformers\n","To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.\n","\n","Let’s take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention.\n","\n","\n","The Transformer. Image from 4\n","Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.\n","\n","\n","Image from 4\n","Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network.\n","\n","\n","Image from 4\n","The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.\n","\n","\n","Image from 4\n","Self-Attention\n","Note: This section comes from Jay Allamar blog post\n","\n","Let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.\n","\n","\n","Image taken from 4\n","Each word is embedded into a vector of size 512. We’ll represent those vectors with these simple boxes.\n","\n","The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512.\n","\n","In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.\n","\n","\n","Image from 4\n","Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\n","\n","Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.\n","\n","Self-Attention\n","Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented — using matrices.\n","\n","\n","Figuring out relation of words within a sentence and giving the right attention to it. Image from 8\n","The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n","\n","Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\n","\n","\n","Image taken from 4\n","Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.\n","\n","What are the “query”, “key”, and “value” vectors?\n","\n","They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\n","\n","The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n","\n","The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n","\n","\n","Image from 4\n","The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\n","\n","\n","Image from 4\n","This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\n","\n","The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n","\n","The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n","\n","\n","Image from 4\n","That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\n","\n","Multihead attention\n","Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention.\n","\n","The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”, etc…\n","\n","\n","\n","\n","Images from 8\n","Positional Encoding\n","Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.\n","\n","Overview\n","I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique\n","\n","The Unreasonable Effectiveness of Recurrent Neural Networks\n","Understanding LSTM Networks\n","Visualizing A Neural Machine Translation Model\n","The Illustrated Transformer\n","The Transformer — Attention is all you need\n","The Annotated Transformer\n","Attention is all you need attentional neural network models\n","Self-Attention For Generative Models\n","OpenAI GPT-2: Understanding Language Generation through Visualization\n","WaveNet: A Generative Model for Raw Audio\n","\"\"\"\n","data.update({\"a2\": \"https://towardsdatascience.com/transformers-141e32e69591\"})"],"metadata":{"id":"He3atxojW1mn","executionInfo":{"status":"ok","timestamp":1663206784668,"user_tz":300,"elapsed":6,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### A3 - Illustrated Guide to Transformers- Step by Step Explanation"],"metadata":{"id":"gSuAjDMJZwsO"}},{"cell_type":"code","source":["a3 = \"\"\"Transformers are taking the natural language processing world by storm. These incredible models are breaking multiple NLP records and pushing the state of the art. They are used in many applications like machine language translation, conversational chatbots, and even to power better search engines. Transformers are the rage in deep learning nowadays, but how do they work? Why have they outperform the previous king of sequence problems, like recurrent neural networks, GRU’s, and LSTM’s? You’ve probably heard of different famous transformers models like BERT, GPT, and GPT2. In this post, we’ll focus on the one paper that started it all, “Attention is all you need”.\n","\n","Check out the link below if you’d like to watch the video version instead.\n","\n","\n","Attention Mechanism\n","To understand transformers we first must understand the attention mechanism. The Attention mechanism enables the transformers to have extremely long term memory. A transformer model can “attend” or “focus” on all previous tokens that have been generated.\n","\n","Let’s walk through an example. Say we want to write a short sci-fi novel with a generative transformer. Using Hugging Face’s Write With Transformer application, we can do just that. We’ll prime the model with our input, and the model will generate the rest.\n","\n","\n","Our input: “As Aliens entered our planet”.\n","\n","Transformer output: “and began to colonized Earth, a certain group of extraterrestrials began to manipulate our society through their influences of a certain number of the elite to keep and iron grip over the populace.”\n","\n","Ok, so the story is a little dark but what’s interesting is how the model generated it. As the model generates the text word by word, it can “attend” or “focus” on words that are relevant to the generated word. The ability to know what words to attend too is all learned during training through backpropagation.\n","\n","\n","Attention mechanism focusing on different tokens while generating words 1 by 1\n","Recurrent neural networks (RNN) are also capable of looking at previous inputs too. But the power of the attention mechanism is that it doesn’t suffer from short term memory. RNN’s have a shorter window to reference from, so when the story gets longer, RNN’s can’t access words generated earlier in the sequence. This is still true for Gated Recurrent Units (GRU’s) and Long-short Term Memory (LSTM’s) networks, although they do a bigger capacity to achieve longer-term memory, therefore, having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an infinite window to reference from, therefore being capable of using the entire context of the story while generating the text.\n","\n","\n","Hypothetical reference window of Attention, RNN’s, GRU’s & LSTM’s\n","Attention Is All You Need — Step by Step Walkthrough\n","The attention mechanism’s power was demonstrated in the paper “Attention Is All You Need”, where the authors introduced a new novel neural network called the Transformers which is an attention-based encoder-decoder type architecture.\n","\n","\n","Transformer Model\n","On a high level, the encoder maps an input sequence into an abstract continuous representation that holds all the learned information of that input. The decoder then takes that continuous representation and step by step generates a single output while also being fed the previous output.\n","\n","Let’s walk through an example. The paper applied the Transformer model on a neural machine translation problem. In this post, we’ll demonstrate how it’ll work for a conversational chatbot.\n","\n","Our Input: “Hi how are you”\n","\n","Transformer Output: “I am fine”\n","\n","Input Embeddings\n","The first step is feeding out input into a word embedding layer. A word embedding layer can be thought of as a lookup table to grab a learned vector representation of each word. Neural networks learn through numbers so each word maps to a vector with continuous values to represent that word.\n","\n","\n","converting Words to Input Embeddings\n","Positional Encoding\n","The next step is to inject positional information into the embeddings. Because the transformer encoder has no recurrence like recurrent neural networks, we must add some information about the positions into the input embeddings. This is done using positional encoding. The authors came up with a clever trick using sin and cosine functions.\n","\n","\n","We won’t go into the mathematical details of positional encoding, but here are the basics. For every odd index on the input vector, create a vector using the cos function. For every even index, create a vector using the sin function. Then add those vectors to their corresponding input embeddings. This successfully gives the network information on the position of each vector. The sin and cosine functions were chosen in tandem because they have linear properties the model can easily learn to attend to.\n","\n","Encoder Layer\n","Now we have the encoder layer. The Encoders layers job is to map all input sequences into an abstract continuous representation that holds the learned information for that entire sequence. It contains 2 sub-modules, multi-headed attention, followed by a fully connected network. There are also residual connections around each of the two sublayers followed by a layer normalization.\n","\n","\n","Encoder Layer Sub Modules\n","To break this down, let’s first look at the multi-headed attention module.\n","\n","Multi-Headed Attention\n","Multi-headed attention in the encoder applies a specific attention mechanism called self-attention. Self-attention allows the models to associate each word in the input, to other words. So in our example, it’s possible that our model can learn to associate the word “you”, with “how” and “are”. It’s also possible that the model learns that words structured in this pattern are typically a question so respond appropriately.\n","\n","\n","Encoder Self-Attention Operations. Reference this when looking at Illustrations below.\n","Query, Key, and Value Vectors\n","To achieve self-attention, we feed the input into 3 distinct fully connected layers to create the query, key, and value vectors.\n","\n","What are these vectors exactly? I found a good explanation on stack exchange stating….\n","\n","“The query key and value concept come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n","\n","Dot Product of Query and Key\n","After feeding the query, key, and value vector through a linear layer, the queries and keys undergo a dot product matrix multiplication to produce a score matrix.\n","\n","\n","Dot Product multiplication of the query and the key\n","The score matrix determines how much focus should a word be put on other words. So each word will have a score that corresponds to other words in the time-step. The higher the score the more focus. This is how the queries are mapped to the keys.\n","\n","\n","Attention scores from the dot product.\n","Scaling Down the Attention Scores\n","Then, the scores get scaled down by getting divided by the square root of the dimension of query and key. This is to allow for more stable gradients, as multiplying values can have exploding effects.\n","\n","\n","Scaling down the Attention scores\n","Softmax of the Scaled Scores\n","Next, you take the softmax of the scaled score to get the attention weights, which gives you probability values between 0 and 1. By doing a softmax the higher scores get heighten, and lower scores are depressed. This allows the model to be more confident about which words to attend too.\n","\n","\n","Taking the softmax of the scaled scores to get probability values\n","Multiply Softmax Output with Value vector\n","Then you take the attention weights and multiply it by your value vector to get an output vector. The higher softmax scores will keep the value of words the model learns is more important. The lower scores will drown out the irrelevant words. Then you feed the output of that into a linear layer to process.\n","\n","\n","Computing Multi-headed Attention\n","To make this a multi-headed attention computation, you need to split the query, key, and value into N vectors before applying self-attention. The split vectors then go through the self-attention process individually. Each self-attention process is called a head. Each head produces an output vector that gets concatenated into a single vector before going through the final linear layer. In theory, each head would learn something different therefore giving the encoder model more representation power.\n","\n","\n","Splitting Q, K, V, N times before applying self-attention\n","To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence.\n","\n","The Residual Connections, Layer Normalization, and Feed Forward Network\n","The multi-headed attention output vector is added to the original positional input embedding. This is called a residual connection. The output of the residual connection goes through a layer normalization.\n","\n","\n","Residual connection of the positional input embedding and the output of Multi-headed Attention\n","The normalized residual output gets projected through a pointwise feed-forward network for further processing. The pointwise feed-forward network is a couple of linear layers with a ReLU activation in between. The output of that is then again added to the input of the pointwise feed-forward network and further normalized.\n","\n","\n","Residual connection of the input and output of the point-wise feedforward layer.\n","The residual connections help the network train, by allowing gradients to flow through the networks directly. The layer normalizations are used to stabilize the network which results in substantially reducing the training time necessary. The pointwise feedforward layer is used to project the attention outputs potentially giving it a richer representation.\n","\n","Encoder Wrap-up\n","That wraps up the encoder layer. All of these operations are to encode the input to a continuous representation with attention information. This will help the decoder focus on the appropriate words in the input during the decoding process. You can stack the encoder N times to further encode the information, where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network.\n","\n","Decoder Layer\n","The decoder’s job is to generate text sequences. The decoder has a similar sub-layer as the encoder. it has two multi-headed attention layers, a pointwise feed-forward layer, and residual connections, and layer normalization after each sub-layer. These sub-layers behave similarly to the layers in the encoder but each multi-headed attention layer has a different job. The decoder is capped off with a linear layer that acts as a classifier, and a softmax to get the word probabilities.\n","\n","\n","Decoder Layer. Reference This diagram while reading.\n","The decoder is autoregressive, it begins with a start token, and it takes in a list of previous outputs as inputs, as well as the encoder outputs that contain the attention information from the input. The decoder stops decoding when it generates a token as an output.\n","\n","\n","The decoder is autoregressive as it generates a token 1 at a time while being fed in the previous outputs.\n","Let’s walk through the decoding steps.\n","\n","Decoder Input Embeddings & Positional Encoding\n","The beginning of the decoder is pretty much the same as the encoder. The input goes through an embedding layer and positional encoding layer to get positional embeddings. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input.\n","\n","Decoders First Multi-Headed Attention\n","This multi-headed attention layer operates slightly differently. Since the decoder is autoregressive and generates the sequence word by word, you need to prevent it from conditioning to future tokens. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after. The word “am” should only have access to itself and the words before it. This is true for all other words, where they can only attend to previous words.\n","\n","\n","A depiction of Decoder’s first Multi-headed Attention scaled attention scores. The word “am”, should not any values for the word “fine”. This is true for all other words.\n","We need a method to prevent computing attention scores for future words. This method is called masking. To prevent the decoder from looking at future tokens, you apply a look ahead mask. The mask is added before calculating the softmax, and after scaling the scores. Let’s take a look at how this works.\n","\n","Look-Ahead Mask\n","The mask is a matrix that’s the same size as the attention scores filled with values of 0’s and negative infinities. When you add the mask to the scaled attention scores, you get a matrix of the scores, with the top right triangle filled with negativity infinities.\n","\n","\n","Adding a look-ahead mask to the scaled scores\n","The reason for the mask is because once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens. As you can see in the figure below, the attention scores for “am”, has values for itself and all words before it but is zero for the word “fine”. This essentially tells the model to put no focus on those words.\n","\n","\n","This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer. This layer still has multiple heads, that the mask is being applied to, before getting concatenated and fed through a linear layer for further processing. The output of the first multi-headed attention is a masked output vector with information on how the model should attend on the decoder’s input.\n","\n","\n","Multi-Headed Attention with Masking\n","Decoder Second Multi-Headed Attention, and Point-wise Feed Forward Layer\n","The second multi-headed attention layer. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. This process matches the encoder’s input to the decoder’s input, allowing the decoder to decide which encoder input is relevant to put a focus on. The output of the second multi-headed attention goes through a pointwise feedforward layer for further processing.\n","\n","Linear Classifier and Final Softmax for Output Probabilities\n","The output of the final pointwise feedforward layer goes through a final linear layer, that acts as a classifier. The classifier is as big as the number of classes you have. For example, if you have 10,000 classes for 10,000 words, the output of that classier will be of size 10,000. The output of the classifier then gets fed into a softmax layer, which will produce probability scores between 0 and 1. We take the index of the highest probability score, and that equals our predicted word.\n","\n","\n","Linear Classifier with Softmax to get the Output Probabilities\n","The decoder then takes the output, add’s it to the list of decoder inputs, and continues decoding again until a token is predicted. For our case, the highest probability prediction is the final class which is assigned to the end token.\n","\n","The decoder can also be stacked N layers high, each layer taking in inputs from the encoder and the layers before it. By stacking the layers, the model can learn to extract and focus on different combinations of attention from its attention heads, potentially boosting its predictive power.\n","\n","\n","Stacked Encoder and Decoder\n","And That’s It!\n","And that’s it! That’s the mechanics of the transformers. Transformers leverage the power of the attention mechanism to make better predictions. Recurrent Neural networks try to achieve similar things, but because they suffer from short term memory. Transformers can be better especially if you want to encode or generate long sequences. Because of the transformer architecture, the natural language processing industry can achieve unprecedented results.\n","\n","Check out michaelphi.com for more content like this.\n","\"\"\"\n","data.update({\"a3\": \"https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0\"})"],"metadata":{"id":"yfkAxANuXuxL","executionInfo":{"status":"ok","timestamp":1663206784668,"user_tz":300,"elapsed":5,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### A4 - Introduction to Word Embedding and Word2Vec"],"metadata":{"id":"4u34yf6_Z_lS"}},{"cell_type":"code","source":["a4 = \"\"\"Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n","\n","What are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context?\n","\n","Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n","\n","Let’s tackle this part by part.\n","\n","Why do we need them?\n","\n","Consider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let’s call it V), it would have V = {Have, a, good, great, day}.\n","\n","Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\n","\n","Have = [1,0,0,0,0]`; a=[0,1,0,0,0]` ; good=[0,0,1,0,0]` ; great=[0,0,0,1,0]` ; day=[0,0,0,0,1]` (` represents transpose)\n","\n","If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true.\n","\n","Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.\n","\n","\n","Google Images\n","Here comes the idea of generating distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other, as mentioned earlier.\n","\n","How does Word2Vec work?\n","\n","Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n","\n","CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. Consider our example: Have a great day.\n","\n","Let the input to the Neural Network be the word, great. Notice that here we are trying to predict a target word (day) using a single context input word great. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day). In the process of predicting the target word, we learn the vector representation of the target word.\n","\n","Let us look deeper into the actual architecture.\n","\n","\n","CBOW Model\n","The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.\n","\n","Let’s get the terms in the picture right:\n","- Wvn is the weight matrix that maps the input x to the hidden layer (V*N dimensional matrix)\n","-W`nv is the weight matrix that maps the hidden layer outputs to the final output layer (N*V dimensional matrix)\n","\n","I won’t get into the mathematics. We’ll just get an idea of what’s going on.\n","\n","The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.\n","\n","But, the above model used a single context word to predict the target. We can use multiple context words to do the same.\n","\n","\n","Google images\n","The above model takes C context words. When Wvn is used to calculate hidden layer inputs, we take an average over all these C context word inputs.\n","\n","So, we have seen how word representations are generated using the context words. But there’s one more way we can do the same. We can use the target word (whose representation we want to generate) to predict the context and in the process, we produce the representations. Another variant, called Skip Gram model does this.\n","\n","Skip-Gram model:\n","\n","\n","This looks like multiple-context CBOW model just got flipped. To some extent that is true.\n","\n","We input the target word into the network. The model outputs C probability distributions. What does this mean?\n","\n","For each context position, we get C probability distributions of V probabilities, one for each word.\n","\n","In both the cases, the network uses back-propagation to learn. Detailed math can be found here\n","\n","Who wins?\n","\n","Both have their own advantages and disadvantages. According to Mikolov, Skip Gram works well with small amount of data and is found to represent rare words well.\n","\n","On the other hand, CBOW is faster and has better representations for more frequent words.\n","\n","What’s ahead?\n","\n","The above explanation is a very basic one. It just gives you a high-level idea of what word embeddings are and how Word2Vec works.\n","\n","There’s a lot more to it. For example, to make the algorithm computationally more efficient, tricks like Hierarchical Softmax and Skip-Gram Negative Sampling are used. All of it can be found here.\n","\n","Thanks for reading! I have started my personal blog and I don’t intend to write more amazing articles on Medium. Support my blog by subscribing to thenlp.space\"\"\"\n","a4 = a4.replace(\"→\", \"->\")\n","data.update({\"a4\": \"https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\"})"],"metadata":{"id":"rq7MVIlTX3-k","executionInfo":{"status":"ok","timestamp":1663206784668,"user_tz":300,"elapsed":5,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### A5 - Your Guide to Natural Language Processing (NLP)"],"metadata":{"id":"aboi7m64aCnN"}},{"cell_type":"code","source":["a5 = \"\"\"Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information.\n","\n","But there is a problem: one person may generate hundreds or thousands of words in a declaration, each sentence with its corresponding complexity. If you want to scale and analyze several hundreds, thousands or millions of people or declarations in a given geography, then the situation is unmanageable.\n","\n","Data generated from conversations, declarations or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret a text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way it is possible to detect figures of speech like irony, or even perform sentiment analysis.\n","\n","Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.\n","\n","It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.\n","\n","Use Cases of NLP\n","In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.\n","\n","NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples:\n","\n","NLP enables the recognition and prediction of diseases based on electronic health records and patient’s own speech. This capability is being explored in health conditions that go from cardiovascular diseases to depression and even schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications and treatment outcomes from patient notes, clinical trial reports and other electronic health records.\n","Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media. This sentiment analysis can provide a lot of information about customers choices and their decision drivers.\n","An inventor at IBM developed a cognitive assistant that works like a personalized search engine by learning all about you and then remind you of a name, a song, or anything you can’t remember the moment you need it to.\n","Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spam before they even enter your inbox.\n","To help identifying fake news, the NLP Group at MIT developed a new system to determine if a source is accurate or politically biased, detecting if a news source can be trusted or not.\n","Amazon’s Alexa and Apple’s Siri are examples of intelligent voice driven interfaces that use NLP to respond to vocal prompts and do everything like find a particular shop, tell us the weather forecast, suggest the best route to the office or turn on the lights at home.\n","Having an insight into what is happening and what people are talking about can be very valuable to financial traders. NLP is being used to track news, reports, comments about possible mergers between companies, everything can be then incorporated into a trading algorithm to generate massive profits. Remember: buy the rumor, sell the news.\n","NLP is also being used in both the search and selection phases of talent recruitment, identifying the skills of potential hires and also spotting prospects before they become active on the job market.\n","Powered by IBM Watson NLP technology, LegalMation developed a platform to automate routine litigation tasks and help legal teams save time, drive down costs and shift strategic focus.\n","NLP is particularly booming in the healthcare industry. This technology is improving care delivery, disease diagnosis and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this.\n","\n","\n","Number of publications containing the sentence “natural language processing” in PubMed in the period 1978–2018. As of 2018, PubMed comprised more than 29 million citations for biomedical literature\n","Companies like Winterlight Labs are making huge improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment through speech and they can also support clinical trials and studies for a wide range of central nervous system disorders. Following a similar approach, Stanford University developed Woebot, a chatbot therapist with the aim of helping people with anxiety and other disorders.\n","\n","But serious controversy is around the subject. A couple of years ago Microsoft demonstrated that by analyzing large samples of search engine queries, they could identify internet users who were suffering from pancreatic cancer even before they have received a diagnosis of the disease. How would users react to such diagnosis? And what would happen if you were tested as a false positive? (meaning that you can be diagnosed with the disease even though you don’t have it). This recalls the case of Google Flu Trends which in 2009 was announced as being able to predict influenza but later on vanished due to its low accuracy and inability to meet its projected rates.\n","\n","NLP may be the key to an effective clinical support in the future, but there are still many challenges to face in the short term.\n","\n","Basic NLP to impress your non-NLP friends\n","The main drawbacks we face these days with NLP relate to the fact that language is very tricky. The process of understanding and manipulating language is extremely complex, and for this reason it is common to use different techniques to handle different challenges before binding everything together. Programming languages like Python or R are highly used to perform these techniques, but before diving into code lines (that will be the topic of a different article), it’s important to understand the concepts beneath them. Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms:\n","\n","Bag of Words\n","Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.\n","\n","To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles:\n","\n","Words are flowing out like endless rain into a paper cup,\n","\n","They slither while they pass, they slip away across the universe\n","\n","Now let’s count the words:\n","\n","\n","This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”).\n","\n","To solve this problem, one approach is to rescale the frequency of words by how often they appear in all texts (not just the one we are analyzing) so that the scores for frequent words like “the”, that are also frequent across other texts, get penalized. This approach to scoring is called “Term Frequency — Inverse Document Frequency” (TFIDF), and improves the bag of words by weights. Through TFIDF frequent terms in the text are “rewarded” (like the word “they” in our example), but they also get “punished” if those terms are frequent in other texts we include in the algorithm too. On the contrary, this method highlights and “rewards” unique or rare terms considering all texts. Nevertheless, this approach still has no context nor semantics.\n","\n","Tokenization\n","Is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Following our example, the result of tokenization would be:\n","\n","\n","Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).\n","\n","Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.\n","\n","The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks.\n","\n","For deeper details on tokenization, you can find a great explanation in this article.\n","\n","Stop Words Removal\n","Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.\n","\n","Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.\n","\n","There is no universal list of stop words. These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all.\n","\n","The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective.\n","\n","Stemming\n","Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).\n","\n","Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).\n","\n","The problem is that affixes can create or expand new forms of the same word (called inflectional affixes), or even create new words themselves (called derivational affixes). In English, prefixes are always derivational (the affix creates a new word as in the example of the prefix “eco” in the word “ecosystem”), but suffixes can be derivational (the affix creates a new word as in the example of the suffix “ist” in the word “guitarist”) or inflectional (the affix creates a new form of word as in the example of the suffix “er” in the word “faster”).\n","\n","Ok, so how can we tell the difference and chop the right bit?\n","\n","\n","A possible approach is to consider a list of common affixes and rules (Python and R languages have different libraries containing affixes and methods) and perform stemming based on them, but of course this approach presents limitations. Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To offset this effect you can edit those predefined methods by adding or removing affixes and rules, but you must consider that you might be improving the performance in one area while producing a degradation in another one. Always look at the whole picture and test your model’s performance.\n","\n","So if stemming has serious limitations, why do we use it? First of all, it can be used to correct spelling errors from the tokens. Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise.\n","\n","Lemmatization\n","Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.\n","\n","Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.\n","\n","For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.\n","\n","\n","Lemmatization also takes into consideration the context of the word in order to solve other problems like disambiguation, which means it can discriminate between identical words that have different meanings depending on the specific context. Think about words like “bat” (which can correspond to the animal or to the metal/wooden club used in baseball) or “bank” (corresponding to the financial institution or to the land alongside a body of water). By providing a part-of-speech parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to define a role for that word in the sentence and remove disambiguation.\n","\n","As you might already pictured, lemmatization is a much more resource-intensive task than performing a stemming process. At the same time, since it requires more knowledge about the language structure than a stemming approach, it demands more computational power than setting up or adapting a stemming algorithm.\n","\n","Topic Modeling\n","Is as a method for uncovering hidden structures in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts.\n","\n","From the universe of topic modelling techniques, Latent Dirichlet Allocation (LDA) is probably the most commonly used. This relatively new algorithm (invented less than 20 years ago) works as an unsupervised learning method that discovers different topics underlying a collection of documents. In unsupervised learning methods like this one, there is no output variable to guide the learning process and data is explored by algorithms to find patterns. To be more specific, LDA finds groups of related words by:\n","\n","Assigning each word to a random topic, where the user defines the number of topics it wishes to uncover. You don’t define the topics themselves (you define just the number of topics) and the algorithm will map all documents to the topics in a way that words in each document are mostly captured by those imaginary topics.\n","The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic. These probabilities are calculated multiple times, until the convergence of the algorithm.\n","Unlike other clustering algorithms like K-means that perform hard clustering (where topics are disjointed), LDA assigns each document to a mixture of topics, which means that each document can be described by one or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B and 10% of topic C) and reflect more realistic results.\n","\n","\n","Topic modeling is extremely useful for classifying texts, building recommender systems (e.g. to recommend you books based on your past readings) or even detecting trends in online publications.\n","\n","How does the future look like?\n","At the moment NLP is battling to detect nuances in language meaning, whether due to lack of context, spelling errors or dialectal differences.\n","\n","On March 2016 Microsoft launched Tay, an Artificial Intelligence (AI) chatbot released on Twitter as a NLP experiment. The idea was that as more users conversed with Tay, the smarter it would get. Well, the result was that after 16 hours Tay had to be removed due to its racist and abusive comments:\n","\n","\n","\n","Microsoft learnt from its own experience and some months later released Zo, its second generation English-language chatbot that won’t be caught making the same mistakes as its predecessor. Zo uses a combination of innovative approaches to recognize and generate conversation, and other companies are exploring with bots that can remember details specific to an individual conversation.\n","\n","Although the future looks extremely challenging and full of threats for NLP, the discipline is developing at a very fast pace (probably like never before) and we are likely to reach a level of advancement in the coming years that will make complex applications look possible.\n","\n","Thanks Jesús del Valle , Jannis Busch and Sabrina Steinert for your valuable inputs\n","\n","Interested in these topics? Follow me on Linkedin or Twitter\n","\"\"\"\n","data.update({\"a5\": \"https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1\"})"],"metadata":{"id":"IYthnjdCX_kR","executionInfo":{"status":"ok","timestamp":1663206784848,"user_tz":300,"elapsed":13,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9lhDLMJLeNG","executionInfo":{"status":"ok","timestamp":1663206784849,"user_tz":300,"elapsed":13,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"3c76c380-ecd5-4c54-8964-f6c2e532b544"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a1': 'https://towardsdatascience.com/gpt-what-why-this-groundbreaking-model-is-driving-the-future-of-ai-and-nlp-e38fcf891172',\n"," 'a2': 'https://towardsdatascience.com/transformers-141e32e69591',\n"," 'a3': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0',\n"," 'a4': 'https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa',\n"," 'a5': 'https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1'}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["a1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"zoGHePELYJvj","executionInfo":{"status":"ok","timestamp":1663206784849,"user_tz":300,"elapsed":11,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"f2d8c133-9ff8-465c-b760-975aaa2e691b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hype over GPT-3 reached an all-time high on Twitter over the weekend and many are calling the technological development a groundbreaking inflection point for future AI research. In this article I explore what GPT is, what it means for AI development, and where we might be headed from here.\\nOpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools. Those with early API access through OpenAI’s beta program went to Twitter to showcase impressive early tools built using GPT-3 technology:\\n\\n\\n\\n\\nFor non-engineers, this may look like magic, but there is a lot to be unpacked here. In this article I will provide a brief overview of GPT and what it can be used for.\\n\\nWhat is OpenAI and GPT-3?\\n\\nOpenAI is an AI research laboratory founded in 2015 by Elon Musk, Sam Altman, and others with the mission of creating AI that benefits all of humanity. The company recently received $1 billion of additional funding from Microsoft in 2019 and is considered a leader in AI research and development.\\n\\nHistorically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). Normally, this can be extremely time consuming and expensive. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task.\\n\\nFor example, suppose you would like to learn a new language — German. Initially, you will still think about your sentences in English, then translate and rearrange words to come up with the German equivalent. The reality is, you are still indirectly applying learnings about sentence structure, language, and communication from the previous language even though the actual words and grammar are different. This is why learning new languages is typically easier if you already know another language.\\n\\nApplying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. In this great walkthrough, Francois Chollet compared the effectiveness of an AI model trained from scratch to one built from a pre-trained model. His results showed that the latter had 15% greater predictive accuracy after training both with the same amount of training data.\\n\\nIn 2018, OpenAI presented convincing research showing that this strategy (pairing supervised learning with unsupervised pre-training) is particularly very effective in NLP tasks. They first produced a generative pre-trained model (“GPT”) using “a diverse corpus of unlabeled text” (i.e. over 7,000 unique unpublished books from a variety of genres), essentially creating a model that “understood” English and language. Next, this pre-trained model could be further fine-tuned and trained to perform specific tasks using supervised learning. As an analogy, this would be like teaching someone English, then training him or her for the specific task of reading and classifying resumes of acceptable and unacceptable candidates for hiring.\\n\\nGPT-3 is the latest iteration of the GPT model and was first described in May 2020. It contains 175 billion parameters compared to the 1.5 billion in GPT-2 (117x increase) and training it consumed several thousand petaflop/s-days of computing power. GPT-3 is fed with much more data and tuned with more parameters than GPT-2, and as a result, it has produced some amazing NLP capabilities so far. The volume of data and computing resources required makes it impossible for many organizations to recreate this, but luckily they won’t have to since OpenAI plans to release access via API in the future.\\n\\nCritical reception\\n\\nAdmittedly, GPT-3 didn’t get much attention until last week’s viral tweets by Sharif Shameem and others (above). They demonstrated that GPT-3 could be used to create websites based on plain English instructions, envisioning a new era of no-code technologies where people can create apps by simply describing them in words. Early adopter Kevin Lacker tested the model with a Turing test and saw amazing results. GPT-3 performed exceptionally well in the initial Q&A and displayed many aspects of “common sense” that AI systems traditionally struggle with.\\n\\nHowever, the model is far from perfect. Max Woolf performed a critical analysis noting several issues such as model latency, implementation issues, and concerning biases in the data that need to be re-considered. Several users have reported these issues on Twitter as well:\\n\\n\\n\\nOpenAI’s blog discusses some of the key drawbacks of the model, most notably that GPT’s entire understanding of the world is based on the texts it was trained on. Case in point: it was trained in October 2019 and therefore does not know about COVID-19. It is unclear how these texts were chosen and what oversight was performed (or required) in this process.\\n\\nAdditionally, the enormous computing resources required to produce and maintain these models raise serious questions about the environmental impact of AI technologies. Although often overlooked, both hardware and software usage significantly contribute to depletion of energy resources, excessive waste generation, and excessive mining of rare earth minerals with the associated negative impacts to human health.\\n\\nTo quell concerns, OpenAI has repeatedly stated its mission to produce AI for the good of humanity and aims to stop access to its API if misuse is detected. Even in it’s beta access form, it asks candidates to describe their intentions with the technology and the benefits and risks to society.\\n\\n\\nWhere do we go from here?\\n\\nWithout a doubt, GPT-3 still represents a major milestone in AI development. Many early users have built impressive apps that accurately process natural language and produce amazing results. In summary:\\n\\nGPT-3 is a major improvement upon GPT-2 and features far greater accuracy for better use cases. This is a significant step forward for AI development, impressively accomplished in just a two-year time frame\\nEarly tools that have been built on GPT-3 show great promise for commercial usability such as: no-code platforms that allow you to build apps by describing then; advanced search platforms using plain English; and better data analytics tools that make data gathering and processing much faster\\nOpenAI announced plans to release a commercial API, which will enable organizations to build products powered by GPT-3 at scale. However, many questions remain about how exactly this will be executed — pricing, SLA, model latency, etc.\\nUsers have pointed out several issues that need to be addressed before widespread commercial use. Inherent biases in the model, questions around fairness and ethics, and concerns about misuse (fake news, bots, etc.) need to be thought through and oversight might be necessary\\nOpenAI is openly committed to creating AI for the benefit of humanity, but still, monitoring for misuse at scale will be difficult to achieve. This raises a broader question about the necessity of government involvement to protect the rights of individuals\\nAll said, I’m extremely excited to see which new technologies are built on GPT-3 and how OpenAI continues to improve on its model. Increased attention and funding in NLP and GPT-3 might be enough to ward off fears from many critics that an AI winter might be coming (myself included). Despite the shortfalls of the model, I am hoping that everyone can be optimistic about a future where humans and machines will communicate with each other in a unified language and the ability to create tools using technology will be accessible to billions of more people.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["articles = [a1, a2, a3, a4, a5]"],"metadata":{"id":"lYVG0jwcYbvG","executionInfo":{"status":"ok","timestamp":1663206784850,"user_tz":300,"elapsed":10,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Write Text to Files"],"metadata":{"id":"Pme16DOTYxcJ"}},{"cell_type":"code","source":["def write_to_file(txt, title, remove_newline=False):\n","    if remove_newline:\n","        txt = re.sub(\"\\s+\", \" \", txt)       # Remove newlines to make article's code work\n","    with open(\"%s.txt\"%(title), \"w\") as file:\n","        file.write(txt)"],"metadata":{"id":"j8K0y0kHY1U3","executionInfo":{"status":"ok","timestamp":1663206784850,"user_tz":300,"elapsed":9,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["for i, a in enumerate(articles):\n","    write_to_file(a, \"a%d\"%(i+1), remove_newline=True)"],"metadata":{"id":"YJRt3KkGZhyY","executionInfo":{"status":"ok","timestamp":1663206784850,"user_tz":300,"elapsed":9,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Article's Code"],"metadata":{"id":"dv3e6F5DG58H"}},{"cell_type":"code","source":["txt = \"\"\"In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\"\"\n","with open(\"msft.txt\", \"w\") as file:\n","    file.write(txt)"],"metadata":{"id":"e_cRMKS8NEnH","executionInfo":{"status":"ok","timestamp":1663206784851,"user_tz":300,"elapsed":10,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def read_article(file_name):\n","    file = open(file_name, \"r\")\n","    filedata = file.readlines()\n","    article = filedata[0].split(\". \")\n","    sentences = []\n","    for sentence in article:\n","        # print(sentence)\n","        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","        # sentences.pop() \n","    \n","    return sentences"],"metadata":{"id":"CVYlyAoMG5f-","executionInfo":{"status":"ok","timestamp":1663206784851,"user_tz":300,"elapsed":9,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def sentence_similarity(sent1, sent2, stopwords=None):\n","    if stopwords is None:\n","        stopwords = []\n"," \n","    sent1 = [w.lower() for w in sent1]\n","    sent2 = [w.lower() for w in sent2]\n"," \n","    all_words = list(set(sent1 + sent2))\n"," \n","    vector1 = [0] * len(all_words)\n","    vector2 = [0] * len(all_words)\n"," \n","    # build the vector for the first sentence\n","    for w in sent1:\n","        if w in stopwords:\n","            continue\n","        vector1[all_words.index(w)] += 1\n"," \n","    # build the vector for the second sentence\n","    for w in sent2:\n","        if w in stopwords:\n","            continue\n","        vector2[all_words.index(w)] += 1\n"," \n","    return 1 - cosine_distance(vector1, vector2)"],"metadata":{"id":"0SFJaUqHH1R5","executionInfo":{"status":"ok","timestamp":1663206784851,"user_tz":300,"elapsed":9,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def build_similarity_matrix(sentences, stop_words):\n","    # Create an empty similarity matrix\n","    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n"," \n","    for idx1 in range(len(sentences)):\n","        for idx2 in range(len(sentences)):\n","            if idx1 == idx2: #ignore if both are same sentences\n","                continue \n","            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n","    return similarity_matrix"],"metadata":{"id":"Wa3bjLIKHDwS","executionInfo":{"status":"ok","timestamp":1663206784852,"user_tz":300,"elapsed":10,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def generate_summary_old(file_name, top_n=5):\n","    stop_words = stopwords.words('english')\n","    summarize_text = []\n","    # Step 1 - Read text and tokenize\n","    sentences =  read_article(file_name)\n","    # Step 2 - Generate Similary Martix across sentences\n","    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n","    # Step 3 - Rank sentences in similarity martix\n","    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n","    scores = nx.pagerank(sentence_similarity_graph)\n","    # Step 4 - Sort the rank and pick top sentences\n","    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n","    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n","    for i in range(top_n):\n","        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n","        # Step 5 - Offcourse, output the summarize texr\n","        print(\"\\nSummarize Text %d: \\n%s\"%(i, \". \".join(summarize_text)))"],"metadata":{"id":"4fC9NY_mHH8D","executionInfo":{"status":"ok","timestamp":1663206784852,"user_tz":300,"elapsed":10,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Test on article's data\n","generate_summary_old(\"msft.txt\", 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAaTzdKXHNph","executionInfo":{"status":"ok","timestamp":1663206785076,"user_tz":300,"elapsed":234,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"85951c06-8271-4112-c232-37c34423c040"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Indexes of top ranked_sentence order are  [(0.13083383500129075, ['This', 'program', 'also', 'included', 'developer-focused', 'AI', 'school', 'that', 'provided', 'a', 'bunch', 'of', 'assets', 'to', 'help', 'build', 'AI', 'skills.']), (0.1262117608296295, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.11386767381829474, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10100416115804903, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.09409890128802637, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.0911373445844822, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI']), (0.08899375984519095, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow.\"', 'The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.08689211093261075, ['That’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.08543021908426283, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.08153023345816275, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well'])]\n","\n","Summarize Text 0: \n","This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\n","\n","Summarize Text 1: \n","This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services\n"]}]},{"cell_type":"markdown","source":["# Assignment Code/Modified Article Code"],"metadata":{"id":"b4X9meDmLNr9"}},{"cell_type":"markdown","source":["## Load Files"],"metadata":{"id":"1vmLI_PEP4tj"}},{"cell_type":"code","source":["# Effectivly unmodified (removed print and a broken list pop)\n","def read_article(file_name):\n","    file = open(file_name, \"r\")\n","    filedata = file.readlines()\n","    article = filedata[0].split(\". \")\n","    sentences = []\n","    for sentence in article:\n","        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","    \n","    return sentences"],"metadata":{"id":"DHRdaKYOP52W","executionInfo":{"status":"ok","timestamp":1663206785076,"user_tz":300,"elapsed":3,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Clean Text"],"metadata":{"id":"La6bkvb5dpoa"}},{"cell_type":"code","source":["from nltk.stem.wordnet import WordNetLemmatizer\n","def clean_text(sentence):\n","\n","    # Recombine sentence\n","    sent = \" \".join(sentence)\n","\n","    # Lowercase\n","    sent = sent.lower()\n","\n","    # Drop possessives\n","    sent = re.sub(\"('s)+\", \"\", sent)\n","\n","    # Drop punctuation\n","    punct = list(\"!@#$%^&*(),./<>?;:'\\\"[]\\{}|`~-_\")\n","    sent = re.sub(\"([^a-zA-Z0-9 ])\", \"\", sent)\n","\n","    # Change numbers for single token\n","    sent = re.sub(\"[0-9]+\", \"<NUM>\", sent)\n","\n","    # Re-split sentence\n","    sent = re.sub(\"\\s+\", \" \", sent.strip())\n","    sent = sent.split(\" \")\n","\n","    # Lemmatize words and remove stopwords\n","    lemmatizer = WordNetLemmatizer()\n","    for i, w in enumerate(sent):\n","        sent[i] = lemmatizer.lemmatize(w, pos=\"v\")\n","\n","    return sent"],"metadata":{"id":"rzttN_C-dr6n","executionInfo":{"status":"ok","timestamp":1663206785076,"user_tz":300,"elapsed":3,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## Sentence Similarity"],"metadata":{"id":"6JCgo5BUP-CN"}},{"cell_type":"code","source":["def sentence_similarity(sent1, sent2, stopwords=None):\n","    if stopwords is None:\n","        stopwords = []\n","    stopwords.append(\"<NUM>\")\n","    \n","    # Clean text\n","    sent1 = clean_text(sent1)\n","    sent2 = clean_text(sent2)\n"," \n","    all_words = list(set(sent1 + sent2))\n"," \n","    vector1 = [0] * len(all_words)\n","    vector2 = [0] * len(all_words)\n"," \n","    # build the vector for the first sentence\n","    for w in sent1:\n","        if w in stopwords:\n","            continue\n","        vector1[all_words.index(w)] += 1\n"," \n","    # build the vector for the second sentence\n","    for w in sent2:\n","        if w in stopwords:\n","            continue\n","        vector2[all_words.index(w)] += 1\n"," \n","    return 1 - cosine_distance(vector1, vector2)"],"metadata":{"id":"63i-Jks0oJfP","executionInfo":{"status":"ok","timestamp":1663206785077,"user_tz":300,"elapsed":4,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Unmodified\n","def build_similarity_matrix(sentences, stop_words):\n","    # Create an empty similarity matrix\n","    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n"," \n","    for idx1 in range(len(sentences)):\n","        for idx2 in range(len(sentences)):\n","            if idx1 == idx2: #ignore if both are same sentences\n","                continue \n","            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n","    return similarity_matrix"],"metadata":{"id":"J1rdFLnoQGKL","executionInfo":{"status":"ok","timestamp":1663206785077,"user_tz":300,"elapsed":4,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["## Genereate Summary"],"metadata":{"id":"XHaUJYH2QJ3p"}},{"cell_type":"code","source":["# Unmodified\n","def generate_summary_update(file_name, top_n=5):\n","    stop_words = stopwords.words('english')\n","    summarize_text = []\n","    # Step 1 - Read text and tokenize\n","    sentences =  read_article(file_name)\n","    # Step 2 - Generate Similary Martix across sentences\n","    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n","    # Step 3 - Rank sentences in similarity martix\n","    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n","    scores = nx.pagerank(sentence_similarity_graph)\n","    # Step 4 - Sort the rank and pick top sentences\n","    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n","    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n","    for i in range(top_n):\n","        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n","        # Step 5 - Offcourse, output the summarize texr\n","        print(\"\\nSummarize Text %d: \\n%s\"%(i, \". \".join(summarize_text)))"],"metadata":{"id":"cNLVlEbfQL71","executionInfo":{"status":"ok","timestamp":1663206785077,"user_tz":300,"elapsed":3,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Test on article's data\n","generate_summary_update(\"msft.txt\", 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXTEpAafQMXc","executionInfo":{"status":"ok","timestamp":1663206788969,"user_tz":300,"elapsed":3895,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"e2ce0bb9-0839-443a-f487-0d75aacc8732"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Indexes of top ranked_sentence order are  [(0.13873172763348482, ['This', 'program', 'also', 'included', 'developer-focused', 'AI', 'school', 'that', 'provided', 'a', 'bunch', 'of', 'assets', 'to', 'help', 'build', 'AI', 'skills.']), (0.12941619104824872, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.10637727120952348, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.1023692305838796, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.09550051135411729, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow.\"', 'The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09497513524239187, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.09476269841278578, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08109455752036769, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.08016057565127079, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI']), (0.07661210134392986, ['That’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies'])]\n","\n","Summarize Text 0: \n","This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\n","\n","Summarize Text 1: \n","This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services\n"]}]},{"cell_type":"markdown","source":["# Generate Summaries Using Both Unmodified and Modified Code"],"metadata":{"id":"lR5ztS2CrhDr"}},{"cell_type":"code","source":["for i, a in enumerate(articles):\n","    print(\"Article a%d: =======================================================================\\n\\nOld: ------------------------------------------------------------------------------\\n\"%(i+1))\n","    generate_summary_old(\"a%d.txt\"%(i+1))\n","    print(\"\\nUpdated: ------------------------------------------------------------------------\\n\")\n","    generate_summary_update(\"a%d.txt\"%(i+1))\n","    print(\"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lK74xNJwoCQD","executionInfo":{"status":"ok","timestamp":1663207353886,"user_tz":300,"elapsed":564923,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"a4e222c4-2644-4013-fe58-6d67d56bb848"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Article a1: =======================================================================\n","\n","Old: ------------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.04229507258112702, ['Applying', 'this', 'strategy', 'to', 'AI', 'means', 'that', 'we', 'can', 'use', 'pre-trained', 'models', 'to', 'create', 'new', 'models', 'more', 'quickly', 'with', 'less', 'training', 'data']), (0.03413848933830867, ['To', 'solve', 'this,', 'scientists', 'have', 'used', 'an', 'approach', 'called', 'transfer', 'learning:', 'use', 'the', 'existing', 'representations/information', 'learned', 'in', 'a', 'previously-trained', 'model', 'as', 'a', 'starting', 'point', 'to', 'fine-tune', 'and', 'train', 'a', 'new', 'model', 'for', 'a', 'different', 'task']), (0.0331916180902335, ['Historically,', 'obtaining', 'large', 'quantities', 'of', 'labelled', 'data', 'to', 'use', 'to', 'train', 'models', 'has', 'been', 'a', 'major', 'barrier', 'in', 'NLP', 'development', '(and', 'AI', 'development', 'in', 'general)']), (0.033142312522327765, ['OpenAI’s', 'GPT-3', 'language', 'model', 'gained', 'significant', 'attention', 'last', 'week,', 'leading', 'many', 'to', 'believe', 'that', 'the', 'new', 'technology', 'represents', 'a', 'significant', 'inflection', 'point', 'in', 'the', 'development', 'of', 'Natural', 'Language', 'Processing', '(NLP)', 'tools']), (0.03154812781456989, ['In', 'this', 'great', 'walkthrough,', 'Francois', 'Chollet', 'compared', 'the', 'effectiveness', 'of', 'an', 'AI', 'model', 'trained', 'from', 'scratch', 'to', 'one', 'built', 'from', 'a', 'pre-trained', 'model']), (0.03143381157292195, ['Next,', 'this', 'pre-trained', 'model', 'could', 'be', 'further', 'fine-tuned', 'and', 'trained', 'to', 'perform', 'specific', 'tasks', 'using', 'supervised', 'learning']), (0.030250210014887834, ['This', 'is', 'a', 'significant', 'step', 'forward', 'for', 'AI', 'development,', 'impressively', 'accomplished', 'in', 'just', 'a', 'two-year', 'time', 'frame', 'Early', 'tools', 'that', 'have', 'been', 'built', 'on', 'GPT-3', 'show', 'great', 'promise', 'for', 'commercial', 'usability', 'such', 'as:', 'no-code', 'platforms', 'that', 'allow', 'you', 'to', 'build', 'apps', 'by', 'describing', 'then;', 'advanced', 'search', 'platforms', 'using', 'plain', 'English;', 'and', 'better', 'data', 'analytics', 'tools', 'that', 'make', 'data', 'gathering', 'and', 'processing', 'much', 'faster', 'OpenAI', 'announced', 'plans', 'to', 'release', 'a', 'commercial', 'API,', 'which', 'will', 'enable', 'organizations', 'to', 'build', 'products', 'powered', 'by', 'GPT-3', 'at', 'scale']), (0.02786433423817768, ['GPT-3', 'is', 'the', 'latest', 'iteration', 'of', 'the', 'GPT', 'model', 'and', 'was', 'first', 'described', 'in', 'May', '2020']), (0.02625734467122521, ['What', 'is', 'OpenAI', 'and', 'GPT-3?', 'OpenAI', 'is', 'an', 'AI', 'research', 'laboratory', 'founded', 'in', '2015', 'by', 'Elon', 'Musk,', 'Sam', 'Altman,', 'and', 'others', 'with', 'the', 'mission', 'of', 'creating', 'AI', 'that', 'benefits', 'all', 'of', 'humanity']), (0.025660851147810762, ['Additionally,', 'the', 'enormous', 'computing', 'resources', 'required', 'to', 'produce', 'and', 'maintain', 'these', 'models', 'raise', 'serious', 'questions', 'about', 'the', 'environmental', 'impact', 'of', 'AI', 'technologies']), (0.025539444506008396, ['GPT-3', 'is', 'fed', 'with', 'much', 'more', 'data', 'and', 'tuned', 'with', 'more', 'parameters', 'than', 'GPT-2,', 'and', 'as', 'a', 'result,', 'it', 'has', 'produced', 'some', 'amazing', 'NLP', 'capabilities', 'so', 'far']), (0.0244256449216283, ['This', 'raises', 'a', 'broader', 'question', 'about', 'the', 'necessity', 'of', 'government', 'involvement', 'to', 'protect', 'the', 'rights', 'of', 'individuals', 'All', 'said,', 'I’m', 'extremely', 'excited', 'to', 'see', 'which', 'new', 'technologies', 'are', 'built', 'on', 'GPT-3', 'and', 'how', 'OpenAI', 'continues', 'to', 'improve', 'on', 'its', 'model']), (0.02427953214679208, ['Hype', 'over', 'GPT-3', 'reached', 'an', 'all-time', 'high', 'on', 'Twitter', 'over', 'the', 'weekend', 'and', 'many', 'are', 'calling', 'the', 'technological', 'development', 'a', 'groundbreaking', 'inflection', 'point', 'for', 'future', 'AI', 'research']), (0.024203077470516022, ['Where', 'do', 'we', 'go', 'from', 'here?', 'Without', 'a', 'doubt,', 'GPT-3', 'still', 'represents', 'a', 'major', 'milestone', 'in', 'AI', 'development']), (0.02382055294755228, ['In', 'summary:', 'GPT-3', 'is', 'a', 'major', 'improvement', 'upon', 'GPT-2', 'and', 'features', 'far', 'greater', 'accuracy', 'for', 'better', 'use', 'cases']), (0.02345700539040808, ['They', 'first', 'produced', 'a', 'generative', 'pre-trained', 'model', '(“GPT”)', 'using', '“a', 'diverse', 'corpus', 'of', 'unlabeled', 'text”', '(i.e']), (0.02333564919205295, ['Inherent', 'biases', 'in', 'the', 'model,', 'questions', 'around', 'fairness', 'and', 'ethics,', 'and', 'concerns', 'about', 'misuse', '(fake', 'news,', 'bots,', 'etc.)', 'need', 'to', 'be', 'thought', 'through', 'and', 'oversight', 'might', 'be', 'necessary', 'OpenAI', 'is', 'openly', 'committed', 'to', 'creating', 'AI', 'for', 'the', 'benefit', 'of', 'humanity,', 'but', 'still,', 'monitoring', 'for', 'misuse', 'at', 'scale', 'will', 'be', 'difficult', 'to', 'achieve']), (0.02318825003106307, ['They', 'demonstrated', 'that', 'GPT-3', 'could', 'be', 'used', 'to', 'create', 'websites', 'based', 'on', 'plain', 'English', 'instructions,', 'envisioning', 'a', 'new', 'era', 'of', 'no-code', 'technologies', 'where', 'people', 'can', 'create', 'apps', 'by', 'simply', 'describing', 'them', 'in', 'words']), (0.02239344128476017, ['Despite', 'the', 'shortfalls', 'of', 'the', 'model,', 'I', 'am', 'hoping', 'that', 'everyone', 'can', 'be', 'optimistic', 'about', 'a', 'future', 'where', 'humans', 'and', 'machines', 'will', 'communicate', 'with', 'each', 'other', 'in', 'a', 'unified', 'language', 'and', 'the', 'ability', 'to', 'create', 'tools', 'using', 'technology', 'will', 'be', 'accessible', 'to', 'billions', 'of', 'more', 'people']), (0.022378197369966792, ['Those', 'with', 'early', 'API', 'access', 'through', 'OpenAI’s', 'beta', 'program', 'went', 'to', 'Twitter', 'to', 'showcase', 'impressive', 'early', 'tools', 'built', 'using', 'GPT-3', 'technology:', 'For', 'non-engineers,', 'this', 'may', 'look', 'like', 'magic,', 'but', 'there', 'is', 'a', 'lot', 'to', 'be', 'unpacked', 'here']), (0.021487120990645623, ['Increased', 'attention', 'and', 'funding', 'in', 'NLP', 'and', 'GPT-3', 'might', 'be', 'enough', 'to', 'ward', 'off', 'fears', 'from', 'many', 'critics', 'that', 'an', 'AI', 'winter', 'might', 'be', 'coming', '(myself', 'included)']), (0.021472358374835646, ['over', '7,000', 'unique', 'unpublished', 'books', 'from', 'a', 'variety', 'of', 'genres),', 'essentially', 'creating', 'a', 'model', 'that', '“understood”', 'English', 'and', 'language']), (0.020830771927334384, ['However,', 'the', 'model', 'is', 'far', 'from', 'perfect']), (0.02060444971474681, ['Many', 'early', 'users', 'have', 'built', 'impressive', 'apps', 'that', 'accurately', 'process', 'natural', 'language', 'and', 'produce', 'amazing', 'results']), (0.020308874037556407, ['Several', 'users', 'have', 'reported', 'these', 'issues', 'on', 'Twitter', 'as', 'well:', 'OpenAI’s', 'blog', 'discusses', 'some', 'of', 'the', 'key', 'drawbacks', 'of', 'the', 'model,', 'most', 'notably', 'that', 'GPT’s', 'entire', 'understanding', 'of', 'the', 'world', 'is', 'based', 'on', 'the', 'texts', 'it', 'was', 'trained', 'on']), (0.01951641070540304, ['GPT-3', 'performed', 'exceptionally', 'well', 'in', 'the', 'initial', 'Q&A', 'and', 'displayed', 'many', 'aspects', 'of', '“common', 'sense”', 'that', 'AI', 'systems', 'traditionally', 'struggle', 'with']), (0.01949691358894819, ['Max', 'Woolf', 'performed', 'a', 'critical', 'analysis', 'noting', 'several', 'issues', 'such', 'as', 'model', 'latency,', 'implementation', 'issues,', 'and', 'concerning', 'biases', 'in', 'the', 'data', 'that', 'need', 'to', 'be', 're-considered']), (0.019276843730331422, ['In', 'this', 'article', 'I', 'explore', 'what', 'GPT', 'is,', 'what', 'it', 'means', 'for', 'AI', 'development,', 'and', 'where', 'we', 'might', 'be', 'headed', 'from', 'here']), (0.019206516548083096, ['It', 'contains', '175', 'billion', 'parameters', 'compared', 'to', 'the', '1.5', 'billion', 'in', 'GPT-2', '(117x', 'increase)', 'and', 'training', 'it', 'consumed', 'several', 'thousand', 'petaflop/s-days', 'of', 'computing', 'power']), (0.018341965737122125, ['However,', 'many', 'questions', 'remain', 'about', 'how', 'exactly', 'this', 'will', 'be', 'executed', '—', 'pricing,', 'SLA,', 'model', 'latency,', 'etc']), (0.01740125573394835, ['His', 'results', 'showed', 'that', 'the', 'latter', 'had', '15%', 'greater', 'predictive', 'accuracy', 'after', 'training', 'both', 'with', 'the', 'same', 'amount', 'of', 'training', 'data']), (0.01694114063154223, ['For', 'example,', 'suppose', 'you', 'would', 'like', 'to', 'learn', 'a', 'new', 'language', '—', 'German']), (0.01687607847662407, ['To', 'quell', 'concerns,', 'OpenAI', 'has', 'repeatedly', 'stated', 'its', 'mission', 'to', 'produce', 'AI', 'for', 'the', 'good', 'of', 'humanity', 'and', 'aims', 'to', 'stop', 'access', 'to', 'its', 'API', 'if', 'misuse', 'is', 'detected']), (0.016543741031219375, ['Users', 'have', 'pointed', 'out', 'several', 'issues', 'that', 'need', 'to', 'be', 'addressed', 'before', 'widespread', 'commercial', 'use']), (0.016010705633440547, ['The', 'volume', 'of', 'data', 'and', 'computing', 'resources', 'required', 'makes', 'it', 'impossible', 'for', 'many', 'organizations', 'to', 'recreate', 'this,', 'but', 'luckily', 'they', 'won’t', 'have', 'to', 'since', 'OpenAI', 'plans', 'to', 'release', 'access', 'via', 'API', 'in', 'the', 'future']), (0.015675589812165467, ['Early', 'adopter', 'Kevin', 'Lacker', 'tested', 'the', 'model', 'with', 'a', 'Turing', 'test', 'and', 'saw', 'amazing', 'results']), (0.015035640253251851, ['The', 'reality', 'is,', 'you', 'are', 'still', 'indirectly', 'applying', 'learnings', 'about', 'sentence', 'structure,', 'language,', 'and', 'communication', 'from', 'the', 'previous', 'language', 'even', 'though', 'the', 'actual', 'words', 'and', 'grammar', 'are', 'different']), (0.014898892391498721, ['The', 'company', 'recently', 'received', '$1', 'billion', 'of', 'additional', 'funding', 'from', 'Microsoft', 'in', '2019', 'and', 'is', 'considered', 'a', 'leader', 'in', 'AI', 'research', 'and', 'development']), (0.014631133554097018, ['This', 'is', 'why', 'learning', 'new', 'languages', 'is', 'typically', 'easier', 'if', 'you', 'already', 'know', 'another', 'language']), (0.013579500483524742, ['In', '2018,', 'OpenAI', 'presented', 'convincing', 'research', 'showing', 'that', 'this', 'strategy', '(pairing', 'supervised', 'learning', 'with', 'unsupervised', 'pre-training)', 'is', 'particularly', 'very', 'effective', 'in', 'NLP', 'tasks']), (0.013476509410085775, ['In', 'this', 'article', 'I', 'will', 'provide', 'a', 'brief', 'overview', 'of', 'GPT', 'and', 'what', 'it', 'can', 'be', 'used', 'for']), (0.012846340842715022, ['Case', 'in', 'point:', 'it', 'was', 'trained', 'in', 'October', '2019', 'and', 'therefore', 'does', 'not', 'know', 'about', 'COVID-19']), (0.0126661817007592, ['As', 'an', 'analogy,', 'this', 'would', 'be', 'like', 'teaching', 'someone', 'English,', 'then', 'training', 'him', 'or', 'her', 'for', 'the', 'specific', 'task', 'of', 'reading', 'and', 'classifying', 'resumes', 'of', 'acceptable', 'and', 'unacceptable', 'candidates', 'for', 'hiring']), (0.011757155471448191, ['Critical', 'reception', 'Admittedly,', 'GPT-3', 'didn’t', 'get', 'much', 'attention', 'until', 'last', 'week’s', 'viral', 'tweets', 'by', 'Sharif', 'Shameem', 'and', 'others', '(above)']), (0.009160877301961736, ['Initially,', 'you', 'will', 'still', 'think', 'about', 'your', 'sentences', 'in', 'English,', 'then', 'translate', 'and', 'rearrange', 'words', 'to', 'come', 'up', 'with', 'the', 'German', 'equivalent']), (0.008786939873924439, ['Even', 'in', 'it’s', 'beta', 'access', 'form,', 'it', 'asks', 'candidates', 'to', 'describe', 'their', 'intentions', 'with', 'the', 'technology', 'and', 'the', 'benefits', 'and', 'risks', 'to', 'society']), (0.00852516545169246, ['It', 'is', 'unclear', 'how', 'these', 'texts', 'were', 'chosen', 'and', 'what', 'oversight', 'was', 'performed', '(or', 'required)', 'in', 'this', 'process']), (0.004741781531046327, ['Normally,', 'this', 'can', 'be', 'extremely', 'time', 'consuming', 'and', 'expensive']), (0.004048295813813047, ['Although', 'often', 'overlooked,', 'both', 'hardware', 'and', 'software', 'usage', 'significantly', 'contribute', 'to', 'depletion', 'of', 'energy', 'resources,', 'excessive', 'waste', 'generation,', 'and', 'excessive', 'mining', 'of', 'rare', 'earth', 'minerals', 'with', 'the', 'associated', 'negative', 'impacts', 'to', 'human', 'health']), (0.0030518819938963548, [''])]\n","\n","Summarize Text 0: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data\n","\n","Summarize Text 1: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task\n","\n","Summarize Text 2: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general)\n","\n","Summarize Text 3: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). OpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools\n","\n","Summarize Text 4: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). OpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools. In this great walkthrough, Francois Chollet compared the effectiveness of an AI model trained from scratch to one built from a pre-trained model\n","\n","Updated: ------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.04229507258112702, ['Applying', 'this', 'strategy', 'to', 'AI', 'means', 'that', 'we', 'can', 'use', 'pre-trained', 'models', 'to', 'create', 'new', 'models', 'more', 'quickly', 'with', 'less', 'training', 'data']), (0.03413848933830867, ['To', 'solve', 'this,', 'scientists', 'have', 'used', 'an', 'approach', 'called', 'transfer', 'learning:', 'use', 'the', 'existing', 'representations/information', 'learned', 'in', 'a', 'previously-trained', 'model', 'as', 'a', 'starting', 'point', 'to', 'fine-tune', 'and', 'train', 'a', 'new', 'model', 'for', 'a', 'different', 'task']), (0.0331916180902335, ['Historically,', 'obtaining', 'large', 'quantities', 'of', 'labelled', 'data', 'to', 'use', 'to', 'train', 'models', 'has', 'been', 'a', 'major', 'barrier', 'in', 'NLP', 'development', '(and', 'AI', 'development', 'in', 'general)']), (0.033142312522327765, ['OpenAI’s', 'GPT-3', 'language', 'model', 'gained', 'significant', 'attention', 'last', 'week,', 'leading', 'many', 'to', 'believe', 'that', 'the', 'new', 'technology', 'represents', 'a', 'significant', 'inflection', 'point', 'in', 'the', 'development', 'of', 'Natural', 'Language', 'Processing', '(NLP)', 'tools']), (0.03154812781456989, ['In', 'this', 'great', 'walkthrough,', 'Francois', 'Chollet', 'compared', 'the', 'effectiveness', 'of', 'an', 'AI', 'model', 'trained', 'from', 'scratch', 'to', 'one', 'built', 'from', 'a', 'pre-trained', 'model']), (0.03143381157292195, ['Next,', 'this', 'pre-trained', 'model', 'could', 'be', 'further', 'fine-tuned', 'and', 'trained', 'to', 'perform', 'specific', 'tasks', 'using', 'supervised', 'learning']), (0.030250210014887834, ['This', 'is', 'a', 'significant', 'step', 'forward', 'for', 'AI', 'development,', 'impressively', 'accomplished', 'in', 'just', 'a', 'two-year', 'time', 'frame', 'Early', 'tools', 'that', 'have', 'been', 'built', 'on', 'GPT-3', 'show', 'great', 'promise', 'for', 'commercial', 'usability', 'such', 'as:', 'no-code', 'platforms', 'that', 'allow', 'you', 'to', 'build', 'apps', 'by', 'describing', 'then;', 'advanced', 'search', 'platforms', 'using', 'plain', 'English;', 'and', 'better', 'data', 'analytics', 'tools', 'that', 'make', 'data', 'gathering', 'and', 'processing', 'much', 'faster', 'OpenAI', 'announced', 'plans', 'to', 'release', 'a', 'commercial', 'API,', 'which', 'will', 'enable', 'organizations', 'to', 'build', 'products', 'powered', 'by', 'GPT-3', 'at', 'scale']), (0.02786433423817768, ['GPT-3', 'is', 'the', 'latest', 'iteration', 'of', 'the', 'GPT', 'model', 'and', 'was', 'first', 'described', 'in', 'May', '2020']), (0.02625734467122521, ['What', 'is', 'OpenAI', 'and', 'GPT-3?', 'OpenAI', 'is', 'an', 'AI', 'research', 'laboratory', 'founded', 'in', '2015', 'by', 'Elon', 'Musk,', 'Sam', 'Altman,', 'and', 'others', 'with', 'the', 'mission', 'of', 'creating', 'AI', 'that', 'benefits', 'all', 'of', 'humanity']), (0.025660851147810762, ['Additionally,', 'the', 'enormous', 'computing', 'resources', 'required', 'to', 'produce', 'and', 'maintain', 'these', 'models', 'raise', 'serious', 'questions', 'about', 'the', 'environmental', 'impact', 'of', 'AI', 'technologies']), (0.025539444506008396, ['GPT-3', 'is', 'fed', 'with', 'much', 'more', 'data', 'and', 'tuned', 'with', 'more', 'parameters', 'than', 'GPT-2,', 'and', 'as', 'a', 'result,', 'it', 'has', 'produced', 'some', 'amazing', 'NLP', 'capabilities', 'so', 'far']), (0.0244256449216283, ['This', 'raises', 'a', 'broader', 'question', 'about', 'the', 'necessity', 'of', 'government', 'involvement', 'to', 'protect', 'the', 'rights', 'of', 'individuals', 'All', 'said,', 'I’m', 'extremely', 'excited', 'to', 'see', 'which', 'new', 'technologies', 'are', 'built', 'on', 'GPT-3', 'and', 'how', 'OpenAI', 'continues', 'to', 'improve', 'on', 'its', 'model']), (0.02427953214679208, ['Hype', 'over', 'GPT-3', 'reached', 'an', 'all-time', 'high', 'on', 'Twitter', 'over', 'the', 'weekend', 'and', 'many', 'are', 'calling', 'the', 'technological', 'development', 'a', 'groundbreaking', 'inflection', 'point', 'for', 'future', 'AI', 'research']), (0.024203077470516022, ['Where', 'do', 'we', 'go', 'from', 'here?', 'Without', 'a', 'doubt,', 'GPT-3', 'still', 'represents', 'a', 'major', 'milestone', 'in', 'AI', 'development']), (0.02382055294755228, ['In', 'summary:', 'GPT-3', 'is', 'a', 'major', 'improvement', 'upon', 'GPT-2', 'and', 'features', 'far', 'greater', 'accuracy', 'for', 'better', 'use', 'cases']), (0.02345700539040808, ['They', 'first', 'produced', 'a', 'generative', 'pre-trained', 'model', '(“GPT”)', 'using', '“a', 'diverse', 'corpus', 'of', 'unlabeled', 'text”', '(i.e']), (0.02333564919205295, ['Inherent', 'biases', 'in', 'the', 'model,', 'questions', 'around', 'fairness', 'and', 'ethics,', 'and', 'concerns', 'about', 'misuse', '(fake', 'news,', 'bots,', 'etc.)', 'need', 'to', 'be', 'thought', 'through', 'and', 'oversight', 'might', 'be', 'necessary', 'OpenAI', 'is', 'openly', 'committed', 'to', 'creating', 'AI', 'for', 'the', 'benefit', 'of', 'humanity,', 'but', 'still,', 'monitoring', 'for', 'misuse', 'at', 'scale', 'will', 'be', 'difficult', 'to', 'achieve']), (0.02318825003106307, ['They', 'demonstrated', 'that', 'GPT-3', 'could', 'be', 'used', 'to', 'create', 'websites', 'based', 'on', 'plain', 'English', 'instructions,', 'envisioning', 'a', 'new', 'era', 'of', 'no-code', 'technologies', 'where', 'people', 'can', 'create', 'apps', 'by', 'simply', 'describing', 'them', 'in', 'words']), (0.02239344128476017, ['Despite', 'the', 'shortfalls', 'of', 'the', 'model,', 'I', 'am', 'hoping', 'that', 'everyone', 'can', 'be', 'optimistic', 'about', 'a', 'future', 'where', 'humans', 'and', 'machines', 'will', 'communicate', 'with', 'each', 'other', 'in', 'a', 'unified', 'language', 'and', 'the', 'ability', 'to', 'create', 'tools', 'using', 'technology', 'will', 'be', 'accessible', 'to', 'billions', 'of', 'more', 'people']), (0.022378197369966792, ['Those', 'with', 'early', 'API', 'access', 'through', 'OpenAI’s', 'beta', 'program', 'went', 'to', 'Twitter', 'to', 'showcase', 'impressive', 'early', 'tools', 'built', 'using', 'GPT-3', 'technology:', 'For', 'non-engineers,', 'this', 'may', 'look', 'like', 'magic,', 'but', 'there', 'is', 'a', 'lot', 'to', 'be', 'unpacked', 'here']), (0.021487120990645623, ['Increased', 'attention', 'and', 'funding', 'in', 'NLP', 'and', 'GPT-3', 'might', 'be', 'enough', 'to', 'ward', 'off', 'fears', 'from', 'many', 'critics', 'that', 'an', 'AI', 'winter', 'might', 'be', 'coming', '(myself', 'included)']), (0.021472358374835646, ['over', '7,000', 'unique', 'unpublished', 'books', 'from', 'a', 'variety', 'of', 'genres),', 'essentially', 'creating', 'a', 'model', 'that', '“understood”', 'English', 'and', 'language']), (0.020830771927334384, ['However,', 'the', 'model', 'is', 'far', 'from', 'perfect']), (0.02060444971474681, ['Many', 'early', 'users', 'have', 'built', 'impressive', 'apps', 'that', 'accurately', 'process', 'natural', 'language', 'and', 'produce', 'amazing', 'results']), (0.020308874037556407, ['Several', 'users', 'have', 'reported', 'these', 'issues', 'on', 'Twitter', 'as', 'well:', 'OpenAI’s', 'blog', 'discusses', 'some', 'of', 'the', 'key', 'drawbacks', 'of', 'the', 'model,', 'most', 'notably', 'that', 'GPT’s', 'entire', 'understanding', 'of', 'the', 'world', 'is', 'based', 'on', 'the', 'texts', 'it', 'was', 'trained', 'on']), (0.01951641070540304, ['GPT-3', 'performed', 'exceptionally', 'well', 'in', 'the', 'initial', 'Q&A', 'and', 'displayed', 'many', 'aspects', 'of', '“common', 'sense”', 'that', 'AI', 'systems', 'traditionally', 'struggle', 'with']), (0.01949691358894819, ['Max', 'Woolf', 'performed', 'a', 'critical', 'analysis', 'noting', 'several', 'issues', 'such', 'as', 'model', 'latency,', 'implementation', 'issues,', 'and', 'concerning', 'biases', 'in', 'the', 'data', 'that', 'need', 'to', 'be', 're-considered']), (0.019276843730331422, ['In', 'this', 'article', 'I', 'explore', 'what', 'GPT', 'is,', 'what', 'it', 'means', 'for', 'AI', 'development,', 'and', 'where', 'we', 'might', 'be', 'headed', 'from', 'here']), (0.019206516548083096, ['It', 'contains', '175', 'billion', 'parameters', 'compared', 'to', 'the', '1.5', 'billion', 'in', 'GPT-2', '(117x', 'increase)', 'and', 'training', 'it', 'consumed', 'several', 'thousand', 'petaflop/s-days', 'of', 'computing', 'power']), (0.018341965737122125, ['However,', 'many', 'questions', 'remain', 'about', 'how', 'exactly', 'this', 'will', 'be', 'executed', '—', 'pricing,', 'SLA,', 'model', 'latency,', 'etc']), (0.01740125573394835, ['His', 'results', 'showed', 'that', 'the', 'latter', 'had', '15%', 'greater', 'predictive', 'accuracy', 'after', 'training', 'both', 'with', 'the', 'same', 'amount', 'of', 'training', 'data']), (0.01694114063154223, ['For', 'example,', 'suppose', 'you', 'would', 'like', 'to', 'learn', 'a', 'new', 'language', '—', 'German']), (0.01687607847662407, ['To', 'quell', 'concerns,', 'OpenAI', 'has', 'repeatedly', 'stated', 'its', 'mission', 'to', 'produce', 'AI', 'for', 'the', 'good', 'of', 'humanity', 'and', 'aims', 'to', 'stop', 'access', 'to', 'its', 'API', 'if', 'misuse', 'is', 'detected']), (0.016543741031219375, ['Users', 'have', 'pointed', 'out', 'several', 'issues', 'that', 'need', 'to', 'be', 'addressed', 'before', 'widespread', 'commercial', 'use']), (0.016010705633440547, ['The', 'volume', 'of', 'data', 'and', 'computing', 'resources', 'required', 'makes', 'it', 'impossible', 'for', 'many', 'organizations', 'to', 'recreate', 'this,', 'but', 'luckily', 'they', 'won’t', 'have', 'to', 'since', 'OpenAI', 'plans', 'to', 'release', 'access', 'via', 'API', 'in', 'the', 'future']), (0.015675589812165467, ['Early', 'adopter', 'Kevin', 'Lacker', 'tested', 'the', 'model', 'with', 'a', 'Turing', 'test', 'and', 'saw', 'amazing', 'results']), (0.015035640253251851, ['The', 'reality', 'is,', 'you', 'are', 'still', 'indirectly', 'applying', 'learnings', 'about', 'sentence', 'structure,', 'language,', 'and', 'communication', 'from', 'the', 'previous', 'language', 'even', 'though', 'the', 'actual', 'words', 'and', 'grammar', 'are', 'different']), (0.014898892391498721, ['The', 'company', 'recently', 'received', '$1', 'billion', 'of', 'additional', 'funding', 'from', 'Microsoft', 'in', '2019', 'and', 'is', 'considered', 'a', 'leader', 'in', 'AI', 'research', 'and', 'development']), (0.014631133554097018, ['This', 'is', 'why', 'learning', 'new', 'languages', 'is', 'typically', 'easier', 'if', 'you', 'already', 'know', 'another', 'language']), (0.013579500483524742, ['In', '2018,', 'OpenAI', 'presented', 'convincing', 'research', 'showing', 'that', 'this', 'strategy', '(pairing', 'supervised', 'learning', 'with', 'unsupervised', 'pre-training)', 'is', 'particularly', 'very', 'effective', 'in', 'NLP', 'tasks']), (0.013476509410085775, ['In', 'this', 'article', 'I', 'will', 'provide', 'a', 'brief', 'overview', 'of', 'GPT', 'and', 'what', 'it', 'can', 'be', 'used', 'for']), (0.012846340842715022, ['Case', 'in', 'point:', 'it', 'was', 'trained', 'in', 'October', '2019', 'and', 'therefore', 'does', 'not', 'know', 'about', 'COVID-19']), (0.0126661817007592, ['As', 'an', 'analogy,', 'this', 'would', 'be', 'like', 'teaching', 'someone', 'English,', 'then', 'training', 'him', 'or', 'her', 'for', 'the', 'specific', 'task', 'of', 'reading', 'and', 'classifying', 'resumes', 'of', 'acceptable', 'and', 'unacceptable', 'candidates', 'for', 'hiring']), (0.011757155471448191, ['Critical', 'reception', 'Admittedly,', 'GPT-3', 'didn’t', 'get', 'much', 'attention', 'until', 'last', 'week’s', 'viral', 'tweets', 'by', 'Sharif', 'Shameem', 'and', 'others', '(above)']), (0.009160877301961736, ['Initially,', 'you', 'will', 'still', 'think', 'about', 'your', 'sentences', 'in', 'English,', 'then', 'translate', 'and', 'rearrange', 'words', 'to', 'come', 'up', 'with', 'the', 'German', 'equivalent']), (0.008786939873924439, ['Even', 'in', 'it’s', 'beta', 'access', 'form,', 'it', 'asks', 'candidates', 'to', 'describe', 'their', 'intentions', 'with', 'the', 'technology', 'and', 'the', 'benefits', 'and', 'risks', 'to', 'society']), (0.00852516545169246, ['It', 'is', 'unclear', 'how', 'these', 'texts', 'were', 'chosen', 'and', 'what', 'oversight', 'was', 'performed', '(or', 'required)', 'in', 'this', 'process']), (0.004741781531046327, ['Normally,', 'this', 'can', 'be', 'extremely', 'time', 'consuming', 'and', 'expensive']), (0.004048295813813047, ['Although', 'often', 'overlooked,', 'both', 'hardware', 'and', 'software', 'usage', 'significantly', 'contribute', 'to', 'depletion', 'of', 'energy', 'resources,', 'excessive', 'waste', 'generation,', 'and', 'excessive', 'mining', 'of', 'rare', 'earth', 'minerals', 'with', 'the', 'associated', 'negative', 'impacts', 'to', 'human', 'health']), (0.0030518819938963548, [''])]\n","\n","Summarize Text 0: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data\n","\n","Summarize Text 1: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task\n","\n","Summarize Text 2: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general)\n","\n","Summarize Text 3: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). OpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools\n","\n","Summarize Text 4: \n","Applying this strategy to AI means that we can use pre-trained models to create new models more quickly with less training data. To solve this, scientists have used an approach called transfer learning: use the existing representations/information learned in a previously-trained model as a starting point to fine-tune and train a new model for a different task. Historically, obtaining large quantities of labelled data to use to train models has been a major barrier in NLP development (and AI development in general). OpenAI’s GPT-3 language model gained significant attention last week, leading many to believe that the new technology represents a significant inflection point in the development of Natural Language Processing (NLP) tools. In this great walkthrough, Francois Chollet compared the effectiveness of an AI model trained from scratch to one built from a pre-trained model\n","\n","\n","\n","Article a2: =======================================================================\n","\n","Old: ------------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.01360156817742775, ['We', 'need', 'to', 'score', 'each', 'word', 'of', 'the', 'input', 'sentence', 'against', 'this', 'word']), (0.012963361082404073, ['Image', 'from', '10', 'The', 'reason', 'why', 'Convolutional', 'Neural', 'Networks', 'can', 'work', 'in', 'parallel,', 'is', 'that', 'each', 'word', 'on', 'the', 'input', 'can', 'be', 'processed', 'at', 'the', 'same', 'time', 'and', 'does', 'not', 'necessarily', 'depend', 'on', 'the', 'previous', 'words', 'to', 'be', 'translated']), (0.012336562129130707, ['It', 'helps', 'the', 'encoder', 'look', 'at', 'other', 'words', 'in', 'the', 'input', 'sentence', 'as', 'it', 'encodes', 'a', 'specific', 'word']), (0.011959615233045935, ['However,', 'if', 'you', 'think', 'a', 'bit', 'more,', 'it', 'turns', 'out', 'that…', 'colah.github.io', 'With', 'a', 'cell', 'state,', 'the', 'information', 'in', 'a', 'sentence', 'that', 'is', 'important', 'for', 'translating', 'a', 'word', 'may', 'be', 'passed', 'from', 'one', 'word', 'to', 'another,', 'when', 'translating']), (0.011939721964870604, ['The', 'Recurrent', 'Neural', 'Network', 'passes', 'the', 'information', 'of', 'the', 'previous', 'words', 'to', 'the', 'next', 'network', 'that', 'can', 'use', 'and', 'process', 'that', 'information']), (0.011283824153068604, ['Image', 'from', '8', 'The', 'first', 'step', 'in', 'calculating', 'self-attention', 'is', 'to', 'create', 'three', 'vectors', 'from', 'each', 'of', 'the', 'encoder’s', 'input', 'vectors', '(in', 'this', 'case,', 'the', 'embedding', 'of', 'each', 'word)']), (0.010903713601070926, ['Another', 'problem', 'with', 'RNNs,', 'and', 'LSTMs,', 'is', 'that', 'it’s', 'hard', 'to', 'parallelize', 'the', 'work', 'for', 'processing', 'sentences,', 'since', 'you', 'are', 'have', 'to', 'process', 'word', 'by', 'word']), (0.010652467071534338, ['There', 'are', 'many', 'examples,', 'where', 'words', 'in', 'some', 'sentences', 'refer', 'to', 'words', 'in', 'previous', 'sentences']), (0.010436636640070067, ['When', 'translating', 'a', 'sentence,', 'I', 'pay', 'special', 'attention', 'to', 'the', 'word', 'I’m', 'presently', 'translating']), (0.010305409404919494, ['After', 'embedding', 'the', 'words', 'in', 'our', 'input', 'sequence,', 'each', 'of', 'them', 'flows', 'through', 'each', 'of', 'the', 'two', 'layers', 'of', 'the', 'encoder']), (0.01011908473665126, ['Instead', 'of', 'passing', 'only', 'one', 'hidden', 'state', 'to', 'the', 'decoders', 'as', 'we', 'did', 'before', 'using', 'attention,', 'we', 'pass', 'all', 'the', 'hidden', 'states', 'generated', 'by', 'every', '“word”', 'of', 'the', 'sentence', 'to', 'the', 'decoding', 'stage']), (0.010109957235885756, ['Clearly', 'the', 'word', 'at', 'this', 'position', 'will', 'have', 'the', 'highest', 'softmax', 'score,', 'but', 'sometimes', 'it’s', 'useful', 'to', 'attend', 'to', 'another', 'word', 'that', 'is', 'relevant', 'to', 'the', 'current', 'word']), (0.010094390009670358, ['In', 'this', 'case', 'where', 'the', 'difference', 'between', 'the', 'relevant', 'information', 'and', 'the', 'place', 'that', 'is', 'needed', 'is', 'small,', 'RNNs', 'can', 'learn', 'to', 'use', 'past', 'information', 'and', 'figure', 'out', 'what', 'is', 'the', 'next', 'word', 'for', 'this', 'sentence']), (0.010072207280216443, ['So', 'in', 'order', 'for', 'the', 'decoding', 'to', 'be', 'precise,', 'it', 'needs', 'to', 'take', 'into', 'account', 'every', 'word', 'of', 'the', 'input,', 'using', 'attention']), (0.009994246359956542, ['Internally,', 'a', 'LSTM', 'looks', 'like', 'the', 'following:', 'Image', 'from', '6', 'Each', 'cell', 'takes', 'as', 'inputs', 'x_t', '(a', 'word', 'in', 'the', 'case', 'of', 'a', 'sentence', 'to', 'sentence', 'translation),', 'the', 'previous', 'cell', 'state', 'and', 'the', 'output', 'of', 'the', 'previous', 'cell']), (0.009767373632668644, ['The', 'idea', 'behind', 'it', 'is', 'that', 'whenever', 'you', 'are', 'translating', 'a', 'word,', 'you', 'may', 'pay', 'different', 'attention', 'to', 'each', 'word', 'based', 'on', 'the', 'type', 'of', 'question', 'that', 'you', 'are', 'asking']), (0.00954930416465265, ['The', 'score', 'determines', 'how', 'much', 'focus', 'to', 'place', 'on', 'other', 'parts', 'of', 'the', 'input', 'sentence', 'as', 'we', 'encode', 'a', 'word', 'at', 'a', 'certain', 'position']), (0.009455506483460144, ['If', 'you', 'want', 'to', 'understand', 'in', 'depth', 'how', 'the', 'model', 'works', 'and', 'all', 'its', 'nuances,', 'I', 'recommend', 'the', 'following', 'posts,', 'articles', 'and', 'videos', 'that', 'I', 'used', 'as', 'a', 'base', 'for', 'summarizing', 'the', 'technique', 'The', 'Unreasonable', 'Effectiveness', 'of', 'Recurrent', 'Neural', 'Networks', 'Understanding', 'LSTM', 'Networks', 'Visualizing', 'A', 'Neural', 'Machine', 'Translation', 'Model', 'The', 'Illustrated', 'Transformer', 'The', 'Transformer', '—', 'Attention', 'is', 'all', 'you', 'need', 'The', 'Annotated', 'Transformer', 'Attention', 'is', 'all', 'you', 'need', 'attentional', 'neural', 'network', 'models', 'Self-Attention', 'For', 'Generative', 'Models', 'OpenAI', 'GPT-2:', 'Understanding', 'Language', 'Generation', 'through', 'Visualization', 'WaveNet:', 'A', 'Generative', 'Model', 'for', 'Raw', 'Audio', '']), (0.009440573377883739, ['For', 'example,', 'when', 'translating', 'the', 'sentence', '“Je', 'suis', 'étudiant”', 'to', 'English,', 'requires', 'that', 'the', 'decoding', 'step', 'looks', 'at', 'different', 'words', 'when', 'translating', 'it']), (0.009420059519648243, ['We', 'end', 'up', 'creating', 'a', '“query”,', 'a', '“key”,', 'and', 'a', '“value”', 'projection', 'of', 'each', 'word', 'in', 'the', 'input', 'sentence']), (0.009252123502921534, ['Image', 'taken', 'from', '4', 'Each', 'word', 'is', 'embedded', 'into', 'a', 'vector', 'of', 'size', '512']), (0.009061588206343915, ['For', 'example,', 'processing', 'inputs', '(words)', 'in', 'parallel', 'is', 'not', 'possible']), (0.00901428190574349, ['This', 'produces', 'the', 'output', 'of', 'the', 'self-attention', 'layer', 'at', 'this', 'position', '(for', 'the', 'first', 'word)']), (0.008995308298766073, ['Figuring', 'out', 'relation', 'of', 'words', 'within', 'a', 'sentence', 'and', 'giving', 'the', 'right', 'attention', 'to', 'it']), (0.008938032126991347, ['Image', 'from', '4', 'Here', 'we', 'begin', 'to', 'see', 'one', 'key', 'property', 'of', 'the', 'Transformer,', 'which', 'is', 'that', 'the', 'word', 'in', 'each', 'position', 'flows', 'through', 'its', 'own', 'path', 'in', 'the', 'encoder']), (0.008922866192695807, ['Each', 'word', 'is', 'processed', 'separately,', 'and', 'the', 'resulting', 'sentence', 'is', 'generated', 'by', 'passing', 'a', 'hidden', 'state', 'to', 'the', 'decoding', 'stage', 'that,', 'then,', 'generates', 'the', 'output']), (0.008849469359372964, ['GIF', 'from', '3', 'The', 'idea', 'behind', 'it', 'is', 'that', 'there', 'might', 'be', 'relevant', 'information', 'in', 'every', 'word', 'in', 'a', 'sentence']), (0.008751711546596727, ['To', 'solve', 'these', 'problems,', 'Attention', 'is', 'a', 'technique', 'that', 'is', 'used', 'in', 'a', 'neural', 'network']), (0.008693965376717568, ['Encoding', 'the', 'position', 'of', 'each', 'word', 'is', 'relevant,', 'since', 'the', 'position', 'of', 'each', 'word', 'is', 'relevant', 'to', 'the', 'translation']), (0.008684168412113311, ['For', 'RNNs,', 'instead', 'of', 'only', 'encoding', 'the', 'whole', 'sentence', 'in', 'a', 'hidden', 'state,', 'each', 'word', 'has', 'a', 'corresponding', 'hidden', 'state', 'that', 'is', 'passed', 'all', 'the', 'way', 'to', 'the', 'decoding', 'stage']), (0.008584061720384792, ['Transformers', 'To', 'solve', 'the', 'problem', 'of', 'parallelization,', 'Transformers', 'try', 'to', 'solve', 'the', 'problem', 'by', 'using', 'Convolutional', 'Neural', 'Networks', 'together', 'with', 'attention', 'models']), (0.008572896989296213, ['Say', 'we’re', 'calculating', 'the', 'self-attention', 'for', 'the', 'first', 'word', 'in', 'this', 'example,', '“Thinking”']), (0.008539485068662245, ['The', 'following', 'picture', 'shows', 'how', 'usually', 'a', 'sequence', 'to', 'sequence', 'model', 'works', 'using', 'Recurrent', 'Neural', 'Networks']), (0.008431672831206993, ['Or', 'ask', 'other', 'questions,', 'like', '“Did', 'what?”,', 'etc…', 'Images', 'from', '8', 'Positional', 'Encoding', 'Another', 'important', 'step', 'on', 'the', 'Transformer', 'is', 'to', 'add', 'positional', 'encoding', 'when', 'encoding', 'each', 'word']), (0.008426775506992624, ['If', 'we', 'are', 'trying', 'to', 'predict', 'the', 'next', 'word', 'of', 'the', 'sentence', '“the', 'clouds', 'in', 'the', 'sky”,', 'we', 'don’t', 'need', 'further', 'context']), (0.008386802879941332, ['Recurrent', 'Neural', 'Networks', '(RNNs)', 'and', 'Convolutional', 'Neural', 'Networks', '(CNNs)', 'have', 'been', 'used', 'to', 'deal', 'with', 'this', 'problem', 'because', 'of', 'their', 'properties']), (0.008377561787870684, ['Each', 'hidden', 'state', 'is', 'used', 'in', 'the', 'decoding', 'stage,', 'to', 'figure', 'out', 'where', 'the', 'network', 'should', 'pay', 'attention', 'to']), (0.008200240629538703, ['Neural', 'networks', 'can', 'achieve', 'this', 'same', 'behavior', 'using', 'attention,', 'focusing', 'on', 'part', 'of', 'a', 'subset', 'of', 'the', 'information', 'they', 'are', 'given']), (0.00819379981793236, ['As', 'is', 'the', 'case', 'in', 'NLP', 'applications', 'in', 'general,', 'we', 'begin', 'by', 'turning', 'each', 'input', 'word', 'into', 'a', 'vector', 'using', 'an', 'embedding', 'algorithm']), (0.008173647088304214, ['So', 'if', 'we’re', 'processing', 'the', 'self-attention', 'for', 'the', 'word', 'in', 'position', '#1,', 'the', 'first', 'score', 'would', 'be', 'the', 'dot', 'product', 'of', 'q1', 'and', 'k1']), (0.00816844662422643, ['To', 'summarize,', 'LSTMs', 'and', 'RNNs', 'present', '3', 'problems:', 'Sequential', 'computation', 'inhibits', 'parallelization', 'No', 'explicit', 'modeling', 'of', 'long', 'and', 'short', 'range', 'dependencies', '“Distance”', 'between', 'positions', 'is', 'linear', 'Attention', 'To', 'solve', 'some', 'of', 'these', 'problems,', 'researchers', 'created', 'a', 'technique', 'for', 'paying', 'attention', 'to', 'specific', 'words']), (0.007948348721696823, ['GIF', 'from', '3', 'The', 'problem', 'of', 'long-term', 'dependencies', 'Consider', 'a', 'language', 'model', 'that', 'is', 'trying', 'to', 'predict', 'the', 'next', 'word', 'based', 'on', 'the', 'previous', 'ones']), (0.007910464213576354, ['The', 'intuition', 'here', 'is', 'to', 'keep', 'intact', 'the', 'values', 'of', 'the', 'word(s)', 'we', 'want', 'to', 'focus', 'on,', 'and', 'drown-out', 'irrelevant', 'words', '(by', 'multiplying', 'them', 'by', 'tiny', 'numbers', 'like', '0.001,', 'for', 'example)']), (0.007866424593087309, ['In', 'that', 'way,', 'if', 'we', 'want', 'to', 'translate', 'some', 'text,', 'we', 'can', 'set', 'each', 'input', 'as', 'the', 'word', 'in', 'that', 'text']), (0.0076825831557531, ['Not', 'only', 'that,', 'but', 'the', '“distance”', 'between', 'the', 'output', 'word', 'and', 'any', 'input', 'for', 'a', 'CNN', 'is', 'in', 'the', 'order', 'of', 'log(N)', '—', 'that', 'is', 'the', 'size', 'of', 'the', 'height', 'of', 'the', 'tree', 'generated', 'from', 'the', 'output', 'to', 'the', 'input', '(you', 'can', 'see', 'it', 'on', 'the', 'GIF', 'above']), (0.00767460830240553, ['Image', 'from', '4', 'This', 'softmax', 'score', 'determines', 'how', 'much', 'how', 'much', 'each', 'word', 'will', 'be', 'expressed', 'at', 'this', 'position']), (0.007603734881586363, ['Image', 'from', '4', 'The', 'encoder’s', 'inputs', 'first', 'flow', 'through', 'a', 'self-attention', 'layer']), (0.007588210788570523, ['The', 'problem', 'is', 'that', 'Convolutional', 'Neural', 'Networks', 'do', 'not', 'necessarily', 'help', 'with', 'the', 'problem', 'of', 'figuring', 'out', 'the', 'problem', 'of', 'dependencies', 'when', 'translating', 'sentences']), (0.007469891970952932, ['The', 'reason', 'for', 'that', 'is', 'that', 'the', 'probability', 'of', 'keeping', 'the', 'context', 'from', 'a', 'word', 'that', 'is', 'far', 'away', 'from', 'the', 'current', 'word', 'being', 'processed', 'decreases', 'exponentially', 'with', 'the', 'distance', 'from', 'it']), (0.007347259948755914, ['With', 'them', 'we', 'can', 'Trivial', 'to', 'parallelize', '(per', 'layer)', 'Exploits', 'local', 'dependencies', 'Distance', 'between', 'positions', 'is', 'logarithmic', 'Some', 'of', 'the', 'most', 'popular', 'neural', 'networks', 'for', 'sequence', 'transduction,', 'Wavenet', 'and', 'Bytenet,', 'are', 'Convolutional', 'Neural', 'Networks']), (0.007296991842765063, ['The', 'input', 'is', 'represented', 'as', 'x_t', 'In', 'the', 'figure', 'above,', 'we', 'see', 'part', 'of', 'the', 'neural', 'network,', 'A,', 'processing', 'some', 'input', 'x_t', 'and', 'outputs', 'h_t']), (0.0072694423661273004, ['Image', 'from', '3', 'But', 'some', 'of', 'the', 'problems', 'that', 'we', 'discussed,', 'still', 'are', 'not', 'solved', 'with', 'RNNs', 'using', 'attention']), (0.007268943674119861, ['Recurrent', 'Neural', 'Networks', 'Recurrent', 'Neural', 'Networks', 'have', 'loops', 'in', 'them,', 'allowing', 'information', 'to', 'persist']), (0.007221947721736785, ['The', 'band', 'was', 'formed', 'in', '1968,', 'during', 'the', 'height', 'of', 'Japanese', 'music', 'history”', 'In', 'this', 'example,', 'the', 'word', '“the', 'band”', 'in', 'the', 'second', 'sentence', 'refers', 'to', 'the', 'band', '“The', 'Transformers”', 'introduced', 'in', 'the', 'first', 'sentence']), (0.007201891657266166, ['Transformer', 'is', 'a', 'model', 'that', 'uses', 'attention', 'to', 'boost', 'the', 'speed']), (0.00715021176973635, ['The', 'score', 'is', 'calculated', 'by', 'taking', 'the', 'dot', 'product', 'of', 'the', 'query', 'vector', 'with', 'the', 'key', 'vector', 'of', 'the', 'respective', 'word', 'we’re', 'scoring']), (0.007016951821802244, ['GIF', 'from', '3', 'Or', 'for', 'example,', 'when', 'you', 'translate', 'the', 'sentence', '“L’accord', 'sur', 'la', 'zone', 'économique', 'européenne', 'a', 'été', 'signé', 'en', 'août', '1992.”', 'from', 'French', 'to', 'English,', 'and', 'how', 'much', 'attention', 'it', 'is', 'paid', 'to', 'each', 'input']), (0.007005142154586975, ['Image', 'taken', 'from', '4', 'Multiplying', 'x1', 'by', 'the', 'WQ', 'weight', 'matrix', 'produces', 'q1,', 'the', '“query”', 'vector', 'associated', 'with', 'that', 'word']), (0.006930253216594185, ['For', 'attention', 'to', 'be', 'brought', 'to', 'RNNs', 'in', 'sequence', 'transduction,', 'we', 'divide', 'the', 'encoding', 'and', 'decoding', 'into', '2', 'main', 'steps']), (0.0069186900722803205, ['Then,', 'the', 'hidden', 'states', 'are', 'used', 'at', 'each', 'step', 'of', 'the', 'RNN', 'to', 'decode']), (0.006874272544771056, ['For', 'example,', 'instead', 'of', 'only', 'paying', 'attention', 'to', 'each', 'other', 'in', 'one', 'dimension,', 'Transformers', 'use', 'the', 'concept', 'of', 'Multihead', 'attention']), (0.006857771437965554, ['For', 'translating', 'sentences', 'like', 'that,', 'a', 'model', 'needs', 'to', 'figure', 'out', 'these', 'sort', 'of', 'dependencies', 'and', 'connections']), (0.006840586677517464, ['Each', 'encoder', 'consists', 'of', 'two', 'layers:', 'Self-attention', 'and', 'a', 'feed', 'Forward', 'Neural', 'Network']), (0.006780919526094156, ['Convolutional', 'Neural', 'Networks', 'Convolutional', 'Neural', 'Networks', 'help', 'solve', 'these', 'problems']), (0.006775464402294607, ['Attention', 'boosts', 'the', 'speed', 'of', 'how', 'fast', 'the', 'model', 'can', 'translate', 'from', 'one', 'sequence', 'to', 'another']), (0.006595897152120655, ['It’s', 'pretty', 'obvious', 'that', 'the', 'next', 'word', 'is', 'going', 'to', 'be', 'sky']), (0.006589966832489746, ['Wavenet,', 'model', 'is', 'a', 'Convolutional', 'Neural', 'Network', '(CNN)']), (0.006520069665875203, ['The', 'decoder', 'has', 'both', 'those', 'layers,', 'but', 'between', 'them', 'is', 'an', 'attention', 'layer', 'that', 'helps', 'the', 'decoder', 'focus', 'on', 'relevant', 'parts', 'of', 'the', 'input', 'sentence']), (0.006506341520381835, ['Self-Attention', 'Let’s', 'first', 'look', 'at', 'how', 'to', 'calculate', 'self-attention', 'using', 'vectors,', 'then', 'proceed', 'to', 'look', 'at', 'how', 'it’s', 'actually', 'implemented', '—', 'using', 'matrices']), (0.006487739106596569, ['Consider', 'what', 'happens', 'if', 'we', 'unroll', 'the', 'loop:', 'An', 'unrolled', 'recurrent', 'neural', 'network', 'This', 'chain-like', 'nature', 'shows', 'that', 'recurrent', 'neural', 'networks', 'are', 'clearly', 'related', 'to', 'sequences', 'and', 'lists']), (0.006471733811018462, ['GIF', 'from', '3', 'The', 'step', 'in', 'green', 'in', 'charge', 'of', 'creating', 'the', 'hidden', 'states', 'from', 'the', 'input']), (0.006451488355745262, ['So', 'for', 'each', 'word,', 'we', 'create', 'a', 'Query', 'vector,', 'a', 'Key', 'vector,', 'and', 'a', 'Value', 'vector']), (0.006394958676573792, ['A', 'Recurrent', 'Neural', 'Network', 'can', 'be', 'thought', 'of', 'as', 'multiple', 'copies', 'of', 'the', 'same', 'network,', 'A,', 'each', 'network', 'passing', 'a', 'message', 'to', 'a', 'successor']), (0.006377473451093033, ['So', 'let’s', 'look', 'at', 'that', 'now', 'that', 'we’ve', 'seen', 'the', 'intuition', 'of', 'the', 'calculation', 'on', 'the', 'word', 'level']), (0.0063552839684501115, ['Recent', 'information', 'suggests', 'that', 'the', 'next', 'word', 'is', 'probably', 'a', 'language,', 'but', 'if', 'we', 'want', 'to', 'narrow', 'down', 'which', 'language,', 'we', 'need', 'context', 'of', 'France,', 'that', 'is', 'further', 'back', 'in', 'the', 'text']), (0.006351698382884842, ['In', 'the', 'bottom', 'encoder', 'that', 'would', 'be', 'the', 'word', 'embeddings,', 'but', 'in', 'other', 'encoders,', 'it', 'would', 'be', 'the', 'output', 'of', 'the', 'encoder', 'that’s', 'directly', 'below']), (0.006351330899483612, ['Image', 'from', '4', 'The', 'third', 'and', 'forth', 'steps', 'are', 'to', 'divide', 'the', 'scores', 'by', '8', '(the', 'square', 'root', 'of', 'the', 'dimension', 'of', 'the', 'key', 'vectors', 'used', 'in', 'the', 'paper', '—', '64']), (0.006346792352434351, ['Depending', 'on', 'the', 'answer,', 'the', 'translation', 'of', 'the', 'word', 'to', 'another', 'language', 'can', 'change']), (0.006184893955354517, ['For', 'example', 'let’s', 'say', 'that', 'we', 'are', 'translating', 'the', 'following', 'sentence', 'to', 'another', 'language', '(French):', '“The', 'Transformers”', 'are', 'a', 'Japanese', '[[hardcore', 'punk]]', 'band']), (0.0061561666713323285, ['Transformers', 'were', 'developed', 'to', 'solve', 'the', 'problem', 'of', 'sequence', 'transduction,', 'or', 'neural', 'machine', 'translation']), (0.006153361260002699, ['The', 'darker', 'the', 'color', 'is,', 'the', 'more', 'weight', 'is', 'associated', 'to', 'each', 'word']), (0.006089303371120487, ['Image', 'from', '4', 'Self-Attention', 'Note:', 'This', 'section', 'comes', 'from', 'Jay', 'Allamar', 'blog', 'post', 'Let’s', 'start', 'to', 'look', 'at', 'the', 'various', 'vectors/tensors', 'and', 'how', 'they', 'flow', 'between', 'these', 'components', 'to', 'turn', 'the', 'input', 'of', 'a', 'trained', 'model', 'into', 'an', 'output']), (0.006026812323443973, ['For', 'example,', 'let’s', 'say', 'that', 'you', 'are', 'trying', 'to', 'predict', 'the', 'last', 'word', 'of', 'the', 'text:', '“I', 'grew', 'up', 'in', 'France…', 'I', 'speak', 'fluent', '…”']), (0.006015919937324221, ['This', 'gif', 'shows', 'how', 'the', 'weight', 'that', 'is', 'given', 'to', 'each', 'hidden', 'state', 'when', 'translating', 'the', 'sentence', '“Je', 'suis', 'étudiant”', 'to', 'English']), (0.005989220585149477, ['Image', 'from', '4', 'Internally,', 'the', 'Transformer', 'has', 'a', 'similar', 'kind', 'of', 'architecture', 'as', 'the', 'previous', 'models', 'above']), (0.005930810586467195, ['That', 'means', 'that', 'when', 'sentences', 'are', 'long,', 'the', 'model', 'often', 'forgets', 'the', 'content', 'of', 'distant', 'positions', 'in', 'the', 'sequence']), (0.005893510136395593, ['If', 'you', 'want', 'to', 'understand', 'how', 'each', 'cell', 'works,', 'I', 'recommend', 'Christopher’s', 'blog', 'post:', 'Understanding', 'LSTM', 'Networks', '--', \"colah's\", 'blog', 'These', 'loops', 'make', 'recurrent', 'neural', 'networks', 'seem', 'kind', 'of', 'mysterious']), (0.005812145370598573, ['The', 'resulting', 'vector', 'is', 'one', 'we', 'can', 'send', 'along', 'to', 'the', 'feed-forward', 'neural', 'network']), (0.005795207739529156, ['A', 'loop', 'allows', 'information', 'to', 'be', 'passed', 'from', 'one', 'step', 'to', 'the', 'next']), (0.0053369182598711215, ['That', 'means', 'any', 'task', 'that', 'transforms', 'an', 'input', 'sequence', 'to', 'an', 'output', 'sequence']), (0.00533191919505996, ['Next,', 'we’ll', 'switch', 'up', 'the', 'example', 'to', 'a', 'shorter', 'sentence', 'and', 'we’ll', 'look', 'at', 'what', 'happens', 'in', 'each', 'sub-layer', 'of', 'the', 'encoder']), (0.005296532613576682, ['when', 'sentences', 'are', 'too', 'long', 'LSTMs', 'still', 'don’t', 'do', 'too', 'well']), (0.005290871894081272, ['What', 'are', 'the', '“query”,', '“key”,', 'and', '“value”', 'vectors?', 'They’re', 'abstractions', 'that', 'are', 'useful', 'for', 'calculating', 'and', 'thinking', 'about', 'attention']), (0.005252201812477734, ['The', 'second', 'step', 'in', 'calculating', 'self-attention', 'is', 'to', 'calculate', 'a', 'score']), (0.005224752154321493, ['Image', 'from', '4', 'Each', 'encoder', 'is', 'very', 'similar', 'to', 'each', 'other']), (0.00516776524244333, ['Overview', 'I', 'gave', 'an', 'overview', 'of', 'how', 'Transformers', 'work', 'and', 'why', 'this', 'is', 'the', 'technique', 'used', 'for', 'sequence', 'transduction']), (0.0051227919388096375, ['It', 'manipulates', 'these', 'inputs', 'and', 'based', 'on', 'them,', 'it', 'generates', 'a', 'new', 'cell', 'state,', 'and', 'an', 'output']), (0.005005581217095572, ['The', 'green', 'step', 'is', 'called', 'the', 'encoding', 'stage', 'and', 'the', 'purple', 'step', 'is', 'the', 'decoding', 'stage']), (0.005005581217095572, ['The', 'green', 'step', 'is', 'called', 'the', 'encoding', 'stage', 'and', 'the', 'purple', 'step', 'is', 'the', 'decoding', 'stage']), (0.004979907233686077, ['More', 'specifically,', 'it', 'uses', 'self-attention']), (0.004898925924242562, ['With', 'LSTMs,', 'the', 'information', 'flows', 'through', 'a', 'mechanism', 'known', 'as', 'cell', 'states']), (0.004887757795349091, ['Multihead', 'attention', 'Transformers', 'basically', 'work', 'like', 'that']), (0.004804374644796877, ['The', 'fifth', 'step', 'is', 'to', 'multiply', 'each', 'value', 'vector', 'by', 'the', 'softmax', 'score', '(in', 'preparation', 'to', 'sum', 'them', 'up)']), (0.004783611206187308, ['People', 'say', \"'It's\", 'as', 'plain', 'as', 'the', 'nose', 'on', 'your', \"face.'…\", 'www.holloway.com', 'Transformers', 'are', 'a', 'type', 'of', 'neural', 'network', 'architecture', 'that', 'have', 'been', 'gaining', 'popularity']), (0.00472127324715663, ['The', 'input', 'is', 'represented', 'in', 'green,', 'the', 'model', 'is', 'represented', 'in', 'blue,', 'and', 'the', 'output', 'is', 'represented', 'in', 'purple']), (0.004668219888037754, ['Image', 'from', '4', 'That', 'concludes', 'the', 'self-attention', 'calculation']), (0.004628706842449632, ['Image', 'from', '6', 'RNNs', 'become', 'very', 'ineffective', 'when', 'the', 'gap', 'between', 'the', 'relevant', 'information', 'and', 'the', 'point', 'where', 'it', 'is', 'needed', 'become', 'very', 'large']), (0.004598370202809837, ['That', 'is', 'much', 'better', 'than', 'the', 'distance', 'of', 'the', 'output', 'of', 'a', 'RNN', 'and', 'an', 'input,', 'which', 'is', 'on', 'the', 'order', 'of', 'N']), (0.0045930129020198985, ['At', 'every', 'time', 'step,', 'it', 'focuses', 'on', 'different', 'positions', 'in', 'the', 'other', 'RNN']), (0.004529056559463237, ['The', 'sixth', 'step', 'is', 'to', 'sum', 'up', 'the', 'weighted', 'value', 'vectors']), (0.004514212869203724, ['Let’s', 'take', 'a', 'look', 'at', 'how', 'Transformer', 'works']), (0.0045120268415122285, ['GIF', 'from', '3', 'For', 'models', 'to', 'perform', 'sequence', 'transduction,', 'it', 'is', 'necessary', 'to', 'have', 'some', 'sort', 'of', 'memory']), (0.004495789787599886, ['Transformers', 'were', 'recently', 'used', 'by', 'OpenAI', 'in', 'their', 'language', 'models,', 'and', 'also', 'used', 'recently', 'by', 'DeepMind', 'for', 'AlphaStar', '—', 'their', 'program', 'to', 'defeat', 'a', 'top', 'professional', 'Starcraft', 'player']), (0.004445628424087723, ['Once', 'you', 'proceed', 'with', 'reading', 'how', 'attention', 'is', 'calculated', 'below,', 'you’ll', 'know', 'pretty', 'much', 'all', 'you', 'need', 'to', 'know', 'about', 'the', 'role', 'each', 'of', 'these', 'vectors', 'plays']), (0.0043886333824598705, ['One', 'step', 'is', 'represented', 'in', 'green', 'and', 'the', 'other', 'in', 'purple']), (0.004321029110317015, ['If', 'you', 'liked', 'this', 'post', 'and', 'want', 'to', 'learn', 'how', 'machine', 'learning', 'algorithms', 'work,', 'how', 'did', 'they', 'arise,', 'and', 'where', 'are', 'they', 'going,', 'I', 'recommend', 'the', 'following:', 'Making', 'Things', 'Think:', 'How', 'AI', 'and', 'Deep', 'Learning', 'Power', 'the', 'Products', 'We', 'Use', '-', 'Holloway', 'It', 'is', 'the', 'obvious', 'which', 'is', 'so', 'difficult', 'to', 'see', 'most', 'of', 'the', 'time']), (0.00431975566583481, ['Image', 'from', '6', 'But', 'there', 'are', 'cases', 'where', 'we', 'need', 'more', 'context']), (0.004305135877528992, ['That’s', 'why', 'Transformers', 'were', 'created,', 'they', 'are', 'a', 'combination', 'of', 'both', 'CNNs', 'with', 'attention']), (0.0042565771578163755, ['There', 'are', 'dependencies', 'between', 'these', 'paths', 'in', 'the', 'self-attention', 'layer']), (0.004250005699350652, ['When', 'you', 'read', 'about', 'the', 'band', 'in', 'the', 'second', 'sentence,', 'you', 'know', 'that', 'it', 'is', 'referencing', 'to', 'the', '“The', 'Transformers”', 'band']), (0.004216410918890298, ['For', 'example,', 'whenever', 'you', 'are', 'translating', '“kicked”', 'in', 'the', 'sentence', '“I', 'kicked', 'the', 'ball”,', 'you', 'may', 'ask', '“Who', 'kicked”']), (0.004147702806061595, ['The', 'problem', 'with', 'LSTMs', 'The', 'same', 'problem', 'that', 'happens', 'to', 'RNNs', 'generally,', 'happen', 'with', 'LSTMs,', 'i.e']), (0.004125261353216669, ['That', 'is', 'due', 'to', 'the', 'fact', 'that', 'the', 'information', 'is', 'passed', 'at', 'each', 'step', 'and', 'the', 'longer', 'the', 'chain', 'is,', 'the', 'more', 'probable', 'the', 'information', 'is', 'lost', 'along', 'the', 'chain']), (0.00399445411860705, ['The', 'images', 'below', 'show', 'what', 'that', 'means']), (0.003967062781191181, ['Translating', 'the', 'sentence', '“L’accord', 'sur', 'la', 'zone', 'économique', 'européenne', 'a', 'été', 'signé', 'en', 'août', '1992.”', 'to', 'English']), (0.003866449980310768, ['They', 'don’t', 'HAVE', 'to', 'be', 'smaller,', 'this', 'is', 'an', 'architecture', 'choice', 'to', 'make', 'the', 'computation', 'of', 'multiheaded', 'attention', '(mostly)', 'constant']), (0.003827890588975816, ['Notice', 'that', 'these', 'new', 'vectors', 'are', 'smaller', 'in', 'dimension', 'than', 'the', 'embedding', 'vector']), (0.003675334876481451, ['For', 'example,', 'an', 'RNN', 'can', 'attend', 'over', 'the', 'output', 'of', 'another', 'RNN']), (0.003631128478135189, ['These', 'vectors', 'are', 'created', 'by', 'multiplying', 'the', 'embedding', 'by', 'three', 'matrices', 'that', 'we', 'trained', 'during', 'the', 'training', 'process']), (0.003621354764996563, ['The', 'entire', 'information', 'is', 'modified,', 'and', 'there', 'is', 'no', 'consideration', 'of', 'what', 'is', 'important', 'and', 'what', 'is', 'not']), (0.0035949892649106674, ['Sequence', 'transduction']), (0.003535578792014722, ['Not', 'only', 'that', 'but', 'there', 'is', 'no', 'model', 'of', 'long', 'and', 'short', 'range', 'dependencies']), (0.003491987594298868, ['LSTM,', 'a', 'special', 'type', 'of', 'RNN,', 'tries', 'to', 'solve', 'this', 'kind', 'of', 'problem']), (0.003442235433604755, ['The', 'embedding', 'only', 'happens', 'in', 'the', 'bottom-most', 'encoder']), (0.0033737257404968584, ['Whenever', 'it', 'adds', 'new', 'information,', 'it', 'transforms', 'existing', 'information', 'completely', 'by', 'applying', 'a', 'function']), (0.0033573981203933213, ['LSTMs', 'make', 'small', 'modifications', 'to', 'the', 'information', 'by', 'multiplications', 'and', 'additions']), (0.00334072154718154, ['Their', 'dimensionality', 'is', '64,', 'while', 'the', 'embedding', 'and', 'encoder', 'input/output', 'vectors', 'have', 'dimensionality', 'of', '512']), (0.0032878514292511746, ['RNNs', 'don’t', 'do', 'that']), (0.0032717637390706194, ['In', 'this', 'way,', 'LSTMs', 'can', 'selectively', 'remember', 'or', 'forget', 'things', 'that', 'are', 'important', 'and', 'not', 'so', 'important']), (0.0032454880594193903, ['In', 'theory,', 'RNNs', 'could', 'learn', 'this', 'long-term', 'dependencies']), (0.0030484233372887024, ['The', 'feed-forward', 'layer', 'does', 'not', 'have', 'those', 'dependencies,', 'however,', 'and', 'thus', 'the', 'various', 'paths', 'can', 'be', 'executed', 'in', 'parallel', 'while', 'flowing', 'through', 'the', 'feed-forward', 'layer']), (0.00303473671216365, ['That', 'may', 'be', 'important', 'for', 'translation']), (0.0030338629919541725, ['The', 'following', 'gif', 'shows', 'how', 'that', 'happens']), (0.002919975297956694, ['But', 'the', 'Transformer', 'consists', 'of', 'six', 'encoders', 'and', 'six', 'decoders']), (0.0029057866393564996, ['We’ll', 'represent', 'those', 'vectors', 'with', 'these', 'simple', 'boxes']), (0.002827524054951756, ['The', 'abstraction', 'that', 'is', 'common', 'to', 'all', 'the', 'encoders', 'is', 'that', 'they', 'receive', 'a', 'list', 'of', 'vectors', 'each', 'of', 'the', 'size', '512']), (0.0027906389261080893, ['The', 'Transformer']), (0.0027097417645150805, ['The', 'second', 'score', 'would', 'be', 'the', 'dot', 'product', 'of', 'q1', 'and', 'k2']), (0.00261576628636781, ['There', 'could', 'be', 'other', 'possible', 'values', 'here,', 'but', 'this', 'is', 'the', 'default),', 'then', 'pass', 'the', 'result', 'through', 'a', 'softmax', 'operation']), (0.0026134736457179, ['There', 'are', 'a', 'few', 'other', 'details', 'that', 'make', 'them', 'work', 'better']), (0.0025832572571235176, ['For', 'a', 'large', 'corpus', 'of', 'text,', 'this', 'increases', 'the', 'time', 'spent', 'translating', 'the', 'text']), (0.002463661716469042, ['Softmax', 'normalizes', 'the', 'scores', 'so', 'they’re', 'all', 'positive', 'and', 'add', 'up', 'to', '1']), (0.0024267035411976714, ['All', 'encoders', 'have', 'the', 'same', 'architecture']), (0.0023675172654073155, ['The', 'loops', 'can', 'be', 'thought', 'in', 'a', 'different', 'way']), (0.00235246519618009, ['If', 'there', 'is', 'anything', 'important,', 'we', 'can', 'cancel', 'some', 'of', 'the', 'meetings', 'and', 'accommodate', 'what', 'is', 'important']), (0.002176734152797527, ['In', 'practice,', 'they', 'don’t', 'seem', 'to', 'learn', 'them']), (0.002110699309102898, ['Let’s', 'go', 'over', 'these', 'two', 'architectures', 'and', 'their', 'drawbacks']), (0.002092560247013469, ['And', 'if', 'you', 'ask', 'me', 'to', 'describe', 'the', 'room', 'I’m', 'sitting', 'in,', 'I’ll', 'glance', 'around', 'at', 'the', 'objects', 'I’m', 'describing', 'as', 'I', 'do', 'so']), (0.0020748422209120165, ['I', 'won’t', 'go', 'into', 'detail', 'on', 'the', 'mechanics', 'of', 'each', 'cell']), (0.0019785868807306174, ['In', 'the', 'actual', 'implementation,', 'however,', 'this', 'calculation', 'is', 'done', 'in', 'matrix', 'form', 'for', 'faster', 'processing']), (0.0018775584666645344, ['When', 'I’m', 'transcribing', 'an', 'audio', 'recording,', 'I', 'listen', 'carefully', 'to', 'the', 'segment', 'I’m', 'actively', 'writing', 'down']), (0.001551165452317706, ['they', 'are', 'also', 'very', 'similar', 'to', 'each', 'other']), (0.0013891739090498957, ['Decoders', 'share', 'the', 'same', 'property,', 'i.e']), (0.0012496868549244037, ['Long-Short', 'Term', 'Memory', '(LSTM)', 'When', 'arranging', 'one’s', 'calendar', 'for', 'the', 'day,', 'we', 'prioritize', 'our', 'appointments']), (0.0009718537612711428, ['This', 'includes', 'speech', 'recognition,', 'text-to-speech', 'transformation,', 'etc.']), (0.0009082652134423253, ['This', 'leads', 'to', 'having', 'more', 'stable', 'gradients'])]\n","\n","Summarize Text 0: \n","We need to score each word of the input sentence against this word\n","\n","Summarize Text 1: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated\n","\n","Summarize Text 2: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word\n","\n","Summarize Text 3: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word. However, if you think a bit more, it turns out that… colah.github.io With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating\n","\n","Summarize Text 4: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word. However, if you think a bit more, it turns out that… colah.github.io With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information\n","\n","Updated: ------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.01360156817742775, ['We', 'need', 'to', 'score', 'each', 'word', 'of', 'the', 'input', 'sentence', 'against', 'this', 'word']), (0.012963361082404073, ['Image', 'from', '10', 'The', 'reason', 'why', 'Convolutional', 'Neural', 'Networks', 'can', 'work', 'in', 'parallel,', 'is', 'that', 'each', 'word', 'on', 'the', 'input', 'can', 'be', 'processed', 'at', 'the', 'same', 'time', 'and', 'does', 'not', 'necessarily', 'depend', 'on', 'the', 'previous', 'words', 'to', 'be', 'translated']), (0.012336562129130707, ['It', 'helps', 'the', 'encoder', 'look', 'at', 'other', 'words', 'in', 'the', 'input', 'sentence', 'as', 'it', 'encodes', 'a', 'specific', 'word']), (0.011959615233045935, ['However,', 'if', 'you', 'think', 'a', 'bit', 'more,', 'it', 'turns', 'out', 'that…', 'colah.github.io', 'With', 'a', 'cell', 'state,', 'the', 'information', 'in', 'a', 'sentence', 'that', 'is', 'important', 'for', 'translating', 'a', 'word', 'may', 'be', 'passed', 'from', 'one', 'word', 'to', 'another,', 'when', 'translating']), (0.011939721964870604, ['The', 'Recurrent', 'Neural', 'Network', 'passes', 'the', 'information', 'of', 'the', 'previous', 'words', 'to', 'the', 'next', 'network', 'that', 'can', 'use', 'and', 'process', 'that', 'information']), (0.011283824153068604, ['Image', 'from', '8', 'The', 'first', 'step', 'in', 'calculating', 'self-attention', 'is', 'to', 'create', 'three', 'vectors', 'from', 'each', 'of', 'the', 'encoder’s', 'input', 'vectors', '(in', 'this', 'case,', 'the', 'embedding', 'of', 'each', 'word)']), (0.010903713601070926, ['Another', 'problem', 'with', 'RNNs,', 'and', 'LSTMs,', 'is', 'that', 'it’s', 'hard', 'to', 'parallelize', 'the', 'work', 'for', 'processing', 'sentences,', 'since', 'you', 'are', 'have', 'to', 'process', 'word', 'by', 'word']), (0.010652467071534338, ['There', 'are', 'many', 'examples,', 'where', 'words', 'in', 'some', 'sentences', 'refer', 'to', 'words', 'in', 'previous', 'sentences']), (0.010436636640070067, ['When', 'translating', 'a', 'sentence,', 'I', 'pay', 'special', 'attention', 'to', 'the', 'word', 'I’m', 'presently', 'translating']), (0.010305409404919494, ['After', 'embedding', 'the', 'words', 'in', 'our', 'input', 'sequence,', 'each', 'of', 'them', 'flows', 'through', 'each', 'of', 'the', 'two', 'layers', 'of', 'the', 'encoder']), (0.01011908473665126, ['Instead', 'of', 'passing', 'only', 'one', 'hidden', 'state', 'to', 'the', 'decoders', 'as', 'we', 'did', 'before', 'using', 'attention,', 'we', 'pass', 'all', 'the', 'hidden', 'states', 'generated', 'by', 'every', '“word”', 'of', 'the', 'sentence', 'to', 'the', 'decoding', 'stage']), (0.010109957235885756, ['Clearly', 'the', 'word', 'at', 'this', 'position', 'will', 'have', 'the', 'highest', 'softmax', 'score,', 'but', 'sometimes', 'it’s', 'useful', 'to', 'attend', 'to', 'another', 'word', 'that', 'is', 'relevant', 'to', 'the', 'current', 'word']), (0.010094390009670358, ['In', 'this', 'case', 'where', 'the', 'difference', 'between', 'the', 'relevant', 'information', 'and', 'the', 'place', 'that', 'is', 'needed', 'is', 'small,', 'RNNs', 'can', 'learn', 'to', 'use', 'past', 'information', 'and', 'figure', 'out', 'what', 'is', 'the', 'next', 'word', 'for', 'this', 'sentence']), (0.010072207280216443, ['So', 'in', 'order', 'for', 'the', 'decoding', 'to', 'be', 'precise,', 'it', 'needs', 'to', 'take', 'into', 'account', 'every', 'word', 'of', 'the', 'input,', 'using', 'attention']), (0.009994246359956542, ['Internally,', 'a', 'LSTM', 'looks', 'like', 'the', 'following:', 'Image', 'from', '6', 'Each', 'cell', 'takes', 'as', 'inputs', 'x_t', '(a', 'word', 'in', 'the', 'case', 'of', 'a', 'sentence', 'to', 'sentence', 'translation),', 'the', 'previous', 'cell', 'state', 'and', 'the', 'output', 'of', 'the', 'previous', 'cell']), (0.009767373632668644, ['The', 'idea', 'behind', 'it', 'is', 'that', 'whenever', 'you', 'are', 'translating', 'a', 'word,', 'you', 'may', 'pay', 'different', 'attention', 'to', 'each', 'word', 'based', 'on', 'the', 'type', 'of', 'question', 'that', 'you', 'are', 'asking']), (0.00954930416465265, ['The', 'score', 'determines', 'how', 'much', 'focus', 'to', 'place', 'on', 'other', 'parts', 'of', 'the', 'input', 'sentence', 'as', 'we', 'encode', 'a', 'word', 'at', 'a', 'certain', 'position']), (0.009455506483460144, ['If', 'you', 'want', 'to', 'understand', 'in', 'depth', 'how', 'the', 'model', 'works', 'and', 'all', 'its', 'nuances,', 'I', 'recommend', 'the', 'following', 'posts,', 'articles', 'and', 'videos', 'that', 'I', 'used', 'as', 'a', 'base', 'for', 'summarizing', 'the', 'technique', 'The', 'Unreasonable', 'Effectiveness', 'of', 'Recurrent', 'Neural', 'Networks', 'Understanding', 'LSTM', 'Networks', 'Visualizing', 'A', 'Neural', 'Machine', 'Translation', 'Model', 'The', 'Illustrated', 'Transformer', 'The', 'Transformer', '—', 'Attention', 'is', 'all', 'you', 'need', 'The', 'Annotated', 'Transformer', 'Attention', 'is', 'all', 'you', 'need', 'attentional', 'neural', 'network', 'models', 'Self-Attention', 'For', 'Generative', 'Models', 'OpenAI', 'GPT-2:', 'Understanding', 'Language', 'Generation', 'through', 'Visualization', 'WaveNet:', 'A', 'Generative', 'Model', 'for', 'Raw', 'Audio', '']), (0.009440573377883739, ['For', 'example,', 'when', 'translating', 'the', 'sentence', '“Je', 'suis', 'étudiant”', 'to', 'English,', 'requires', 'that', 'the', 'decoding', 'step', 'looks', 'at', 'different', 'words', 'when', 'translating', 'it']), (0.009420059519648243, ['We', 'end', 'up', 'creating', 'a', '“query”,', 'a', '“key”,', 'and', 'a', '“value”', 'projection', 'of', 'each', 'word', 'in', 'the', 'input', 'sentence']), (0.009252123502921534, ['Image', 'taken', 'from', '4', 'Each', 'word', 'is', 'embedded', 'into', 'a', 'vector', 'of', 'size', '512']), (0.009061588206343915, ['For', 'example,', 'processing', 'inputs', '(words)', 'in', 'parallel', 'is', 'not', 'possible']), (0.00901428190574349, ['This', 'produces', 'the', 'output', 'of', 'the', 'self-attention', 'layer', 'at', 'this', 'position', '(for', 'the', 'first', 'word)']), (0.008995308298766073, ['Figuring', 'out', 'relation', 'of', 'words', 'within', 'a', 'sentence', 'and', 'giving', 'the', 'right', 'attention', 'to', 'it']), (0.008938032126991347, ['Image', 'from', '4', 'Here', 'we', 'begin', 'to', 'see', 'one', 'key', 'property', 'of', 'the', 'Transformer,', 'which', 'is', 'that', 'the', 'word', 'in', 'each', 'position', 'flows', 'through', 'its', 'own', 'path', 'in', 'the', 'encoder']), (0.008922866192695807, ['Each', 'word', 'is', 'processed', 'separately,', 'and', 'the', 'resulting', 'sentence', 'is', 'generated', 'by', 'passing', 'a', 'hidden', 'state', 'to', 'the', 'decoding', 'stage', 'that,', 'then,', 'generates', 'the', 'output']), (0.008849469359372964, ['GIF', 'from', '3', 'The', 'idea', 'behind', 'it', 'is', 'that', 'there', 'might', 'be', 'relevant', 'information', 'in', 'every', 'word', 'in', 'a', 'sentence']), (0.008751711546596727, ['To', 'solve', 'these', 'problems,', 'Attention', 'is', 'a', 'technique', 'that', 'is', 'used', 'in', 'a', 'neural', 'network']), (0.008693965376717568, ['Encoding', 'the', 'position', 'of', 'each', 'word', 'is', 'relevant,', 'since', 'the', 'position', 'of', 'each', 'word', 'is', 'relevant', 'to', 'the', 'translation']), (0.008684168412113311, ['For', 'RNNs,', 'instead', 'of', 'only', 'encoding', 'the', 'whole', 'sentence', 'in', 'a', 'hidden', 'state,', 'each', 'word', 'has', 'a', 'corresponding', 'hidden', 'state', 'that', 'is', 'passed', 'all', 'the', 'way', 'to', 'the', 'decoding', 'stage']), (0.008584061720384792, ['Transformers', 'To', 'solve', 'the', 'problem', 'of', 'parallelization,', 'Transformers', 'try', 'to', 'solve', 'the', 'problem', 'by', 'using', 'Convolutional', 'Neural', 'Networks', 'together', 'with', 'attention', 'models']), (0.008572896989296213, ['Say', 'we’re', 'calculating', 'the', 'self-attention', 'for', 'the', 'first', 'word', 'in', 'this', 'example,', '“Thinking”']), (0.008539485068662245, ['The', 'following', 'picture', 'shows', 'how', 'usually', 'a', 'sequence', 'to', 'sequence', 'model', 'works', 'using', 'Recurrent', 'Neural', 'Networks']), (0.008431672831206993, ['Or', 'ask', 'other', 'questions,', 'like', '“Did', 'what?”,', 'etc…', 'Images', 'from', '8', 'Positional', 'Encoding', 'Another', 'important', 'step', 'on', 'the', 'Transformer', 'is', 'to', 'add', 'positional', 'encoding', 'when', 'encoding', 'each', 'word']), (0.008426775506992624, ['If', 'we', 'are', 'trying', 'to', 'predict', 'the', 'next', 'word', 'of', 'the', 'sentence', '“the', 'clouds', 'in', 'the', 'sky”,', 'we', 'don’t', 'need', 'further', 'context']), (0.008386802879941332, ['Recurrent', 'Neural', 'Networks', '(RNNs)', 'and', 'Convolutional', 'Neural', 'Networks', '(CNNs)', 'have', 'been', 'used', 'to', 'deal', 'with', 'this', 'problem', 'because', 'of', 'their', 'properties']), (0.008377561787870684, ['Each', 'hidden', 'state', 'is', 'used', 'in', 'the', 'decoding', 'stage,', 'to', 'figure', 'out', 'where', 'the', 'network', 'should', 'pay', 'attention', 'to']), (0.008200240629538703, ['Neural', 'networks', 'can', 'achieve', 'this', 'same', 'behavior', 'using', 'attention,', 'focusing', 'on', 'part', 'of', 'a', 'subset', 'of', 'the', 'information', 'they', 'are', 'given']), (0.00819379981793236, ['As', 'is', 'the', 'case', 'in', 'NLP', 'applications', 'in', 'general,', 'we', 'begin', 'by', 'turning', 'each', 'input', 'word', 'into', 'a', 'vector', 'using', 'an', 'embedding', 'algorithm']), (0.008173647088304214, ['So', 'if', 'we’re', 'processing', 'the', 'self-attention', 'for', 'the', 'word', 'in', 'position', '#1,', 'the', 'first', 'score', 'would', 'be', 'the', 'dot', 'product', 'of', 'q1', 'and', 'k1']), (0.00816844662422643, ['To', 'summarize,', 'LSTMs', 'and', 'RNNs', 'present', '3', 'problems:', 'Sequential', 'computation', 'inhibits', 'parallelization', 'No', 'explicit', 'modeling', 'of', 'long', 'and', 'short', 'range', 'dependencies', '“Distance”', 'between', 'positions', 'is', 'linear', 'Attention', 'To', 'solve', 'some', 'of', 'these', 'problems,', 'researchers', 'created', 'a', 'technique', 'for', 'paying', 'attention', 'to', 'specific', 'words']), (0.007948348721696823, ['GIF', 'from', '3', 'The', 'problem', 'of', 'long-term', 'dependencies', 'Consider', 'a', 'language', 'model', 'that', 'is', 'trying', 'to', 'predict', 'the', 'next', 'word', 'based', 'on', 'the', 'previous', 'ones']), (0.007910464213576354, ['The', 'intuition', 'here', 'is', 'to', 'keep', 'intact', 'the', 'values', 'of', 'the', 'word(s)', 'we', 'want', 'to', 'focus', 'on,', 'and', 'drown-out', 'irrelevant', 'words', '(by', 'multiplying', 'them', 'by', 'tiny', 'numbers', 'like', '0.001,', 'for', 'example)']), (0.007866424593087309, ['In', 'that', 'way,', 'if', 'we', 'want', 'to', 'translate', 'some', 'text,', 'we', 'can', 'set', 'each', 'input', 'as', 'the', 'word', 'in', 'that', 'text']), (0.0076825831557531, ['Not', 'only', 'that,', 'but', 'the', '“distance”', 'between', 'the', 'output', 'word', 'and', 'any', 'input', 'for', 'a', 'CNN', 'is', 'in', 'the', 'order', 'of', 'log(N)', '—', 'that', 'is', 'the', 'size', 'of', 'the', 'height', 'of', 'the', 'tree', 'generated', 'from', 'the', 'output', 'to', 'the', 'input', '(you', 'can', 'see', 'it', 'on', 'the', 'GIF', 'above']), (0.00767460830240553, ['Image', 'from', '4', 'This', 'softmax', 'score', 'determines', 'how', 'much', 'how', 'much', 'each', 'word', 'will', 'be', 'expressed', 'at', 'this', 'position']), (0.007603734881586363, ['Image', 'from', '4', 'The', 'encoder’s', 'inputs', 'first', 'flow', 'through', 'a', 'self-attention', 'layer']), (0.007588210788570523, ['The', 'problem', 'is', 'that', 'Convolutional', 'Neural', 'Networks', 'do', 'not', 'necessarily', 'help', 'with', 'the', 'problem', 'of', 'figuring', 'out', 'the', 'problem', 'of', 'dependencies', 'when', 'translating', 'sentences']), (0.007469891970952932, ['The', 'reason', 'for', 'that', 'is', 'that', 'the', 'probability', 'of', 'keeping', 'the', 'context', 'from', 'a', 'word', 'that', 'is', 'far', 'away', 'from', 'the', 'current', 'word', 'being', 'processed', 'decreases', 'exponentially', 'with', 'the', 'distance', 'from', 'it']), (0.007347259948755914, ['With', 'them', 'we', 'can', 'Trivial', 'to', 'parallelize', '(per', 'layer)', 'Exploits', 'local', 'dependencies', 'Distance', 'between', 'positions', 'is', 'logarithmic', 'Some', 'of', 'the', 'most', 'popular', 'neural', 'networks', 'for', 'sequence', 'transduction,', 'Wavenet', 'and', 'Bytenet,', 'are', 'Convolutional', 'Neural', 'Networks']), (0.007296991842765063, ['The', 'input', 'is', 'represented', 'as', 'x_t', 'In', 'the', 'figure', 'above,', 'we', 'see', 'part', 'of', 'the', 'neural', 'network,', 'A,', 'processing', 'some', 'input', 'x_t', 'and', 'outputs', 'h_t']), (0.0072694423661273004, ['Image', 'from', '3', 'But', 'some', 'of', 'the', 'problems', 'that', 'we', 'discussed,', 'still', 'are', 'not', 'solved', 'with', 'RNNs', 'using', 'attention']), (0.007268943674119861, ['Recurrent', 'Neural', 'Networks', 'Recurrent', 'Neural', 'Networks', 'have', 'loops', 'in', 'them,', 'allowing', 'information', 'to', 'persist']), (0.007221947721736785, ['The', 'band', 'was', 'formed', 'in', '1968,', 'during', 'the', 'height', 'of', 'Japanese', 'music', 'history”', 'In', 'this', 'example,', 'the', 'word', '“the', 'band”', 'in', 'the', 'second', 'sentence', 'refers', 'to', 'the', 'band', '“The', 'Transformers”', 'introduced', 'in', 'the', 'first', 'sentence']), (0.007201891657266166, ['Transformer', 'is', 'a', 'model', 'that', 'uses', 'attention', 'to', 'boost', 'the', 'speed']), (0.00715021176973635, ['The', 'score', 'is', 'calculated', 'by', 'taking', 'the', 'dot', 'product', 'of', 'the', 'query', 'vector', 'with', 'the', 'key', 'vector', 'of', 'the', 'respective', 'word', 'we’re', 'scoring']), (0.007016951821802244, ['GIF', 'from', '3', 'Or', 'for', 'example,', 'when', 'you', 'translate', 'the', 'sentence', '“L’accord', 'sur', 'la', 'zone', 'économique', 'européenne', 'a', 'été', 'signé', 'en', 'août', '1992.”', 'from', 'French', 'to', 'English,', 'and', 'how', 'much', 'attention', 'it', 'is', 'paid', 'to', 'each', 'input']), (0.007005142154586975, ['Image', 'taken', 'from', '4', 'Multiplying', 'x1', 'by', 'the', 'WQ', 'weight', 'matrix', 'produces', 'q1,', 'the', '“query”', 'vector', 'associated', 'with', 'that', 'word']), (0.006930253216594185, ['For', 'attention', 'to', 'be', 'brought', 'to', 'RNNs', 'in', 'sequence', 'transduction,', 'we', 'divide', 'the', 'encoding', 'and', 'decoding', 'into', '2', 'main', 'steps']), (0.0069186900722803205, ['Then,', 'the', 'hidden', 'states', 'are', 'used', 'at', 'each', 'step', 'of', 'the', 'RNN', 'to', 'decode']), (0.006874272544771056, ['For', 'example,', 'instead', 'of', 'only', 'paying', 'attention', 'to', 'each', 'other', 'in', 'one', 'dimension,', 'Transformers', 'use', 'the', 'concept', 'of', 'Multihead', 'attention']), (0.006857771437965554, ['For', 'translating', 'sentences', 'like', 'that,', 'a', 'model', 'needs', 'to', 'figure', 'out', 'these', 'sort', 'of', 'dependencies', 'and', 'connections']), (0.006840586677517464, ['Each', 'encoder', 'consists', 'of', 'two', 'layers:', 'Self-attention', 'and', 'a', 'feed', 'Forward', 'Neural', 'Network']), (0.006780919526094156, ['Convolutional', 'Neural', 'Networks', 'Convolutional', 'Neural', 'Networks', 'help', 'solve', 'these', 'problems']), (0.006775464402294607, ['Attention', 'boosts', 'the', 'speed', 'of', 'how', 'fast', 'the', 'model', 'can', 'translate', 'from', 'one', 'sequence', 'to', 'another']), (0.006595897152120655, ['It’s', 'pretty', 'obvious', 'that', 'the', 'next', 'word', 'is', 'going', 'to', 'be', 'sky']), (0.006589966832489746, ['Wavenet,', 'model', 'is', 'a', 'Convolutional', 'Neural', 'Network', '(CNN)']), (0.006520069665875203, ['The', 'decoder', 'has', 'both', 'those', 'layers,', 'but', 'between', 'them', 'is', 'an', 'attention', 'layer', 'that', 'helps', 'the', 'decoder', 'focus', 'on', 'relevant', 'parts', 'of', 'the', 'input', 'sentence']), (0.006506341520381835, ['Self-Attention', 'Let’s', 'first', 'look', 'at', 'how', 'to', 'calculate', 'self-attention', 'using', 'vectors,', 'then', 'proceed', 'to', 'look', 'at', 'how', 'it’s', 'actually', 'implemented', '—', 'using', 'matrices']), (0.006487739106596569, ['Consider', 'what', 'happens', 'if', 'we', 'unroll', 'the', 'loop:', 'An', 'unrolled', 'recurrent', 'neural', 'network', 'This', 'chain-like', 'nature', 'shows', 'that', 'recurrent', 'neural', 'networks', 'are', 'clearly', 'related', 'to', 'sequences', 'and', 'lists']), (0.006471733811018462, ['GIF', 'from', '3', 'The', 'step', 'in', 'green', 'in', 'charge', 'of', 'creating', 'the', 'hidden', 'states', 'from', 'the', 'input']), (0.006451488355745262, ['So', 'for', 'each', 'word,', 'we', 'create', 'a', 'Query', 'vector,', 'a', 'Key', 'vector,', 'and', 'a', 'Value', 'vector']), (0.006394958676573792, ['A', 'Recurrent', 'Neural', 'Network', 'can', 'be', 'thought', 'of', 'as', 'multiple', 'copies', 'of', 'the', 'same', 'network,', 'A,', 'each', 'network', 'passing', 'a', 'message', 'to', 'a', 'successor']), (0.006377473451093033, ['So', 'let’s', 'look', 'at', 'that', 'now', 'that', 'we’ve', 'seen', 'the', 'intuition', 'of', 'the', 'calculation', 'on', 'the', 'word', 'level']), (0.0063552839684501115, ['Recent', 'information', 'suggests', 'that', 'the', 'next', 'word', 'is', 'probably', 'a', 'language,', 'but', 'if', 'we', 'want', 'to', 'narrow', 'down', 'which', 'language,', 'we', 'need', 'context', 'of', 'France,', 'that', 'is', 'further', 'back', 'in', 'the', 'text']), (0.006351698382884842, ['In', 'the', 'bottom', 'encoder', 'that', 'would', 'be', 'the', 'word', 'embeddings,', 'but', 'in', 'other', 'encoders,', 'it', 'would', 'be', 'the', 'output', 'of', 'the', 'encoder', 'that’s', 'directly', 'below']), (0.006351330899483612, ['Image', 'from', '4', 'The', 'third', 'and', 'forth', 'steps', 'are', 'to', 'divide', 'the', 'scores', 'by', '8', '(the', 'square', 'root', 'of', 'the', 'dimension', 'of', 'the', 'key', 'vectors', 'used', 'in', 'the', 'paper', '—', '64']), (0.006346792352434351, ['Depending', 'on', 'the', 'answer,', 'the', 'translation', 'of', 'the', 'word', 'to', 'another', 'language', 'can', 'change']), (0.006184893955354517, ['For', 'example', 'let’s', 'say', 'that', 'we', 'are', 'translating', 'the', 'following', 'sentence', 'to', 'another', 'language', '(French):', '“The', 'Transformers”', 'are', 'a', 'Japanese', '[[hardcore', 'punk]]', 'band']), (0.0061561666713323285, ['Transformers', 'were', 'developed', 'to', 'solve', 'the', 'problem', 'of', 'sequence', 'transduction,', 'or', 'neural', 'machine', 'translation']), (0.006153361260002699, ['The', 'darker', 'the', 'color', 'is,', 'the', 'more', 'weight', 'is', 'associated', 'to', 'each', 'word']), (0.006089303371120487, ['Image', 'from', '4', 'Self-Attention', 'Note:', 'This', 'section', 'comes', 'from', 'Jay', 'Allamar', 'blog', 'post', 'Let’s', 'start', 'to', 'look', 'at', 'the', 'various', 'vectors/tensors', 'and', 'how', 'they', 'flow', 'between', 'these', 'components', 'to', 'turn', 'the', 'input', 'of', 'a', 'trained', 'model', 'into', 'an', 'output']), (0.006026812323443973, ['For', 'example,', 'let’s', 'say', 'that', 'you', 'are', 'trying', 'to', 'predict', 'the', 'last', 'word', 'of', 'the', 'text:', '“I', 'grew', 'up', 'in', 'France…', 'I', 'speak', 'fluent', '…”']), (0.006015919937324221, ['This', 'gif', 'shows', 'how', 'the', 'weight', 'that', 'is', 'given', 'to', 'each', 'hidden', 'state', 'when', 'translating', 'the', 'sentence', '“Je', 'suis', 'étudiant”', 'to', 'English']), (0.005989220585149477, ['Image', 'from', '4', 'Internally,', 'the', 'Transformer', 'has', 'a', 'similar', 'kind', 'of', 'architecture', 'as', 'the', 'previous', 'models', 'above']), (0.005930810586467195, ['That', 'means', 'that', 'when', 'sentences', 'are', 'long,', 'the', 'model', 'often', 'forgets', 'the', 'content', 'of', 'distant', 'positions', 'in', 'the', 'sequence']), (0.005893510136395593, ['If', 'you', 'want', 'to', 'understand', 'how', 'each', 'cell', 'works,', 'I', 'recommend', 'Christopher’s', 'blog', 'post:', 'Understanding', 'LSTM', 'Networks', '--', \"colah's\", 'blog', 'These', 'loops', 'make', 'recurrent', 'neural', 'networks', 'seem', 'kind', 'of', 'mysterious']), (0.005812145370598573, ['The', 'resulting', 'vector', 'is', 'one', 'we', 'can', 'send', 'along', 'to', 'the', 'feed-forward', 'neural', 'network']), (0.005795207739529156, ['A', 'loop', 'allows', 'information', 'to', 'be', 'passed', 'from', 'one', 'step', 'to', 'the', 'next']), (0.0053369182598711215, ['That', 'means', 'any', 'task', 'that', 'transforms', 'an', 'input', 'sequence', 'to', 'an', 'output', 'sequence']), (0.00533191919505996, ['Next,', 'we’ll', 'switch', 'up', 'the', 'example', 'to', 'a', 'shorter', 'sentence', 'and', 'we’ll', 'look', 'at', 'what', 'happens', 'in', 'each', 'sub-layer', 'of', 'the', 'encoder']), (0.005296532613576682, ['when', 'sentences', 'are', 'too', 'long', 'LSTMs', 'still', 'don’t', 'do', 'too', 'well']), (0.005290871894081272, ['What', 'are', 'the', '“query”,', '“key”,', 'and', '“value”', 'vectors?', 'They’re', 'abstractions', 'that', 'are', 'useful', 'for', 'calculating', 'and', 'thinking', 'about', 'attention']), (0.005252201812477734, ['The', 'second', 'step', 'in', 'calculating', 'self-attention', 'is', 'to', 'calculate', 'a', 'score']), (0.005224752154321493, ['Image', 'from', '4', 'Each', 'encoder', 'is', 'very', 'similar', 'to', 'each', 'other']), (0.00516776524244333, ['Overview', 'I', 'gave', 'an', 'overview', 'of', 'how', 'Transformers', 'work', 'and', 'why', 'this', 'is', 'the', 'technique', 'used', 'for', 'sequence', 'transduction']), (0.0051227919388096375, ['It', 'manipulates', 'these', 'inputs', 'and', 'based', 'on', 'them,', 'it', 'generates', 'a', 'new', 'cell', 'state,', 'and', 'an', 'output']), (0.005005581217095572, ['The', 'green', 'step', 'is', 'called', 'the', 'encoding', 'stage', 'and', 'the', 'purple', 'step', 'is', 'the', 'decoding', 'stage']), (0.005005581217095572, ['The', 'green', 'step', 'is', 'called', 'the', 'encoding', 'stage', 'and', 'the', 'purple', 'step', 'is', 'the', 'decoding', 'stage']), (0.004979907233686077, ['More', 'specifically,', 'it', 'uses', 'self-attention']), (0.004898925924242562, ['With', 'LSTMs,', 'the', 'information', 'flows', 'through', 'a', 'mechanism', 'known', 'as', 'cell', 'states']), (0.004887757795349091, ['Multihead', 'attention', 'Transformers', 'basically', 'work', 'like', 'that']), (0.004804374644796877, ['The', 'fifth', 'step', 'is', 'to', 'multiply', 'each', 'value', 'vector', 'by', 'the', 'softmax', 'score', '(in', 'preparation', 'to', 'sum', 'them', 'up)']), (0.004783611206187308, ['People', 'say', \"'It's\", 'as', 'plain', 'as', 'the', 'nose', 'on', 'your', \"face.'…\", 'www.holloway.com', 'Transformers', 'are', 'a', 'type', 'of', 'neural', 'network', 'architecture', 'that', 'have', 'been', 'gaining', 'popularity']), (0.00472127324715663, ['The', 'input', 'is', 'represented', 'in', 'green,', 'the', 'model', 'is', 'represented', 'in', 'blue,', 'and', 'the', 'output', 'is', 'represented', 'in', 'purple']), (0.004668219888037754, ['Image', 'from', '4', 'That', 'concludes', 'the', 'self-attention', 'calculation']), (0.004628706842449632, ['Image', 'from', '6', 'RNNs', 'become', 'very', 'ineffective', 'when', 'the', 'gap', 'between', 'the', 'relevant', 'information', 'and', 'the', 'point', 'where', 'it', 'is', 'needed', 'become', 'very', 'large']), (0.004598370202809837, ['That', 'is', 'much', 'better', 'than', 'the', 'distance', 'of', 'the', 'output', 'of', 'a', 'RNN', 'and', 'an', 'input,', 'which', 'is', 'on', 'the', 'order', 'of', 'N']), (0.0045930129020198985, ['At', 'every', 'time', 'step,', 'it', 'focuses', 'on', 'different', 'positions', 'in', 'the', 'other', 'RNN']), (0.004529056559463237, ['The', 'sixth', 'step', 'is', 'to', 'sum', 'up', 'the', 'weighted', 'value', 'vectors']), (0.004514212869203724, ['Let’s', 'take', 'a', 'look', 'at', 'how', 'Transformer', 'works']), (0.0045120268415122285, ['GIF', 'from', '3', 'For', 'models', 'to', 'perform', 'sequence', 'transduction,', 'it', 'is', 'necessary', 'to', 'have', 'some', 'sort', 'of', 'memory']), (0.004495789787599886, ['Transformers', 'were', 'recently', 'used', 'by', 'OpenAI', 'in', 'their', 'language', 'models,', 'and', 'also', 'used', 'recently', 'by', 'DeepMind', 'for', 'AlphaStar', '—', 'their', 'program', 'to', 'defeat', 'a', 'top', 'professional', 'Starcraft', 'player']), (0.004445628424087723, ['Once', 'you', 'proceed', 'with', 'reading', 'how', 'attention', 'is', 'calculated', 'below,', 'you’ll', 'know', 'pretty', 'much', 'all', 'you', 'need', 'to', 'know', 'about', 'the', 'role', 'each', 'of', 'these', 'vectors', 'plays']), (0.0043886333824598705, ['One', 'step', 'is', 'represented', 'in', 'green', 'and', 'the', 'other', 'in', 'purple']), (0.004321029110317015, ['If', 'you', 'liked', 'this', 'post', 'and', 'want', 'to', 'learn', 'how', 'machine', 'learning', 'algorithms', 'work,', 'how', 'did', 'they', 'arise,', 'and', 'where', 'are', 'they', 'going,', 'I', 'recommend', 'the', 'following:', 'Making', 'Things', 'Think:', 'How', 'AI', 'and', 'Deep', 'Learning', 'Power', 'the', 'Products', 'We', 'Use', '-', 'Holloway', 'It', 'is', 'the', 'obvious', 'which', 'is', 'so', 'difficult', 'to', 'see', 'most', 'of', 'the', 'time']), (0.00431975566583481, ['Image', 'from', '6', 'But', 'there', 'are', 'cases', 'where', 'we', 'need', 'more', 'context']), (0.004305135877528992, ['That’s', 'why', 'Transformers', 'were', 'created,', 'they', 'are', 'a', 'combination', 'of', 'both', 'CNNs', 'with', 'attention']), (0.0042565771578163755, ['There', 'are', 'dependencies', 'between', 'these', 'paths', 'in', 'the', 'self-attention', 'layer']), (0.004250005699350652, ['When', 'you', 'read', 'about', 'the', 'band', 'in', 'the', 'second', 'sentence,', 'you', 'know', 'that', 'it', 'is', 'referencing', 'to', 'the', '“The', 'Transformers”', 'band']), (0.004216410918890298, ['For', 'example,', 'whenever', 'you', 'are', 'translating', '“kicked”', 'in', 'the', 'sentence', '“I', 'kicked', 'the', 'ball”,', 'you', 'may', 'ask', '“Who', 'kicked”']), (0.004147702806061595, ['The', 'problem', 'with', 'LSTMs', 'The', 'same', 'problem', 'that', 'happens', 'to', 'RNNs', 'generally,', 'happen', 'with', 'LSTMs,', 'i.e']), (0.004125261353216669, ['That', 'is', 'due', 'to', 'the', 'fact', 'that', 'the', 'information', 'is', 'passed', 'at', 'each', 'step', 'and', 'the', 'longer', 'the', 'chain', 'is,', 'the', 'more', 'probable', 'the', 'information', 'is', 'lost', 'along', 'the', 'chain']), (0.00399445411860705, ['The', 'images', 'below', 'show', 'what', 'that', 'means']), (0.003967062781191181, ['Translating', 'the', 'sentence', '“L’accord', 'sur', 'la', 'zone', 'économique', 'européenne', 'a', 'été', 'signé', 'en', 'août', '1992.”', 'to', 'English']), (0.003866449980310768, ['They', 'don’t', 'HAVE', 'to', 'be', 'smaller,', 'this', 'is', 'an', 'architecture', 'choice', 'to', 'make', 'the', 'computation', 'of', 'multiheaded', 'attention', '(mostly)', 'constant']), (0.003827890588975816, ['Notice', 'that', 'these', 'new', 'vectors', 'are', 'smaller', 'in', 'dimension', 'than', 'the', 'embedding', 'vector']), (0.003675334876481451, ['For', 'example,', 'an', 'RNN', 'can', 'attend', 'over', 'the', 'output', 'of', 'another', 'RNN']), (0.003631128478135189, ['These', 'vectors', 'are', 'created', 'by', 'multiplying', 'the', 'embedding', 'by', 'three', 'matrices', 'that', 'we', 'trained', 'during', 'the', 'training', 'process']), (0.003621354764996563, ['The', 'entire', 'information', 'is', 'modified,', 'and', 'there', 'is', 'no', 'consideration', 'of', 'what', 'is', 'important', 'and', 'what', 'is', 'not']), (0.0035949892649106674, ['Sequence', 'transduction']), (0.003535578792014722, ['Not', 'only', 'that', 'but', 'there', 'is', 'no', 'model', 'of', 'long', 'and', 'short', 'range', 'dependencies']), (0.003491987594298868, ['LSTM,', 'a', 'special', 'type', 'of', 'RNN,', 'tries', 'to', 'solve', 'this', 'kind', 'of', 'problem']), (0.003442235433604755, ['The', 'embedding', 'only', 'happens', 'in', 'the', 'bottom-most', 'encoder']), (0.0033737257404968584, ['Whenever', 'it', 'adds', 'new', 'information,', 'it', 'transforms', 'existing', 'information', 'completely', 'by', 'applying', 'a', 'function']), (0.0033573981203933213, ['LSTMs', 'make', 'small', 'modifications', 'to', 'the', 'information', 'by', 'multiplications', 'and', 'additions']), (0.00334072154718154, ['Their', 'dimensionality', 'is', '64,', 'while', 'the', 'embedding', 'and', 'encoder', 'input/output', 'vectors', 'have', 'dimensionality', 'of', '512']), (0.0032878514292511746, ['RNNs', 'don’t', 'do', 'that']), (0.0032717637390706194, ['In', 'this', 'way,', 'LSTMs', 'can', 'selectively', 'remember', 'or', 'forget', 'things', 'that', 'are', 'important', 'and', 'not', 'so', 'important']), (0.0032454880594193903, ['In', 'theory,', 'RNNs', 'could', 'learn', 'this', 'long-term', 'dependencies']), (0.0030484233372887024, ['The', 'feed-forward', 'layer', 'does', 'not', 'have', 'those', 'dependencies,', 'however,', 'and', 'thus', 'the', 'various', 'paths', 'can', 'be', 'executed', 'in', 'parallel', 'while', 'flowing', 'through', 'the', 'feed-forward', 'layer']), (0.00303473671216365, ['That', 'may', 'be', 'important', 'for', 'translation']), (0.0030338629919541725, ['The', 'following', 'gif', 'shows', 'how', 'that', 'happens']), (0.002919975297956694, ['But', 'the', 'Transformer', 'consists', 'of', 'six', 'encoders', 'and', 'six', 'decoders']), (0.0029057866393564996, ['We’ll', 'represent', 'those', 'vectors', 'with', 'these', 'simple', 'boxes']), (0.002827524054951756, ['The', 'abstraction', 'that', 'is', 'common', 'to', 'all', 'the', 'encoders', 'is', 'that', 'they', 'receive', 'a', 'list', 'of', 'vectors', 'each', 'of', 'the', 'size', '512']), (0.0027906389261080893, ['The', 'Transformer']), (0.0027097417645150805, ['The', 'second', 'score', 'would', 'be', 'the', 'dot', 'product', 'of', 'q1', 'and', 'k2']), (0.00261576628636781, ['There', 'could', 'be', 'other', 'possible', 'values', 'here,', 'but', 'this', 'is', 'the', 'default),', 'then', 'pass', 'the', 'result', 'through', 'a', 'softmax', 'operation']), (0.0026134736457179, ['There', 'are', 'a', 'few', 'other', 'details', 'that', 'make', 'them', 'work', 'better']), (0.0025832572571235176, ['For', 'a', 'large', 'corpus', 'of', 'text,', 'this', 'increases', 'the', 'time', 'spent', 'translating', 'the', 'text']), (0.002463661716469042, ['Softmax', 'normalizes', 'the', 'scores', 'so', 'they’re', 'all', 'positive', 'and', 'add', 'up', 'to', '1']), (0.0024267035411976714, ['All', 'encoders', 'have', 'the', 'same', 'architecture']), (0.0023675172654073155, ['The', 'loops', 'can', 'be', 'thought', 'in', 'a', 'different', 'way']), (0.00235246519618009, ['If', 'there', 'is', 'anything', 'important,', 'we', 'can', 'cancel', 'some', 'of', 'the', 'meetings', 'and', 'accommodate', 'what', 'is', 'important']), (0.002176734152797527, ['In', 'practice,', 'they', 'don’t', 'seem', 'to', 'learn', 'them']), (0.002110699309102898, ['Let’s', 'go', 'over', 'these', 'two', 'architectures', 'and', 'their', 'drawbacks']), (0.002092560247013469, ['And', 'if', 'you', 'ask', 'me', 'to', 'describe', 'the', 'room', 'I’m', 'sitting', 'in,', 'I’ll', 'glance', 'around', 'at', 'the', 'objects', 'I’m', 'describing', 'as', 'I', 'do', 'so']), (0.0020748422209120165, ['I', 'won’t', 'go', 'into', 'detail', 'on', 'the', 'mechanics', 'of', 'each', 'cell']), (0.0019785868807306174, ['In', 'the', 'actual', 'implementation,', 'however,', 'this', 'calculation', 'is', 'done', 'in', 'matrix', 'form', 'for', 'faster', 'processing']), (0.0018775584666645344, ['When', 'I’m', 'transcribing', 'an', 'audio', 'recording,', 'I', 'listen', 'carefully', 'to', 'the', 'segment', 'I’m', 'actively', 'writing', 'down']), (0.001551165452317706, ['they', 'are', 'also', 'very', 'similar', 'to', 'each', 'other']), (0.0013891739090498957, ['Decoders', 'share', 'the', 'same', 'property,', 'i.e']), (0.0012496868549244037, ['Long-Short', 'Term', 'Memory', '(LSTM)', 'When', 'arranging', 'one’s', 'calendar', 'for', 'the', 'day,', 'we', 'prioritize', 'our', 'appointments']), (0.0009718537612711428, ['This', 'includes', 'speech', 'recognition,', 'text-to-speech', 'transformation,', 'etc.']), (0.0009082652134423253, ['This', 'leads', 'to', 'having', 'more', 'stable', 'gradients'])]\n","\n","Summarize Text 0: \n","We need to score each word of the input sentence against this word\n","\n","Summarize Text 1: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated\n","\n","Summarize Text 2: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word\n","\n","Summarize Text 3: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word. However, if you think a bit more, it turns out that… colah.github.io With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating\n","\n","Summarize Text 4: \n","We need to score each word of the input sentence against this word. Image from 10 The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. It helps the encoder look at other words in the input sentence as it encodes a specific word. However, if you think a bit more, it turns out that… colah.github.io With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information\n","\n","\n","\n","Article a3: =======================================================================\n","\n","Old: ------------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.013447120904306057, ['Splitting', 'Q,', 'K,', 'V,', 'N', 'times', 'before', 'applying', 'self-attention', 'To', 'sum', 'it', 'up,', 'multi-headed', 'attention', 'is', 'a', 'module', 'in', 'the', 'transformer', 'network', 'that', 'computes', 'the', 'attention', 'weights', 'for', 'the', 'input', 'and', 'produces', 'an', 'output', 'vector', 'with', 'encoded', 'information', 'on', 'how', 'each', 'word', 'should', 'attend', 'to', 'all', 'other', 'words', 'in', 'the', 'sequence']), (0.012690494704969656, ['For', 'this', 'layer,', 'the', 'encoder’s', 'outputs', 'are', 'the', 'queries', 'and', 'the', 'keys,', 'and', 'the', 'first', 'multi-headed', 'attention', 'layer', 'outputs', 'are', 'the', 'values']), (0.012598749648163488, ['The', 'positional', 'embeddings', 'get', 'fed', 'into', 'the', 'first', 'multi-head', 'attention', 'layer', 'which', 'computes', 'the', 'attention', 'scores', 'for', 'the', 'decoder’s', 'input']), (0.012193080679952097, ['For', 'example,', 'when', 'computing', 'attention', 'scores', 'on', 'the', 'word', '“am”,', 'you', 'should', 'not', 'have', 'access', 'to', 'the', 'word', '“fine”,', 'because', 'that', 'word', 'is', 'a', 'future', 'word', 'that', 'was', 'generated', 'after']), (0.011948049269936856, ['This', 'masking', 'is', 'the', 'only', 'difference', 'in', 'how', 'the', 'attention', 'scores', 'are', 'calculated', 'in', 'the', 'first', 'multi-headed', 'attention', 'layer']), (0.011888657539008432, ['The', 'output', 'of', 'the', 'first', 'multi-headed', 'attention', 'is', 'a', 'masked', 'output', 'vector', 'with', 'information', 'on', 'how', 'the', 'model', 'should', 'attend', 'on', 'the', 'decoder’s', 'input']), (0.011865643274938852, ['The', 'Residual', 'Connections,', 'Layer', 'Normalization,', 'and', 'Feed', 'Forward', 'Network', 'The', 'multi-headed', 'attention', 'output', 'vector', 'is', 'added', 'to', 'the', 'original', 'positional', 'input', 'embedding']), (0.01142564560937416, ['Our', 'Input:', '“Hi', 'how', 'are', 'you”', 'Transformer', 'Output:', '“I', 'am', 'fine”', 'Input', 'Embeddings', 'The', 'first', 'step', 'is', 'feeding', 'out', 'input', 'into', 'a', 'word', 'embedding', 'layer']), (0.011397861836894022, ['Multi-Headed', 'Attention', 'with', 'Masking', 'Decoder', 'Second', 'Multi-Headed', 'Attention,', 'and', 'Point-wise', 'Feed', 'Forward', 'Layer', 'The', 'second', 'multi-headed', 'attention', 'layer']), (0.011298296198058708, ['As', 'you', 'can', 'see', 'in', 'the', 'figure', 'below,', 'the', 'attention', 'scores', 'for', '“am”,', 'has', 'values', 'for', 'itself', 'and', 'all', 'words', 'before', 'it', 'but', 'is', 'zero', 'for', 'the', 'word', '“fine”']), (0.011292510414479907, ['The', 'output', 'of', 'the', 'second', 'multi-headed', 'attention', 'goes', 'through', 'a', 'pointwise', 'feedforward', 'layer', 'for', 'further', 'processing']), (0.010982065063892106, ['Attention', 'mechanism', 'focusing', 'on', 'different', 'tokens', 'while', 'generating', 'words', '1', 'by', '1', 'Recurrent', 'neural', 'networks', '(RNN)', 'are', 'also', 'capable', 'of', 'looking', 'at', 'previous', 'inputs', 'too']), (0.010946344041780895, ['Self-attention', 'allows', 'the', 'models', 'to', 'associate', 'each', 'word', 'in', 'the', 'input,', 'to', 'other', 'words']), (0.01091756446867432, ['Residual', 'connection', 'of', 'the', 'positional', 'input', 'embedding', 'and', 'the', 'output', 'of', 'Multi-headed', 'Attention', 'The', 'normalized', 'residual', 'output', 'gets', 'projected', 'through', 'a', 'pointwise', 'feed-forward', 'network', 'for', 'further', 'processing']), (0.010878215283526553, ['The', 'decoder', 'is', 'autoregressive,', 'it', 'begins', 'with', 'a', 'start', 'token,', 'and', 'it', 'takes', 'in', 'a', 'list', 'of', 'previous', 'outputs', 'as', 'inputs,', 'as', 'well', 'as', 'the', 'encoder', 'outputs', 'that', 'contain', 'the', 'attention', 'information', 'from', 'the', 'input']), (0.010732255139001716, ['The', 'decoder', 'can', 'also', 'be', 'stacked', 'N', 'layers', 'high,', 'each', 'layer', 'taking', 'in', 'inputs', 'from', 'the', 'encoder', 'and', 'the', 'layers', 'before', 'it']), (0.010631847619213049, ['it', 'has', 'two', 'multi-headed', 'attention', 'layers,', 'a', 'pointwise', 'feed-forward', 'layer,', 'and', 'residual', 'connections,', 'and', 'layer', 'normalization', 'after', 'each', 'sub-layer']), (0.01044851995139074, ['As', 'the', 'model', 'generates', 'the', 'text', 'word', 'by', 'word,', 'it', 'can', '“attend”', 'or', '“focus”', 'on', 'words', 'that', 'are', 'relevant', 'to', 'the', 'generated', 'word']), (0.010214725297212127, ['These', 'sub-layers', 'behave', 'similarly', 'to', 'the', 'layers', 'in', 'the', 'encoder', 'but', 'each', 'multi-headed', 'attention', 'layer', 'has', 'a', 'different', 'job']), (0.010134801479507499, ['Multi-Headed', 'Attention', 'Multi-headed', 'attention', 'in', 'the', 'encoder', 'applies', 'a', 'specific', 'attention', 'mechanism', 'called', 'self-attention']), (0.010120265409512708, ['By', 'stacking', 'the', 'layers,', 'the', 'model', 'can', 'learn', 'to', 'extract', 'and', 'focus', 'on', 'different', 'combinations', 'of', 'attention', 'from', 'its', 'attention', 'heads,', 'potentially', 'boosting', 'its', 'predictive', 'power']), (0.009922982597863492, ['Residual', 'connection', 'of', 'the', 'input', 'and', 'output', 'of', 'the', 'point-wise', 'feedforward', 'layer']), (0.00990559352532909, ['We', 'need', 'a', 'method', 'to', 'prevent', 'computing', 'attention', 'scores', 'for', 'future', 'words']), (0.00982841034507594, ['A', 'depiction', 'of', 'Decoder’s', 'first', 'Multi-headed', 'Attention', 'scaled', 'attention', 'scores']), (0.009760091401398007, ['Neural', 'networks', 'learn', 'through', 'numbers', 'so', 'each', 'word', 'maps', 'to', 'a', 'vector', 'with', 'continuous', 'values', 'to', 'represent', 'that', 'word']), (0.009752348818323996, ['A', 'word', 'embedding', 'layer', 'can', 'be', 'thought', 'of', 'as', 'a', 'lookup', 'table', 'to', 'grab', 'a', 'learned', 'vector', 'representation', 'of', 'each', 'word']), (0.009738388743613442, ['The', 'pointwise', 'feedforward', 'layer', 'is', 'used', 'to', 'project', 'the', 'attention', 'outputs', 'potentially', 'giving', 'it', 'a', 'richer', 'representation']), (0.009722213365846364, ['Encoder', 'Layer', 'Sub', 'Modules', 'To', 'break', 'this', 'down,', 'let’s', 'first', 'look', 'at', 'the', 'multi-headed', 'attention', 'module']), (0.009677181127583524, ['The', 'decoder', 'is', 'capped', 'off', 'with', 'a', 'linear', 'layer', 'that', 'acts', 'as', 'a', 'classifier,', 'and', 'a', 'softmax', 'to', 'get', 'the', 'word', 'probabilities']), (0.009674376851846415, ['Decoders', 'First', 'Multi-Headed', 'Attention', 'This', 'multi-headed', 'attention', 'layer', 'operates', 'slightly', 'differently']), (0.009639900363342727, ['The', 'input', 'goes', 'through', 'an', 'embedding', 'layer', 'and', 'positional', 'encoding', 'layer', 'to', 'get', 'positional', 'embeddings']), (0.009518727454897367, ['You', 'can', 'stack', 'the', 'encoder', 'N', 'times', 'to', 'further', 'encode', 'the', 'information,', 'where', 'each', 'layer', 'has', 'the', 'opportunity', 'to', 'learn', 'different', 'attention', 'representations', 'therefore', 'potentially', 'boosting', 'the', 'predictive', 'power', 'of', 'the', 'transformer', 'network']), (0.00945255370604221, ['So', 'each', 'word', 'will', 'have', 'a', 'score', 'that', 'corresponds', 'to', 'other', 'words', 'in', 'the', 'time-step']), (0.0093887737173983, ['The', 'output', 'of', 'the', 'classifier', 'then', 'gets', 'fed', 'into', 'a', 'softmax', 'layer,', 'which', 'will', 'produce', 'probability', 'scores', 'between', '0', 'and', '1']), (0.009308650009030313, ['The', 'higher', 'softmax', 'scores', 'will', 'keep', 'the', 'value', 'of', 'words', 'the', 'model', 'learns', 'is', 'more', 'important']), (0.00929143861476681, ['This', 'will', 'help', 'the', 'decoder', 'focus', 'on', 'the', 'appropriate', 'words', 'in', 'the', 'input', 'during', 'the', 'decoding', 'process']), (0.009250431551384775, ['Scaling', 'down', 'the', 'Attention', 'scores', 'Softmax', 'of', 'the', 'Scaled', 'Scores', 'Next,', 'you', 'take', 'the', 'softmax', 'of', 'the', 'scaled', 'score', 'to', 'get', 'the', 'attention', 'weights,', 'which', 'gives', 'you', 'probability', 'values', 'between', '0', 'and', '1']), (0.009177155893025923, ['The', 'word', '“am”,', 'should', 'not', 'any', 'values', 'for', 'the', 'word', '“fine”']), (0.009124724908787858, ['Computing', 'Multi-headed', 'Attention', 'To', 'make', 'this', 'a', 'multi-headed', 'attention', 'computation,', 'you', 'need', 'to', 'split', 'the', 'query,', 'key,', 'and', 'value', 'into', 'N', 'vectors', 'before', 'applying', 'self-attention']), (0.009106476330445111, ['Taking', 'the', 'softmax', 'of', 'the', 'scaled', 'scores', 'to', 'get', 'probability', 'values', 'Multiply', 'Softmax', 'Output', 'with', 'Value', 'vector', 'Then', 'you', 'take', 'the', 'attention', 'weights', 'and', 'multiply', 'it', 'by', 'your', 'value', 'vector', 'to', 'get', 'an', 'output', 'vector']), (0.008933763229309909, ['Dot', 'Product', 'multiplication', 'of', 'the', 'query', 'and', 'the', 'key', 'The', 'score', 'matrix', 'determines', 'how', 'much', 'focus', 'should', 'a', 'word', 'be', 'put', 'on', 'other', 'words']), (0.008851314167336543, ['Decoder', 'Layer']), (0.008812546515362051, ['Linear', 'Classifier', 'with', 'Softmax', 'to', 'get', 'the', 'Output', 'Probabilities', 'The', 'decoder', 'then', 'takes', 'the', 'output,', 'add’s', 'it', 'to', 'the', 'list', 'of', 'decoder', 'inputs,', 'and', 'continues', 'decoding', 'again', 'until', 'a', 'token', 'is', 'predicted']), (0.008750136817794029, ['Encoder', 'Layer', 'Now', 'we', 'have', 'the', 'encoder', 'layer']), (0.008728754752972375, ['Hypothetical', 'reference', 'window', 'of', 'Attention,', 'RNN’s,', 'GRU’s', '&', 'LSTM’s', 'Attention', 'Is', 'All', 'You', 'Need', '—', 'Step', 'by', 'Step', 'Walkthrough', 'The', 'attention', 'mechanism’s', 'power', 'was', 'demonstrated', 'in', 'the', 'paper', '“Attention', 'Is', 'All', 'You', 'Need”,', 'where', 'the', 'authors', 'introduced', 'a', 'new', 'novel', 'neural', 'network', 'called', 'the', 'Transformers', 'which', 'is', 'an', 'attention-based', 'encoder-decoder', 'type', 'architecture']), (0.008688499765315313, ['The', 'word', '“am”', 'should', 'only', 'have', 'access', 'to', 'itself', 'and', 'the', 'words', 'before', 'it']), (0.008632444712557648, ['This', 'is', 'true', 'for', 'all', 'other', 'words,', 'where', 'they', 'can', 'only', 'attend', 'to', 'previous', 'words']), (0.008548045732038457, ['Then', 'you', 'feed', 'the', 'output', 'of', 'that', 'into', 'a', 'linear', 'layer', 'to', 'process']), (0.008535568120746802, ['Since', 'the', 'decoder', 'is', 'autoregressive', 'and', 'generates', 'the', 'sequence', 'word', 'by', 'word,', 'you', 'need', 'to', 'prevent', 'it', 'from', 'conditioning', 'to', 'future', 'tokens']), (0.008408206217897653, ['All', 'of', 'these', 'operations', 'are', 'to', 'encode', 'the', 'input', 'to', 'a', 'continuous', 'representation', 'with', 'attention', 'information']), (0.008383810253734726, ['This', 'layer', 'still', 'has', 'multiple', 'heads,', 'that', 'the', 'mask', 'is', 'being', 'applied', 'to,', 'before', 'getting', 'concatenated', 'and', 'fed', 'through', 'a', 'linear', 'layer', 'for', 'further', 'processing']), (0.008212253108247896, ['Transformer', 'Model', 'On', 'a', 'high', 'level,', 'the', 'encoder', 'maps', 'an', 'input', 'sequence', 'into', 'an', 'abstract', 'continuous', 'representation', 'that', 'holds', 'all', 'the', 'learned', 'information', 'of', 'that', 'input']), (0.008049858244144673, ['Linear', 'Classifier', 'and', 'Final', 'Softmax', 'for', 'Output', 'Probabilities', 'The', 'output', 'of', 'the', 'final', 'pointwise', 'feedforward', 'layer', 'goes', 'through', 'a', 'final', 'linear', 'layer,', 'that', 'acts', 'as', 'a', 'classifier']), (0.00803401182741913, ['The', 'output', 'of', 'the', 'residual', 'connection', 'goes', 'through', 'a', 'layer', 'normalization']), (0.008023428736153523, ['So', 'in', 'our', 'example,', 'it’s', 'possible', 'that', 'our', 'model', 'can', 'learn', 'to', 'associate', 'the', 'word', '“you”,', 'with', '“how”', 'and', '“are”']), (0.008017199035344223, ['Attention', 'scores', 'from', 'the', 'dot', 'product']), (0.007929353974724846, ['Adding', 'a', 'look-ahead', 'mask', 'to', 'the', 'scaled', 'scores', 'The', 'reason', 'for', 'the', 'mask', 'is', 'because', 'once', 'you', 'take', 'the', 'softmax', 'of', 'the', 'masked', 'scores,', 'the', 'negative', 'infinities', 'get', 'zeroed', 'out,', 'leaving', 'zero', 'attention', 'scores', 'for', 'future', 'tokens']), (0.007908044246178185, ['This', 'process', 'matches', 'the', 'encoder’s', 'input', 'to', 'the', 'decoder’s', 'input,', 'allowing', 'the', 'decoder', 'to', 'decide', 'which', 'encoder', 'input', 'is', 'relevant', 'to', 'put', 'a', 'focus', 'on']), (0.007850831558754472, ['For', 'example,', 'if', 'you', 'have', '10,000', 'classes', 'for', '10,000', 'words,', 'the', 'output', 'of', 'that', 'classier', 'will', 'be', 'of', 'size', '10,000']), (0.007739905525970249, ['Because', 'the', 'transformer', 'encoder', 'has', 'no', 'recurrence', 'like', 'recurrent', 'neural', 'networks,', 'we', 'must', 'add', 'some', 'information', 'about', 'the', 'positions', 'into', 'the', 'input', 'embeddings']), (0.007724926746773095, ['Each', 'head', 'produces', 'an', 'output', 'vector', 'that', 'gets', 'concatenated', 'into', 'a', 'single', 'vector', 'before', 'going', 'through', 'the', 'final', 'linear', 'layer']), (0.007723653536137959, ['Scaling', 'Down', 'the', 'Attention', 'Scores', 'Then,', 'the', 'scores', 'get', 'scaled', 'down', 'by', 'getting', 'divided', 'by', 'the', 'square', 'root', 'of', 'the', 'dimension', 'of', 'query', 'and', 'key']), (0.007708567161487125, ['Query,', 'Key,', 'and', 'Value', 'Vectors', 'To', 'achieve', 'self-attention,', 'we', 'feed', 'the', 'input', 'into', '3', 'distinct', 'fully', 'connected', 'layers', 'to', 'create', 'the', 'query,', 'key,', 'and', 'value', 'vectors']), (0.007706936419580402, ['The', 'output', 'of', 'that', 'is', 'then', 'again', 'added', 'to', 'the', 'input', 'of', 'the', 'pointwise', 'feed-forward', 'network', 'and', 'further', 'normalized']), (0.007652436919134711, ['When', 'you', 'add', 'the', 'mask', 'to', 'the', 'scaled', 'attention', 'scores,', 'you', 'get', 'a', 'matrix', 'of', 'the', 'scores,', 'with', 'the', 'top', 'right', 'triangle', 'filled', 'with', 'negativity', 'infinities']), (0.007630245373723772, ['This', 'allows', 'the', 'model', 'to', 'be', 'more', 'confident', 'about', 'which', 'words', 'to', 'attend', 'too']), (0.007383147546616534, ['Decoder', 'Layer', 'The', 'decoder’s', 'job', 'is', 'to', 'generate', 'text', 'sequences']), (0.007349264692489651, ['This', 'is', 'true', 'for', 'all', 'other', 'words']), (0.007340783094901604, ['Decoder', 'Input', 'Embeddings', '&', 'Positional', 'Encoding', 'The', 'beginning', 'of', 'the', 'decoder', 'is', 'pretty', 'much', 'the', 'same', 'as', 'the', 'encoder']), (0.0073138199598396835, ['converting', 'Words', 'to', 'Input', 'Embeddings', 'Positional', 'Encoding', 'The', 'next', 'step', 'is', 'to', 'inject', 'positional', 'information', 'into', 'the', 'embeddings']), (0.007197634192367358, ['The', 'lower', 'scores', 'will', 'drown', 'out', 'the', 'irrelevant', 'words']), (0.007192190464985935, ['We', 'take', 'the', 'index', 'of', 'the', 'highest', 'probability', 'score,', 'and', 'that', 'equals', 'our', 'predicted', 'word']), (0.007083875802121429, ['The', 'decoder', 'then', 'takes', 'that', 'continuous', 'representation', 'and', 'step', 'by', 'step', 'generates', 'a', 'single', 'output', 'while', 'also', 'being', 'fed', 'the', 'previous', 'output']), (0.007019387675821468, ['The', 'Encoders', 'layers', 'job', 'is', 'to', 'map', 'all', 'input', 'sequences', 'into', 'an', 'abstract', 'continuous', 'representation', 'that', 'holds', 'the', 'learned', 'information', 'for', 'that', 'entire', 'sequence']), (0.007005773436421986, ['This', 'essentially', 'tells', 'the', 'model', 'to', 'put', 'no', 'focus', 'on', 'those', 'words']), (0.006815372316309101, ['Look-Ahead', 'Mask', 'The', 'mask', 'is', 'a', 'matrix', 'that’s', 'the', 'same', 'size', 'as', 'the', 'attention', 'scores', 'filled', 'with', 'values', 'of', '0’s', 'and', 'negative', 'infinities']), (0.006773108329241246, ['Attention', 'Mechanism', 'To', 'understand', 'transformers', 'we', 'first', 'must', 'understand', 'the', 'attention', 'mechanism']), (0.006755250966886971, ['Dot', 'Product', 'of', 'Query', 'and', 'Key', 'After', 'feeding', 'the', 'query,', 'key,', 'and', 'value', 'vector', 'through', 'a', 'linear', 'layer,', 'the', 'queries', 'and', 'keys', 'undergo', 'a', 'dot', 'product', 'matrix', 'multiplication', 'to', 'produce', 'a', 'score', 'matrix']), (0.0065137197217321925, ['We’ll', 'prime', 'the', 'model', 'with', 'our', 'input,', 'and', 'the', 'model', 'will', 'generate', 'the', 'rest']), (0.006435795557861956, ['Encoder', 'Wrap-up', 'That', 'wraps', 'up', 'the', 'encoder', 'layer']), (0.006392232329417964, ['It', 'contains', '2', 'sub-modules,', 'multi-headed', 'attention,', 'followed', 'by', 'a', 'fully', 'connected', 'network']), (0.006356143640228866, ['The', 'pointwise', 'feed-forward', 'network', 'is', 'a', 'couple', 'of', 'linear', 'layers', 'with', 'a', 'ReLU', 'activation', 'in', 'between']), (0.006301162312724359, ['The', 'decoder', 'is', 'autoregressive', 'as', 'it', 'generates', 'a', 'token', '1', 'at', 'a', 'time', 'while', 'being', 'fed', 'in', 'the', 'previous', 'outputs']), (0.006292698238052719, ['The', 'decoder', 'stops', 'decoding', 'when', 'it', 'generates', 'a', 'token', 'as', 'an', 'output']), (0.00621495187832357, ['A', 'transformer', 'model', 'can', '“attend”', 'or', '“focus”', 'on', 'all', 'previous', 'tokens', 'that', 'have', 'been', 'generated']), (0.006037563677123441, ['The', 'attention', 'mechanism,', 'in', 'theory,', 'and', 'given', 'enough', 'compute', 'resources,', 'have', 'an', 'infinite', 'window', 'to', 'reference', 'from,', 'therefore', 'being', 'capable', 'of', 'using', 'the', 'entire', 'context', 'of', 'the', 'story', 'while', 'generating', 'the', 'text']), (0.005844781539059747, ['The', 'ability', 'to', 'know', 'what', 'words', 'to', 'attend', 'too', 'is', 'all', 'learned', 'during', 'training', 'through', 'backpropagation']), (0.005804396829877209, ['RNN’s', 'have', 'a', 'shorter', 'window', 'to', 'reference', 'from,', 'so', 'when', 'the', 'story', 'gets', 'longer,', 'RNN’s', 'can’t', 'access', 'words', 'generated', 'earlier', 'in', 'the', 'sequence']), (0.00580262787224999, ['It’s', 'also', 'possible', 'that', 'the', 'model', 'learns', 'that', 'words', 'structured', 'in', 'this', 'pattern', 'are', 'typically', 'a', 'question', 'so', 'respond', 'appropriately']), (0.005781557467756223, ['But', 'the', 'power', 'of', 'the', 'attention', 'mechanism', 'is', 'that', 'it', 'doesn’t', 'suffer', 'from', 'short', 'term', 'memory']), (0.005718142516136553, ['Transformers', 'leverage', 'the', 'power', 'of', 'the', 'attention', 'mechanism', 'to', 'make', 'better', 'predictions']), (0.005713616965999812, ['The', 'Attention', 'mechanism', 'enables', 'the', 'transformers', 'to', 'have', 'extremely', 'long', 'term', 'memory']), (0.005667571639369732, ['In', 'this', 'post,', 'we’ll', 'focus', 'on', 'the', 'one', 'paper', 'that', 'started', 'it', 'all,', '“Attention', 'is', 'all', 'you', 'need”']), (0.00560413964461824, ['The', 'mask', 'is', 'added', 'before', 'calculating', 'the', 'softmax,', 'and', 'after', 'scaling', 'the', 'scores']), (0.005563215813071868, ['By', 'doing', 'a', 'softmax', 'the', 'higher', 'scores', 'get', 'heighten,', 'and', 'lower', 'scores', 'are', 'depressed']), (0.005540714825996669, ['The', 'layer', 'normalizations', 'are', 'used', 'to', 'stabilize', 'the', 'network', 'which', 'results', 'in', 'substantially', 'reducing', 'the', 'training', 'time', 'necessary']), (0.0054837319379625275, ['Transformers', 'are', 'the', 'rage', 'in', 'deep', 'learning', 'nowadays,', 'but', 'how', 'do', 'they', 'work?', 'Why', 'have', 'they', 'outperform', 'the', 'previous', 'king', 'of', 'sequence', 'problems,', 'like', 'recurrent', 'neural', 'networks,', 'GRU’s,', 'and', 'LSTM’s?', 'You’ve', 'probably', 'heard', 'of', 'different', 'famous', 'transformers', 'models', 'like', 'BERT,', 'GPT,', 'and', 'GPT2']), (0.005371071697353115, ['In', 'theory,', 'each', 'head', 'would', 'learn', 'something', 'different', 'therefore', 'giving', 'the', 'encoder', 'model', 'more', 'representation', 'power']), (0.005303740032279831, ['For', 'every', 'odd', 'index', 'on', 'the', 'input', 'vector,', 'create', 'a', 'vector', 'using', 'the', 'cos', 'function']), (0.00523145148372606, ['Then', 'add', 'those', 'vectors', 'to', 'their', 'corresponding', 'input', 'embeddings']), (0.0050220943924959615, ['The', 'higher', 'the', 'score', 'the', 'more', 'focus']), (0.004968169458222811, ['There', 'are', 'also', 'residual', 'connections', 'around', 'each', 'of', 'the', 'two', 'sublayers', 'followed', 'by', 'a', 'layer', 'normalization']), (0.004684777818047582, ['The', 'decoder', 'has', 'a', 'similar', 'sub-layer', 'as', 'the', 'encoder']), (0.004526720898516775, ['To', 'prevent', 'the', 'decoder', 'from', 'looking', 'at', 'future', 'tokens,', 'you', 'apply', 'a', 'look', 'ahead', 'mask']), (0.004385590857977359, ['The', 'sin', 'and', 'cosine', 'functions', 'were', 'chosen', 'in', 'tandem', 'because', 'they', 'have', 'linear', 'properties', 'the', 'model', 'can', 'easily', 'learn', 'to', 'attend', 'to']), (0.004250129703568717, ['This', 'is', 'done', 'using', 'positional', 'encoding']), (0.004216484710985326, ['The', 'paper', 'applied', 'the', 'Transformer', 'model', 'on', 'a', 'neural', 'machine', 'translation', 'problem']), (0.004173268491170284, ['This', 'successfully', 'gives', 'the', 'network', 'information', 'on', 'the', 'position', 'of', 'each', 'vector']), (0.004059976289745087, ['Transformers', 'can', 'be', 'better', 'especially', 'if', 'you', 'want', 'to', 'encode', 'or', 'generate', 'long', 'sequences']), (0.004033739738442327, ['Let’s', 'take', 'a', 'look', 'at', 'how', 'this', 'works']), (0.0039950973207651605, ['The', 'residual', 'connections', 'help', 'the', 'network', 'train,', 'by', 'allowing', 'gradients', 'to', 'flow', 'through', 'the', 'networks', 'directly']), (0.003952653997588351, ['Transformer', 'output:', '“and', 'began', 'to', 'colonized', 'Earth,', 'a', 'certain', 'group', 'of', 'extraterrestrials', 'began', 'to', 'manipulate', 'our', 'society', 'through', 'their', 'influences', 'of', 'a', 'certain', 'number', 'of', 'the', 'elite', 'to', 'keep', 'and', 'iron', 'grip', 'over', 'the', 'populace.”', 'Ok,', 'so', 'the', 'story', 'is', 'a', 'little', 'dark', 'but', 'what’s', 'interesting', 'is', 'how', 'the', 'model', 'generated', 'it']), (0.003929129673480688, ['Encoder', 'Self-Attention', 'Operations']), (0.0038441702757113674, ['Each', 'self-attention', 'process', 'is', 'called', 'a', 'head']), (0.003794590623417895, ['Stacked', 'Encoder', 'and', 'Decoder', 'And', 'That’s', 'It!', 'And', 'that’s', 'it!', 'That’s', 'the', 'mechanics', 'of', 'the', 'transformers']), (0.003742392689882511, ['Our', 'input:', '“As', 'Aliens', 'entered', 'our', 'planet”']), (0.003726353903630572, ['For', 'every', 'even', 'index,', 'create', 'a', 'vector', 'using', 'the', 'sin', 'function']), (0.0037070062007277193, ['The', 'split', 'vectors', 'then', 'go', 'through', 'the', 'self-attention', 'process', 'individually']), (0.0036975333305534477, ['Let’s', 'walk', 'through', 'an', 'example']), (0.0036975333305534477, ['Let’s', 'walk', 'through', 'an', 'example']), (0.0035982761782466866, ['“The', 'query', 'key', 'and', 'value', 'concept', 'come', 'from', 'retrieval', 'systems']), (0.0035929754697790746, ['For', 'example,', 'when', 'you', 'type', 'a', 'query', 'to', 'search', 'for', 'some', 'video', 'on', 'Youtube,', 'the', 'search', 'engine', 'will', 'map', 'your', 'query', 'against', 'a', 'set', 'of', 'keys', '(video', 'title,', 'description', 'etc.)', 'associated', 'with', 'candidate', 'videos', 'in', 'the', 'database,', 'then', 'present', 'you', 'the', 'best', 'matched', 'videos', '(values)']), (0.0035878129945890597, ['Recurrent', 'Neural', 'networks', 'try', 'to', 'achieve', 'similar', 'things,', 'but', 'because', 'they', 'suffer', 'from', 'short', 'term', 'memory']), (0.0035256340939284975, ['They', 'are', 'used', 'in', 'many', 'applications', 'like', 'machine', 'language', 'translation,', 'conversational', 'chatbots,', 'and', 'even', 'to', 'power', 'better', 'search', 'engines']), (0.003492014039954455, ['This', 'is', 'still', 'true', 'for', 'Gated', 'Recurrent', 'Units', '(GRU’s)', 'and', 'Long-short', 'Term', 'Memory', '(LSTM’s)', 'networks,', 'although', 'they', 'do', 'a', 'bigger', 'capacity', 'to', 'achieve', 'longer-term', 'memory,', 'therefore,', 'having', 'a', 'longer', 'window', 'to', 'reference', 'from']), (0.003490895793475297, ['Transformers', 'are', 'taking', 'the', 'natural', 'language', 'processing', 'world', 'by', 'storm']), (0.0034347086274565696, ['Let’s', 'walk', 'through', 'the', 'decoding', 'steps']), (0.0034226012118477086, ['This', 'is', 'how', 'the', 'queries', 'are', 'mapped', 'to', 'the', 'keys']), (0.003213852778883776, ['Because', 'of', 'the', 'transformer', 'architecture,', 'the', 'natural', 'language', 'processing', 'industry', 'can', 'achieve', 'unprecedented', 'results']), (0.0031000871266480354, ['Using', 'Hugging', 'Face’s', 'Write', 'With', 'Transformer', 'application,', 'we', 'can', 'do', 'just', 'that']), (0.002922038998885524, ['This', 'method', 'is', 'called', 'masking']), (0.0029119631399797067, ['This', 'is', 'called', 'a', 'residual', 'connection']), (0.0028414176349567605, ['We', 'won’t', 'go', 'into', 'the', 'mathematical', 'details', 'of', 'positional', 'encoding,', 'but', 'here', 'are', 'the', 'basics']), (0.002817940842463092, ['Reference', 'this', 'when', 'looking', 'at', 'Illustrations', 'below']), (0.0025989506257575864, ['The', 'authors', 'came', 'up', 'with', 'a', 'clever', 'trick', 'using', 'sin', 'and', 'cosine', 'functions']), (0.0025797813244151286, ['These', 'incredible', 'models', 'are', 'breaking', 'multiple', 'NLP', 'records', 'and', 'pushing', 'the', 'state', 'of', 'the', 'art']), (0.0024537468733378965, ['This', 'is', 'to', 'allow', 'for', 'more', 'stable', 'gradients,', 'as', 'multiplying', 'values', 'can', 'have', 'exploding', 'effects']), (0.002330135314318617, ['Check', 'out', 'michaelphi.com', 'for', 'more', 'content', 'like', 'this']), (0.002320250682129676, ['Check', 'out', 'the', 'link', 'below', 'if', 'you’d', 'like', 'to', 'watch', 'the', 'video', 'version', 'instead']), (0.0023055066403569924, ['Say', 'we', 'want', 'to', 'write', 'a', 'short', 'sci-fi', 'novel', 'with', 'a', 'generative', 'transformer']), (0.0020518933464564043, ['For', 'our', 'case,', 'the', 'highest', 'probability', 'prediction', 'is', 'the', 'final', 'class', 'which', 'is', 'assigned', 'to', 'the', 'end', 'token']), (0.0018997240351033803, ['The', 'classifier', 'is', 'as', 'big', 'as', 'the', 'number', 'of', 'classes', 'you', 'have']), (0.0018517132955359118, ['Reference', 'This', 'diagram', 'while', 'reading']), (0.0017699289463540273, ['In', 'this', 'post,', 'we’ll', 'demonstrate', 'how', 'it’ll', 'work', 'for', 'a', 'conversational', 'chatbot']), (0.0017628106121404747, ['What', 'are', 'these', 'vectors', 'exactly?', 'I', 'found', 'a', 'good', 'explanation', 'on', 'stack', 'exchange', 'stating…']), (0.001033413709955219, [''])]\n","\n","Summarize Text 0: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence\n","\n","Summarize Text 1: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values\n","\n","Summarize Text 2: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input\n","\n","Summarize Text 3: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after\n","\n","Summarize Text 4: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after. This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer\n","\n","Updated: ------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.013447120904306057, ['Splitting', 'Q,', 'K,', 'V,', 'N', 'times', 'before', 'applying', 'self-attention', 'To', 'sum', 'it', 'up,', 'multi-headed', 'attention', 'is', 'a', 'module', 'in', 'the', 'transformer', 'network', 'that', 'computes', 'the', 'attention', 'weights', 'for', 'the', 'input', 'and', 'produces', 'an', 'output', 'vector', 'with', 'encoded', 'information', 'on', 'how', 'each', 'word', 'should', 'attend', 'to', 'all', 'other', 'words', 'in', 'the', 'sequence']), (0.012690494704969656, ['For', 'this', 'layer,', 'the', 'encoder’s', 'outputs', 'are', 'the', 'queries', 'and', 'the', 'keys,', 'and', 'the', 'first', 'multi-headed', 'attention', 'layer', 'outputs', 'are', 'the', 'values']), (0.012598749648163488, ['The', 'positional', 'embeddings', 'get', 'fed', 'into', 'the', 'first', 'multi-head', 'attention', 'layer', 'which', 'computes', 'the', 'attention', 'scores', 'for', 'the', 'decoder’s', 'input']), (0.012193080679952097, ['For', 'example,', 'when', 'computing', 'attention', 'scores', 'on', 'the', 'word', '“am”,', 'you', 'should', 'not', 'have', 'access', 'to', 'the', 'word', '“fine”,', 'because', 'that', 'word', 'is', 'a', 'future', 'word', 'that', 'was', 'generated', 'after']), (0.011948049269936856, ['This', 'masking', 'is', 'the', 'only', 'difference', 'in', 'how', 'the', 'attention', 'scores', 'are', 'calculated', 'in', 'the', 'first', 'multi-headed', 'attention', 'layer']), (0.011888657539008432, ['The', 'output', 'of', 'the', 'first', 'multi-headed', 'attention', 'is', 'a', 'masked', 'output', 'vector', 'with', 'information', 'on', 'how', 'the', 'model', 'should', 'attend', 'on', 'the', 'decoder’s', 'input']), (0.011865643274938852, ['The', 'Residual', 'Connections,', 'Layer', 'Normalization,', 'and', 'Feed', 'Forward', 'Network', 'The', 'multi-headed', 'attention', 'output', 'vector', 'is', 'added', 'to', 'the', 'original', 'positional', 'input', 'embedding']), (0.01142564560937416, ['Our', 'Input:', '“Hi', 'how', 'are', 'you”', 'Transformer', 'Output:', '“I', 'am', 'fine”', 'Input', 'Embeddings', 'The', 'first', 'step', 'is', 'feeding', 'out', 'input', 'into', 'a', 'word', 'embedding', 'layer']), (0.011397861836894022, ['Multi-Headed', 'Attention', 'with', 'Masking', 'Decoder', 'Second', 'Multi-Headed', 'Attention,', 'and', 'Point-wise', 'Feed', 'Forward', 'Layer', 'The', 'second', 'multi-headed', 'attention', 'layer']), (0.011298296198058708, ['As', 'you', 'can', 'see', 'in', 'the', 'figure', 'below,', 'the', 'attention', 'scores', 'for', '“am”,', 'has', 'values', 'for', 'itself', 'and', 'all', 'words', 'before', 'it', 'but', 'is', 'zero', 'for', 'the', 'word', '“fine”']), (0.011292510414479907, ['The', 'output', 'of', 'the', 'second', 'multi-headed', 'attention', 'goes', 'through', 'a', 'pointwise', 'feedforward', 'layer', 'for', 'further', 'processing']), (0.010982065063892106, ['Attention', 'mechanism', 'focusing', 'on', 'different', 'tokens', 'while', 'generating', 'words', '1', 'by', '1', 'Recurrent', 'neural', 'networks', '(RNN)', 'are', 'also', 'capable', 'of', 'looking', 'at', 'previous', 'inputs', 'too']), (0.010946344041780895, ['Self-attention', 'allows', 'the', 'models', 'to', 'associate', 'each', 'word', 'in', 'the', 'input,', 'to', 'other', 'words']), (0.01091756446867432, ['Residual', 'connection', 'of', 'the', 'positional', 'input', 'embedding', 'and', 'the', 'output', 'of', 'Multi-headed', 'Attention', 'The', 'normalized', 'residual', 'output', 'gets', 'projected', 'through', 'a', 'pointwise', 'feed-forward', 'network', 'for', 'further', 'processing']), (0.010878215283526553, ['The', 'decoder', 'is', 'autoregressive,', 'it', 'begins', 'with', 'a', 'start', 'token,', 'and', 'it', 'takes', 'in', 'a', 'list', 'of', 'previous', 'outputs', 'as', 'inputs,', 'as', 'well', 'as', 'the', 'encoder', 'outputs', 'that', 'contain', 'the', 'attention', 'information', 'from', 'the', 'input']), (0.010732255139001716, ['The', 'decoder', 'can', 'also', 'be', 'stacked', 'N', 'layers', 'high,', 'each', 'layer', 'taking', 'in', 'inputs', 'from', 'the', 'encoder', 'and', 'the', 'layers', 'before', 'it']), (0.010631847619213049, ['it', 'has', 'two', 'multi-headed', 'attention', 'layers,', 'a', 'pointwise', 'feed-forward', 'layer,', 'and', 'residual', 'connections,', 'and', 'layer', 'normalization', 'after', 'each', 'sub-layer']), (0.01044851995139074, ['As', 'the', 'model', 'generates', 'the', 'text', 'word', 'by', 'word,', 'it', 'can', '“attend”', 'or', '“focus”', 'on', 'words', 'that', 'are', 'relevant', 'to', 'the', 'generated', 'word']), (0.010214725297212127, ['These', 'sub-layers', 'behave', 'similarly', 'to', 'the', 'layers', 'in', 'the', 'encoder', 'but', 'each', 'multi-headed', 'attention', 'layer', 'has', 'a', 'different', 'job']), (0.010134801479507499, ['Multi-Headed', 'Attention', 'Multi-headed', 'attention', 'in', 'the', 'encoder', 'applies', 'a', 'specific', 'attention', 'mechanism', 'called', 'self-attention']), (0.010120265409512708, ['By', 'stacking', 'the', 'layers,', 'the', 'model', 'can', 'learn', 'to', 'extract', 'and', 'focus', 'on', 'different', 'combinations', 'of', 'attention', 'from', 'its', 'attention', 'heads,', 'potentially', 'boosting', 'its', 'predictive', 'power']), (0.009922982597863492, ['Residual', 'connection', 'of', 'the', 'input', 'and', 'output', 'of', 'the', 'point-wise', 'feedforward', 'layer']), (0.00990559352532909, ['We', 'need', 'a', 'method', 'to', 'prevent', 'computing', 'attention', 'scores', 'for', 'future', 'words']), (0.00982841034507594, ['A', 'depiction', 'of', 'Decoder’s', 'first', 'Multi-headed', 'Attention', 'scaled', 'attention', 'scores']), (0.009760091401398007, ['Neural', 'networks', 'learn', 'through', 'numbers', 'so', 'each', 'word', 'maps', 'to', 'a', 'vector', 'with', 'continuous', 'values', 'to', 'represent', 'that', 'word']), (0.009752348818323996, ['A', 'word', 'embedding', 'layer', 'can', 'be', 'thought', 'of', 'as', 'a', 'lookup', 'table', 'to', 'grab', 'a', 'learned', 'vector', 'representation', 'of', 'each', 'word']), (0.009738388743613442, ['The', 'pointwise', 'feedforward', 'layer', 'is', 'used', 'to', 'project', 'the', 'attention', 'outputs', 'potentially', 'giving', 'it', 'a', 'richer', 'representation']), (0.009722213365846364, ['Encoder', 'Layer', 'Sub', 'Modules', 'To', 'break', 'this', 'down,', 'let’s', 'first', 'look', 'at', 'the', 'multi-headed', 'attention', 'module']), (0.009677181127583524, ['The', 'decoder', 'is', 'capped', 'off', 'with', 'a', 'linear', 'layer', 'that', 'acts', 'as', 'a', 'classifier,', 'and', 'a', 'softmax', 'to', 'get', 'the', 'word', 'probabilities']), (0.009674376851846415, ['Decoders', 'First', 'Multi-Headed', 'Attention', 'This', 'multi-headed', 'attention', 'layer', 'operates', 'slightly', 'differently']), (0.009639900363342727, ['The', 'input', 'goes', 'through', 'an', 'embedding', 'layer', 'and', 'positional', 'encoding', 'layer', 'to', 'get', 'positional', 'embeddings']), (0.009518727454897367, ['You', 'can', 'stack', 'the', 'encoder', 'N', 'times', 'to', 'further', 'encode', 'the', 'information,', 'where', 'each', 'layer', 'has', 'the', 'opportunity', 'to', 'learn', 'different', 'attention', 'representations', 'therefore', 'potentially', 'boosting', 'the', 'predictive', 'power', 'of', 'the', 'transformer', 'network']), (0.00945255370604221, ['So', 'each', 'word', 'will', 'have', 'a', 'score', 'that', 'corresponds', 'to', 'other', 'words', 'in', 'the', 'time-step']), (0.0093887737173983, ['The', 'output', 'of', 'the', 'classifier', 'then', 'gets', 'fed', 'into', 'a', 'softmax', 'layer,', 'which', 'will', 'produce', 'probability', 'scores', 'between', '0', 'and', '1']), (0.009308650009030313, ['The', 'higher', 'softmax', 'scores', 'will', 'keep', 'the', 'value', 'of', 'words', 'the', 'model', 'learns', 'is', 'more', 'important']), (0.00929143861476681, ['This', 'will', 'help', 'the', 'decoder', 'focus', 'on', 'the', 'appropriate', 'words', 'in', 'the', 'input', 'during', 'the', 'decoding', 'process']), (0.009250431551384775, ['Scaling', 'down', 'the', 'Attention', 'scores', 'Softmax', 'of', 'the', 'Scaled', 'Scores', 'Next,', 'you', 'take', 'the', 'softmax', 'of', 'the', 'scaled', 'score', 'to', 'get', 'the', 'attention', 'weights,', 'which', 'gives', 'you', 'probability', 'values', 'between', '0', 'and', '1']), (0.009177155893025923, ['The', 'word', '“am”,', 'should', 'not', 'any', 'values', 'for', 'the', 'word', '“fine”']), (0.009124724908787858, ['Computing', 'Multi-headed', 'Attention', 'To', 'make', 'this', 'a', 'multi-headed', 'attention', 'computation,', 'you', 'need', 'to', 'split', 'the', 'query,', 'key,', 'and', 'value', 'into', 'N', 'vectors', 'before', 'applying', 'self-attention']), (0.009106476330445111, ['Taking', 'the', 'softmax', 'of', 'the', 'scaled', 'scores', 'to', 'get', 'probability', 'values', 'Multiply', 'Softmax', 'Output', 'with', 'Value', 'vector', 'Then', 'you', 'take', 'the', 'attention', 'weights', 'and', 'multiply', 'it', 'by', 'your', 'value', 'vector', 'to', 'get', 'an', 'output', 'vector']), (0.008933763229309909, ['Dot', 'Product', 'multiplication', 'of', 'the', 'query', 'and', 'the', 'key', 'The', 'score', 'matrix', 'determines', 'how', 'much', 'focus', 'should', 'a', 'word', 'be', 'put', 'on', 'other', 'words']), (0.008851314167336543, ['Decoder', 'Layer']), (0.008812546515362051, ['Linear', 'Classifier', 'with', 'Softmax', 'to', 'get', 'the', 'Output', 'Probabilities', 'The', 'decoder', 'then', 'takes', 'the', 'output,', 'add’s', 'it', 'to', 'the', 'list', 'of', 'decoder', 'inputs,', 'and', 'continues', 'decoding', 'again', 'until', 'a', 'token', 'is', 'predicted']), (0.008750136817794029, ['Encoder', 'Layer', 'Now', 'we', 'have', 'the', 'encoder', 'layer']), (0.008728754752972375, ['Hypothetical', 'reference', 'window', 'of', 'Attention,', 'RNN’s,', 'GRU’s', '&', 'LSTM’s', 'Attention', 'Is', 'All', 'You', 'Need', '—', 'Step', 'by', 'Step', 'Walkthrough', 'The', 'attention', 'mechanism’s', 'power', 'was', 'demonstrated', 'in', 'the', 'paper', '“Attention', 'Is', 'All', 'You', 'Need”,', 'where', 'the', 'authors', 'introduced', 'a', 'new', 'novel', 'neural', 'network', 'called', 'the', 'Transformers', 'which', 'is', 'an', 'attention-based', 'encoder-decoder', 'type', 'architecture']), (0.008688499765315313, ['The', 'word', '“am”', 'should', 'only', 'have', 'access', 'to', 'itself', 'and', 'the', 'words', 'before', 'it']), (0.008632444712557648, ['This', 'is', 'true', 'for', 'all', 'other', 'words,', 'where', 'they', 'can', 'only', 'attend', 'to', 'previous', 'words']), (0.008548045732038457, ['Then', 'you', 'feed', 'the', 'output', 'of', 'that', 'into', 'a', 'linear', 'layer', 'to', 'process']), (0.008535568120746802, ['Since', 'the', 'decoder', 'is', 'autoregressive', 'and', 'generates', 'the', 'sequence', 'word', 'by', 'word,', 'you', 'need', 'to', 'prevent', 'it', 'from', 'conditioning', 'to', 'future', 'tokens']), (0.008408206217897653, ['All', 'of', 'these', 'operations', 'are', 'to', 'encode', 'the', 'input', 'to', 'a', 'continuous', 'representation', 'with', 'attention', 'information']), (0.008383810253734726, ['This', 'layer', 'still', 'has', 'multiple', 'heads,', 'that', 'the', 'mask', 'is', 'being', 'applied', 'to,', 'before', 'getting', 'concatenated', 'and', 'fed', 'through', 'a', 'linear', 'layer', 'for', 'further', 'processing']), (0.008212253108247896, ['Transformer', 'Model', 'On', 'a', 'high', 'level,', 'the', 'encoder', 'maps', 'an', 'input', 'sequence', 'into', 'an', 'abstract', 'continuous', 'representation', 'that', 'holds', 'all', 'the', 'learned', 'information', 'of', 'that', 'input']), (0.008049858244144673, ['Linear', 'Classifier', 'and', 'Final', 'Softmax', 'for', 'Output', 'Probabilities', 'The', 'output', 'of', 'the', 'final', 'pointwise', 'feedforward', 'layer', 'goes', 'through', 'a', 'final', 'linear', 'layer,', 'that', 'acts', 'as', 'a', 'classifier']), (0.00803401182741913, ['The', 'output', 'of', 'the', 'residual', 'connection', 'goes', 'through', 'a', 'layer', 'normalization']), (0.008023428736153523, ['So', 'in', 'our', 'example,', 'it’s', 'possible', 'that', 'our', 'model', 'can', 'learn', 'to', 'associate', 'the', 'word', '“you”,', 'with', '“how”', 'and', '“are”']), (0.008017199035344223, ['Attention', 'scores', 'from', 'the', 'dot', 'product']), (0.007929353974724846, ['Adding', 'a', 'look-ahead', 'mask', 'to', 'the', 'scaled', 'scores', 'The', 'reason', 'for', 'the', 'mask', 'is', 'because', 'once', 'you', 'take', 'the', 'softmax', 'of', 'the', 'masked', 'scores,', 'the', 'negative', 'infinities', 'get', 'zeroed', 'out,', 'leaving', 'zero', 'attention', 'scores', 'for', 'future', 'tokens']), (0.007908044246178185, ['This', 'process', 'matches', 'the', 'encoder’s', 'input', 'to', 'the', 'decoder’s', 'input,', 'allowing', 'the', 'decoder', 'to', 'decide', 'which', 'encoder', 'input', 'is', 'relevant', 'to', 'put', 'a', 'focus', 'on']), (0.007850831558754472, ['For', 'example,', 'if', 'you', 'have', '10,000', 'classes', 'for', '10,000', 'words,', 'the', 'output', 'of', 'that', 'classier', 'will', 'be', 'of', 'size', '10,000']), (0.007739905525970249, ['Because', 'the', 'transformer', 'encoder', 'has', 'no', 'recurrence', 'like', 'recurrent', 'neural', 'networks,', 'we', 'must', 'add', 'some', 'information', 'about', 'the', 'positions', 'into', 'the', 'input', 'embeddings']), (0.007724926746773095, ['Each', 'head', 'produces', 'an', 'output', 'vector', 'that', 'gets', 'concatenated', 'into', 'a', 'single', 'vector', 'before', 'going', 'through', 'the', 'final', 'linear', 'layer']), (0.007723653536137959, ['Scaling', 'Down', 'the', 'Attention', 'Scores', 'Then,', 'the', 'scores', 'get', 'scaled', 'down', 'by', 'getting', 'divided', 'by', 'the', 'square', 'root', 'of', 'the', 'dimension', 'of', 'query', 'and', 'key']), (0.007708567161487125, ['Query,', 'Key,', 'and', 'Value', 'Vectors', 'To', 'achieve', 'self-attention,', 'we', 'feed', 'the', 'input', 'into', '3', 'distinct', 'fully', 'connected', 'layers', 'to', 'create', 'the', 'query,', 'key,', 'and', 'value', 'vectors']), (0.007706936419580402, ['The', 'output', 'of', 'that', 'is', 'then', 'again', 'added', 'to', 'the', 'input', 'of', 'the', 'pointwise', 'feed-forward', 'network', 'and', 'further', 'normalized']), (0.007652436919134711, ['When', 'you', 'add', 'the', 'mask', 'to', 'the', 'scaled', 'attention', 'scores,', 'you', 'get', 'a', 'matrix', 'of', 'the', 'scores,', 'with', 'the', 'top', 'right', 'triangle', 'filled', 'with', 'negativity', 'infinities']), (0.007630245373723772, ['This', 'allows', 'the', 'model', 'to', 'be', 'more', 'confident', 'about', 'which', 'words', 'to', 'attend', 'too']), (0.007383147546616534, ['Decoder', 'Layer', 'The', 'decoder’s', 'job', 'is', 'to', 'generate', 'text', 'sequences']), (0.007349264692489651, ['This', 'is', 'true', 'for', 'all', 'other', 'words']), (0.007340783094901604, ['Decoder', 'Input', 'Embeddings', '&', 'Positional', 'Encoding', 'The', 'beginning', 'of', 'the', 'decoder', 'is', 'pretty', 'much', 'the', 'same', 'as', 'the', 'encoder']), (0.0073138199598396835, ['converting', 'Words', 'to', 'Input', 'Embeddings', 'Positional', 'Encoding', 'The', 'next', 'step', 'is', 'to', 'inject', 'positional', 'information', 'into', 'the', 'embeddings']), (0.007197634192367358, ['The', 'lower', 'scores', 'will', 'drown', 'out', 'the', 'irrelevant', 'words']), (0.007192190464985935, ['We', 'take', 'the', 'index', 'of', 'the', 'highest', 'probability', 'score,', 'and', 'that', 'equals', 'our', 'predicted', 'word']), (0.007083875802121429, ['The', 'decoder', 'then', 'takes', 'that', 'continuous', 'representation', 'and', 'step', 'by', 'step', 'generates', 'a', 'single', 'output', 'while', 'also', 'being', 'fed', 'the', 'previous', 'output']), (0.007019387675821468, ['The', 'Encoders', 'layers', 'job', 'is', 'to', 'map', 'all', 'input', 'sequences', 'into', 'an', 'abstract', 'continuous', 'representation', 'that', 'holds', 'the', 'learned', 'information', 'for', 'that', 'entire', 'sequence']), (0.007005773436421986, ['This', 'essentially', 'tells', 'the', 'model', 'to', 'put', 'no', 'focus', 'on', 'those', 'words']), (0.006815372316309101, ['Look-Ahead', 'Mask', 'The', 'mask', 'is', 'a', 'matrix', 'that’s', 'the', 'same', 'size', 'as', 'the', 'attention', 'scores', 'filled', 'with', 'values', 'of', '0’s', 'and', 'negative', 'infinities']), (0.006773108329241246, ['Attention', 'Mechanism', 'To', 'understand', 'transformers', 'we', 'first', 'must', 'understand', 'the', 'attention', 'mechanism']), (0.006755250966886971, ['Dot', 'Product', 'of', 'Query', 'and', 'Key', 'After', 'feeding', 'the', 'query,', 'key,', 'and', 'value', 'vector', 'through', 'a', 'linear', 'layer,', 'the', 'queries', 'and', 'keys', 'undergo', 'a', 'dot', 'product', 'matrix', 'multiplication', 'to', 'produce', 'a', 'score', 'matrix']), (0.0065137197217321925, ['We’ll', 'prime', 'the', 'model', 'with', 'our', 'input,', 'and', 'the', 'model', 'will', 'generate', 'the', 'rest']), (0.006435795557861956, ['Encoder', 'Wrap-up', 'That', 'wraps', 'up', 'the', 'encoder', 'layer']), (0.006392232329417964, ['It', 'contains', '2', 'sub-modules,', 'multi-headed', 'attention,', 'followed', 'by', 'a', 'fully', 'connected', 'network']), (0.006356143640228866, ['The', 'pointwise', 'feed-forward', 'network', 'is', 'a', 'couple', 'of', 'linear', 'layers', 'with', 'a', 'ReLU', 'activation', 'in', 'between']), (0.006301162312724359, ['The', 'decoder', 'is', 'autoregressive', 'as', 'it', 'generates', 'a', 'token', '1', 'at', 'a', 'time', 'while', 'being', 'fed', 'in', 'the', 'previous', 'outputs']), (0.006292698238052719, ['The', 'decoder', 'stops', 'decoding', 'when', 'it', 'generates', 'a', 'token', 'as', 'an', 'output']), (0.00621495187832357, ['A', 'transformer', 'model', 'can', '“attend”', 'or', '“focus”', 'on', 'all', 'previous', 'tokens', 'that', 'have', 'been', 'generated']), (0.006037563677123441, ['The', 'attention', 'mechanism,', 'in', 'theory,', 'and', 'given', 'enough', 'compute', 'resources,', 'have', 'an', 'infinite', 'window', 'to', 'reference', 'from,', 'therefore', 'being', 'capable', 'of', 'using', 'the', 'entire', 'context', 'of', 'the', 'story', 'while', 'generating', 'the', 'text']), (0.005844781539059747, ['The', 'ability', 'to', 'know', 'what', 'words', 'to', 'attend', 'too', 'is', 'all', 'learned', 'during', 'training', 'through', 'backpropagation']), (0.005804396829877209, ['RNN’s', 'have', 'a', 'shorter', 'window', 'to', 'reference', 'from,', 'so', 'when', 'the', 'story', 'gets', 'longer,', 'RNN’s', 'can’t', 'access', 'words', 'generated', 'earlier', 'in', 'the', 'sequence']), (0.00580262787224999, ['It’s', 'also', 'possible', 'that', 'the', 'model', 'learns', 'that', 'words', 'structured', 'in', 'this', 'pattern', 'are', 'typically', 'a', 'question', 'so', 'respond', 'appropriately']), (0.005781557467756223, ['But', 'the', 'power', 'of', 'the', 'attention', 'mechanism', 'is', 'that', 'it', 'doesn’t', 'suffer', 'from', 'short', 'term', 'memory']), (0.005718142516136553, ['Transformers', 'leverage', 'the', 'power', 'of', 'the', 'attention', 'mechanism', 'to', 'make', 'better', 'predictions']), (0.005713616965999812, ['The', 'Attention', 'mechanism', 'enables', 'the', 'transformers', 'to', 'have', 'extremely', 'long', 'term', 'memory']), (0.005667571639369732, ['In', 'this', 'post,', 'we’ll', 'focus', 'on', 'the', 'one', 'paper', 'that', 'started', 'it', 'all,', '“Attention', 'is', 'all', 'you', 'need”']), (0.00560413964461824, ['The', 'mask', 'is', 'added', 'before', 'calculating', 'the', 'softmax,', 'and', 'after', 'scaling', 'the', 'scores']), (0.005563215813071868, ['By', 'doing', 'a', 'softmax', 'the', 'higher', 'scores', 'get', 'heighten,', 'and', 'lower', 'scores', 'are', 'depressed']), (0.005540714825996669, ['The', 'layer', 'normalizations', 'are', 'used', 'to', 'stabilize', 'the', 'network', 'which', 'results', 'in', 'substantially', 'reducing', 'the', 'training', 'time', 'necessary']), (0.0054837319379625275, ['Transformers', 'are', 'the', 'rage', 'in', 'deep', 'learning', 'nowadays,', 'but', 'how', 'do', 'they', 'work?', 'Why', 'have', 'they', 'outperform', 'the', 'previous', 'king', 'of', 'sequence', 'problems,', 'like', 'recurrent', 'neural', 'networks,', 'GRU’s,', 'and', 'LSTM’s?', 'You’ve', 'probably', 'heard', 'of', 'different', 'famous', 'transformers', 'models', 'like', 'BERT,', 'GPT,', 'and', 'GPT2']), (0.005371071697353115, ['In', 'theory,', 'each', 'head', 'would', 'learn', 'something', 'different', 'therefore', 'giving', 'the', 'encoder', 'model', 'more', 'representation', 'power']), (0.005303740032279831, ['For', 'every', 'odd', 'index', 'on', 'the', 'input', 'vector,', 'create', 'a', 'vector', 'using', 'the', 'cos', 'function']), (0.00523145148372606, ['Then', 'add', 'those', 'vectors', 'to', 'their', 'corresponding', 'input', 'embeddings']), (0.0050220943924959615, ['The', 'higher', 'the', 'score', 'the', 'more', 'focus']), (0.004968169458222811, ['There', 'are', 'also', 'residual', 'connections', 'around', 'each', 'of', 'the', 'two', 'sublayers', 'followed', 'by', 'a', 'layer', 'normalization']), (0.004684777818047582, ['The', 'decoder', 'has', 'a', 'similar', 'sub-layer', 'as', 'the', 'encoder']), (0.004526720898516775, ['To', 'prevent', 'the', 'decoder', 'from', 'looking', 'at', 'future', 'tokens,', 'you', 'apply', 'a', 'look', 'ahead', 'mask']), (0.004385590857977359, ['The', 'sin', 'and', 'cosine', 'functions', 'were', 'chosen', 'in', 'tandem', 'because', 'they', 'have', 'linear', 'properties', 'the', 'model', 'can', 'easily', 'learn', 'to', 'attend', 'to']), (0.004250129703568717, ['This', 'is', 'done', 'using', 'positional', 'encoding']), (0.004216484710985326, ['The', 'paper', 'applied', 'the', 'Transformer', 'model', 'on', 'a', 'neural', 'machine', 'translation', 'problem']), (0.004173268491170284, ['This', 'successfully', 'gives', 'the', 'network', 'information', 'on', 'the', 'position', 'of', 'each', 'vector']), (0.004059976289745087, ['Transformers', 'can', 'be', 'better', 'especially', 'if', 'you', 'want', 'to', 'encode', 'or', 'generate', 'long', 'sequences']), (0.004033739738442327, ['Let’s', 'take', 'a', 'look', 'at', 'how', 'this', 'works']), (0.0039950973207651605, ['The', 'residual', 'connections', 'help', 'the', 'network', 'train,', 'by', 'allowing', 'gradients', 'to', 'flow', 'through', 'the', 'networks', 'directly']), (0.003952653997588351, ['Transformer', 'output:', '“and', 'began', 'to', 'colonized', 'Earth,', 'a', 'certain', 'group', 'of', 'extraterrestrials', 'began', 'to', 'manipulate', 'our', 'society', 'through', 'their', 'influences', 'of', 'a', 'certain', 'number', 'of', 'the', 'elite', 'to', 'keep', 'and', 'iron', 'grip', 'over', 'the', 'populace.”', 'Ok,', 'so', 'the', 'story', 'is', 'a', 'little', 'dark', 'but', 'what’s', 'interesting', 'is', 'how', 'the', 'model', 'generated', 'it']), (0.003929129673480688, ['Encoder', 'Self-Attention', 'Operations']), (0.0038441702757113674, ['Each', 'self-attention', 'process', 'is', 'called', 'a', 'head']), (0.003794590623417895, ['Stacked', 'Encoder', 'and', 'Decoder', 'And', 'That’s', 'It!', 'And', 'that’s', 'it!', 'That’s', 'the', 'mechanics', 'of', 'the', 'transformers']), (0.003742392689882511, ['Our', 'input:', '“As', 'Aliens', 'entered', 'our', 'planet”']), (0.003726353903630572, ['For', 'every', 'even', 'index,', 'create', 'a', 'vector', 'using', 'the', 'sin', 'function']), (0.0037070062007277193, ['The', 'split', 'vectors', 'then', 'go', 'through', 'the', 'self-attention', 'process', 'individually']), (0.0036975333305534477, ['Let’s', 'walk', 'through', 'an', 'example']), (0.0036975333305534477, ['Let’s', 'walk', 'through', 'an', 'example']), (0.0035982761782466866, ['“The', 'query', 'key', 'and', 'value', 'concept', 'come', 'from', 'retrieval', 'systems']), (0.0035929754697790746, ['For', 'example,', 'when', 'you', 'type', 'a', 'query', 'to', 'search', 'for', 'some', 'video', 'on', 'Youtube,', 'the', 'search', 'engine', 'will', 'map', 'your', 'query', 'against', 'a', 'set', 'of', 'keys', '(video', 'title,', 'description', 'etc.)', 'associated', 'with', 'candidate', 'videos', 'in', 'the', 'database,', 'then', 'present', 'you', 'the', 'best', 'matched', 'videos', '(values)']), (0.0035878129945890597, ['Recurrent', 'Neural', 'networks', 'try', 'to', 'achieve', 'similar', 'things,', 'but', 'because', 'they', 'suffer', 'from', 'short', 'term', 'memory']), (0.0035256340939284975, ['They', 'are', 'used', 'in', 'many', 'applications', 'like', 'machine', 'language', 'translation,', 'conversational', 'chatbots,', 'and', 'even', 'to', 'power', 'better', 'search', 'engines']), (0.003492014039954455, ['This', 'is', 'still', 'true', 'for', 'Gated', 'Recurrent', 'Units', '(GRU’s)', 'and', 'Long-short', 'Term', 'Memory', '(LSTM’s)', 'networks,', 'although', 'they', 'do', 'a', 'bigger', 'capacity', 'to', 'achieve', 'longer-term', 'memory,', 'therefore,', 'having', 'a', 'longer', 'window', 'to', 'reference', 'from']), (0.003490895793475297, ['Transformers', 'are', 'taking', 'the', 'natural', 'language', 'processing', 'world', 'by', 'storm']), (0.0034347086274565696, ['Let’s', 'walk', 'through', 'the', 'decoding', 'steps']), (0.0034226012118477086, ['This', 'is', 'how', 'the', 'queries', 'are', 'mapped', 'to', 'the', 'keys']), (0.003213852778883776, ['Because', 'of', 'the', 'transformer', 'architecture,', 'the', 'natural', 'language', 'processing', 'industry', 'can', 'achieve', 'unprecedented', 'results']), (0.0031000871266480354, ['Using', 'Hugging', 'Face’s', 'Write', 'With', 'Transformer', 'application,', 'we', 'can', 'do', 'just', 'that']), (0.002922038998885524, ['This', 'method', 'is', 'called', 'masking']), (0.0029119631399797067, ['This', 'is', 'called', 'a', 'residual', 'connection']), (0.0028414176349567605, ['We', 'won’t', 'go', 'into', 'the', 'mathematical', 'details', 'of', 'positional', 'encoding,', 'but', 'here', 'are', 'the', 'basics']), (0.002817940842463092, ['Reference', 'this', 'when', 'looking', 'at', 'Illustrations', 'below']), (0.0025989506257575864, ['The', 'authors', 'came', 'up', 'with', 'a', 'clever', 'trick', 'using', 'sin', 'and', 'cosine', 'functions']), (0.0025797813244151286, ['These', 'incredible', 'models', 'are', 'breaking', 'multiple', 'NLP', 'records', 'and', 'pushing', 'the', 'state', 'of', 'the', 'art']), (0.0024537468733378965, ['This', 'is', 'to', 'allow', 'for', 'more', 'stable', 'gradients,', 'as', 'multiplying', 'values', 'can', 'have', 'exploding', 'effects']), (0.002330135314318617, ['Check', 'out', 'michaelphi.com', 'for', 'more', 'content', 'like', 'this']), (0.002320250682129676, ['Check', 'out', 'the', 'link', 'below', 'if', 'you’d', 'like', 'to', 'watch', 'the', 'video', 'version', 'instead']), (0.0023055066403569924, ['Say', 'we', 'want', 'to', 'write', 'a', 'short', 'sci-fi', 'novel', 'with', 'a', 'generative', 'transformer']), (0.0020518933464564043, ['For', 'our', 'case,', 'the', 'highest', 'probability', 'prediction', 'is', 'the', 'final', 'class', 'which', 'is', 'assigned', 'to', 'the', 'end', 'token']), (0.0018997240351033803, ['The', 'classifier', 'is', 'as', 'big', 'as', 'the', 'number', 'of', 'classes', 'you', 'have']), (0.0018517132955359118, ['Reference', 'This', 'diagram', 'while', 'reading']), (0.0017699289463540273, ['In', 'this', 'post,', 'we’ll', 'demonstrate', 'how', 'it’ll', 'work', 'for', 'a', 'conversational', 'chatbot']), (0.0017628106121404747, ['What', 'are', 'these', 'vectors', 'exactly?', 'I', 'found', 'a', 'good', 'explanation', 'on', 'stack', 'exchange', 'stating…']), (0.001033413709955219, [''])]\n","\n","Summarize Text 0: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence\n","\n","Summarize Text 1: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values\n","\n","Summarize Text 2: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input\n","\n","Summarize Text 3: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after\n","\n","Summarize Text 4: \n","Splitting Q, K, V, N times before applying self-attention To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after. This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer\n","\n","\n","\n","Article a4: =======================================================================\n","\n","Old: ------------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.033335669736734935, ['Notice', 'that', 'here', 'we', 'are', 'trying', 'to', 'predict', 'a', 'target', 'word', '(day)', 'using', 'a', 'single', 'context', 'input', 'word', 'great']), (0.032027449200539286, ['So,', 'we', 'have', 'seen', 'how', 'word', 'representations', 'are', 'generated', 'using', 'the', 'context', 'words']), (0.03122705232741279, ['CBOW', 'Model', 'The', 'input', 'or', 'the', 'context', 'word', 'is', 'a', 'one', 'hot', 'encoded', 'vector', 'of', 'size', 'V']), (0.030733802168111217, ['It', 'can', 'be', 'obtained', 'using', 'two', 'methods', '(both', 'involving', 'Neural', 'Networks):', 'Skip', 'Gram', 'and', 'Common', 'Bag', 'Of', 'Words', '(CBOW)', 'CBOW', 'Model:', 'This', 'method', 'takes', 'the', 'context', 'of', 'each', 'word', 'as', 'the', 'input', 'and', 'tries', 'to', 'predict', 'the', 'word', 'corresponding', 'to', 'the', 'context']), (0.03007835029698586, ['More', 'specifically,', 'we', 'use', 'the', 'one', 'hot', 'encoding', 'of', 'the', 'input', 'word', 'and', 'measure', 'the', 'output', 'error', 'compared', 'to', 'one', 'hot', 'encoding', 'of', 'the', 'target', 'word', '(day)']), (0.029184659205107794, ['The', 'words', 'in', 'context', 'of', 'this', 'word', 'would', 'get', 'a', 'greater', 'share', 'of', 'this', 'dependence']), (0.028174241729458406, ['Intuitively,', 'we', 'introduce', 'some', 'dependence', 'of', 'one', 'word', 'on', 'the', 'other', 'words']), (0.027914693583487808, ['But,', 'the', 'above', 'model', 'used', 'a', 'single', 'context', 'word', 'to', 'predict', 'the', 'target']), (0.02748328134198404, ['We', 'can', 'use', 'multiple', 'context', 'words', 'to', 'do', 'the', 'same']), (0.026551575124328933, ['What', 'does', 'this', 'mean?', 'For', 'each', 'context', 'position,', 'we', 'get', 'C', 'probability', 'distributions', 'of', 'V', 'probabilities,', 'one', 'for', 'each', 'word']), (0.02598965510811069, ['Let', 'the', 'input', 'to', 'the', 'Neural', 'Network', 'be', 'the', 'word,', 'great']), (0.025624586269611106, ['Google', 'images', 'The', 'above', 'model', 'takes', 'C', 'context', 'words']), (0.0254409824396784, ['In', 'the', 'process', 'of', 'predicting', 'the', 'target', 'word,', 'we', 'learn', 'the', 'vector', 'representation', 'of', 'the', 'target', 'word']), (0.025271386954519096, ['We', 'input', 'the', 'target', 'word', 'into', 'the', 'network']), (0.02502123938710488, ['What', 'are', 'word', 'embeddings', 'exactly?', 'Loosely', 'speaking,', 'they', 'are', 'vector', 'representations', 'of', 'a', 'particular', 'word']), (0.02415527569784813, ['When', 'Wvn', 'is', 'used', 'to', 'calculate', 'hidden', 'layer', 'inputs,', 'we', 'take', 'an', 'average', 'over', 'all', 'these', 'C', 'context', 'word', 'inputs']), (0.02363122947131788, ['Now,', 'let', 'us', 'create', 'a', 'one-hot', 'encoded', 'vector', 'for', 'each', 'of', 'these', 'words', 'in', 'V']), (0.02303114876483879, ['Having', 'said', 'this,', 'what', 'follows', 'is', 'how', 'do', 'we', 'generate', 'them?', 'More', 'importantly,', 'how', 'do', 'they', 'capture', 'the', 'context?', 'Word2Vec', 'is', 'one', 'of', 'the', 'most', 'popular', 'technique', 'to', 'learn', 'word', 'embeddings', 'using', 'shallow', 'neural', 'network']), (0.02298880991615865, ['It', 'is', 'capable', 'of', 'capturing', 'context', 'of', 'a', 'word', 'in', 'a', 'document,', 'semantic', 'and', 'syntactic', 'similarity,', 'relation', 'with', 'other', 'words,', 'etc']), (0.022709717296780108, ['We', 'can', 'use', 'the', 'target', 'word', '(whose', 'representation', 'we', 'want', 'to', 'generate)', 'to', 'predict', 'the', 'context', 'and', 'in', 'the', 'process,', 'we', 'produce', 'the', 'representations']), (0.022294265741342266, ['In', 'one', 'hot', 'encoding', 'representations,', 'all', 'the', 'words', 'are', 'independent', 'of', 'each', 'other,', 'as', 'mentioned', 'earlier']), (0.021830021043666156, ['Our', 'objective', 'is', 'to', 'have', 'words', 'with', 'similar', 'context', 'occupy', 'close', 'spatial', 'positions']), (0.020984370867585245, ['Word', 'embedding', 'is', 'one', 'of', 'the', 'most', 'popular', 'representation', 'of', 'document', 'vocabulary']), (0.018459692816576147, ['It', 'just', 'gives', 'you', 'a', 'high-level', 'idea', 'of', 'what', 'word', 'embeddings', 'are', 'and', 'how', 'Word2Vec', 'works']), (0.018104858664157255, ['According', 'to', 'Mikolov,', 'Skip', 'Gram', 'works', 'well', 'with', 'small', 'amount', 'of', 'data', 'and', 'is', 'found', 'to', 'represent', 'rare', 'words', 'well']), (0.01787935946249495, ['We', 'would', 'have', 'a', 'vector', 'of', 'zeros', 'except', 'for', 'the', 'element', 'at', 'the', 'index', 'representing', 'the', 'corresponding', 'word', 'in', 'the', 'vocabulary']), (0.01773002445198023, ['On', 'the', 'other', 'hand,', 'CBOW', 'is', 'faster', 'and', 'has', 'better', 'representations', 'for', 'more', 'frequent', 'words']), (0.016129032258064516, ['Thanks', 'for', 'reading!', 'I', 'have', 'started', 'my', 'personal', 'blog', 'and', 'I', 'don’t', 'intend', 'to', 'write', 'more', 'amazing', 'articles', 'on', 'Medium']), (0.016129032258064516, ['Support', 'my', 'blog', 'by', 'subscribing', 'to', 'thenlp.space']), (0.016029043714153056, ['This', 'means', '‘good’', 'and', '‘great’', 'are', 'as', 'different', 'as', '‘day’', 'and', '‘have’,', 'which', 'is', 'not', 'true']), (0.015932529805646616, ['If', 'we', 'construct', 'an', 'exhaustive', 'vocabulary', '(let’s', 'call', 'it', 'V),', 'it', 'would', 'have', 'V', '=', '{Have,', 'a,', 'good,', 'great,', 'day}']), (0.013978519334591669, ['Have', '=', '[1,0,0,0,0]`;', 'a=[0,1,0,0,0]`', ';', 'good=[0,0,1,0,0]`', ';', 'great=[0,0,0,1,0]`', ';', 'day=[0,0,0,0,1]`', '(`', 'represents', 'transpose)', 'If', 'we', 'try', 'to', 'visualize', 'these', 'encodings,', 'we', 'can', 'think', 'of', 'a', '5', 'dimensional', 'space,', 'where', 'each', 'word', 'occupies', 'one', 'of', 'the', 'dimensions', 'and', 'has', 'nothing', 'to', 'do', 'with', 'the', 'rest', '(no', 'projection', 'along', 'the', 'other', 'dimensions)']), (0.011951655740640131, ['But', 'there’s', 'one', 'more', 'way', 'we', 'can', 'do', 'the', 'same']), (0.011888633389629897, ['Length', 'of', 'our', 'one-hot', 'encoded', 'vector', 'would', 'be', 'equal', 'to', 'the', 'size', 'of', 'V', '(=5)']), (0.01186926254253794, ['Skip-Gram', 'model:', 'This', 'looks', 'like', 'multiple-context', 'CBOW', 'model', 'just', 'got', 'flipped']), (0.011741537884786102, ['The', 'hidden', 'layer', 'contains', 'N', 'neurons', 'and', 'the', 'output', 'is', 'again', 'a', 'V', 'length', 'vector', 'with', 'the', 'elements', 'being', 'the', 'softmax', 'values']), (0.011711001477712101, ['That', 'particular', 'element', 'would', 'be', 'one']), (0.010830969596835528, ['Let’s', 'get', 'the', 'terms', 'in', 'the', 'picture', 'right:', '-', 'Wvn', 'is', 'the', 'weight', 'matrix', 'that', 'maps', 'the', 'input', 'x', 'to', 'the', 'hidden', 'layer', '(V*N', 'dimensional', 'matrix)', '-W`nv', 'is', 'the', 'weight', 'matrix', 'that', 'maps', 'the', 'hidden', 'layer', 'outputs', 'to', 'the', 'final', 'output', 'layer', '(N*V', 'dimensional', 'matrix)', 'I', 'won’t', 'get', 'into', 'the', 'mathematics']), (0.010816989721808109, ['Why', 'do', 'we', 'need', 'them?', 'Consider', 'the', 'following', 'similar', 'sentences:', 'Have', 'a', 'good', 'day', 'and', 'Have', 'a', 'great', 'day']), (0.010501655818967282, ['Consider', 'our', 'example:', 'Have', 'a', 'great', 'day']), (0.00994278857466075, ['The', 'model', 'outputs', 'C', 'probability', 'distributions']), (0.009342740857461857, ['The', 'hidden', 'layer', 'neurons', 'just', 'copy', 'the', 'weighted', 'sum', 'of', 'inputs', 'to', 'the', 'next', 'layer']), (0.008606525067777662, ['The', 'encodings', 'below', 'would', 'explain', 'this', 'better']), (0.008577192427333423, ['angle', 'close', 'to', '0']), (0.008449322265865589, ['For', 'example,', 'to', 'make', 'the', 'algorithm', 'computationally', 'more', 'efficient,', 'tricks', 'like', 'Hierarchical', 'Softmax', 'and', 'Skip-Gram', 'Negative', 'Sampling', 'are', 'used']), (0.008377996082615928, ['In', 'both', 'the', 'cases,', 'the', 'network', 'uses', 'back-propagation', 'to', 'learn']), (0.00831230181526714, ['What’s', 'ahead?', 'The', 'above', 'explanation', 'is', 'a', 'very', 'basic', 'one']), (0.008287493363545072, ['Google', 'Images', 'Here', 'comes', 'the', 'idea', 'of', 'generating', 'distributed', 'representations']), (0.008182717252577645, ['All', 'of', 'it', 'can', 'be', 'found', 'here']), (0.008061199171903469, ['The', 'only', 'non-linearity', 'is', 'the', 'softmax', 'calculations', 'in', 'the', 'output', 'layer']), (0.007943389524799015, ['Mathematically,', 'the', 'cosine', 'of', 'the', 'angle', 'between', 'such', 'vectors', 'should', 'be', 'close', 'to', '1,', 'i.e']), (0.007112008015189133, ['Detailed', 'math', 'can', 'be', 'found', 'here', 'Who', 'wins?', 'Both', 'have', 'their', 'own', 'advantages', 'and', 'disadvantages']), (0.007054681700456376, ['Another', 'variant,', 'called', 'Skip', 'Gram', 'model', 'does', 'this']), (0.006992321103285103, ['We’ll', 'just', 'get', 'an', 'idea', 'of', 'what’s', 'going', 'on']), (0.005977600029210023, ['Let', 'us', 'look', 'deeper', 'into', 'the', 'actual', 'architecture']), (0.0054025459940283405, ['They', 'hardly', 'have', 'different', 'meaning']), (0.005285667823713152, ['How', 'does', 'Word2Vec', 'work?', 'Word2Vec', 'is', 'a', 'method', 'to', 'construct', 'such', 'an', 'embedding']), (0.0048890012202425625, ['Let’s', 'tackle', 'this', 'part', 'by', 'part']), (0.004178168837038958, ['It', 'was', 'developed', 'by', 'Tomas', 'Mikolov', 'in', '2013', 'at', 'Google']), (0.004132275319732573, ['There’s', 'a', 'lot', 'more', 'to', 'it']), (0.003938041635624346, ['To', 'some', 'extent', 'that', 'is', 'true']), (0.003586759306313246, ['There', 'is', 'no', 'activation', 'like', 'sigmoid,', 'tanh', 'or', 'ReLU'])]\n","\n","Summarize Text 0: \n","Notice that here we are trying to predict a target word (day) using a single context input word great\n","\n","Summarize Text 1: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words\n","\n","Summarize Text 2: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V\n","\n","Summarize Text 3: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context\n","\n","Summarize Text 4: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day)\n","\n","Updated: ------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.033335669736734935, ['Notice', 'that', 'here', 'we', 'are', 'trying', 'to', 'predict', 'a', 'target', 'word', '(day)', 'using', 'a', 'single', 'context', 'input', 'word', 'great']), (0.032027449200539286, ['So,', 'we', 'have', 'seen', 'how', 'word', 'representations', 'are', 'generated', 'using', 'the', 'context', 'words']), (0.03122705232741279, ['CBOW', 'Model', 'The', 'input', 'or', 'the', 'context', 'word', 'is', 'a', 'one', 'hot', 'encoded', 'vector', 'of', 'size', 'V']), (0.030733802168111217, ['It', 'can', 'be', 'obtained', 'using', 'two', 'methods', '(both', 'involving', 'Neural', 'Networks):', 'Skip', 'Gram', 'and', 'Common', 'Bag', 'Of', 'Words', '(CBOW)', 'CBOW', 'Model:', 'This', 'method', 'takes', 'the', 'context', 'of', 'each', 'word', 'as', 'the', 'input', 'and', 'tries', 'to', 'predict', 'the', 'word', 'corresponding', 'to', 'the', 'context']), (0.03007835029698586, ['More', 'specifically,', 'we', 'use', 'the', 'one', 'hot', 'encoding', 'of', 'the', 'input', 'word', 'and', 'measure', 'the', 'output', 'error', 'compared', 'to', 'one', 'hot', 'encoding', 'of', 'the', 'target', 'word', '(day)']), (0.029184659205107794, ['The', 'words', 'in', 'context', 'of', 'this', 'word', 'would', 'get', 'a', 'greater', 'share', 'of', 'this', 'dependence']), (0.028174241729458406, ['Intuitively,', 'we', 'introduce', 'some', 'dependence', 'of', 'one', 'word', 'on', 'the', 'other', 'words']), (0.027914693583487808, ['But,', 'the', 'above', 'model', 'used', 'a', 'single', 'context', 'word', 'to', 'predict', 'the', 'target']), (0.02748328134198404, ['We', 'can', 'use', 'multiple', 'context', 'words', 'to', 'do', 'the', 'same']), (0.026551575124328933, ['What', 'does', 'this', 'mean?', 'For', 'each', 'context', 'position,', 'we', 'get', 'C', 'probability', 'distributions', 'of', 'V', 'probabilities,', 'one', 'for', 'each', 'word']), (0.02598965510811069, ['Let', 'the', 'input', 'to', 'the', 'Neural', 'Network', 'be', 'the', 'word,', 'great']), (0.025624586269611106, ['Google', 'images', 'The', 'above', 'model', 'takes', 'C', 'context', 'words']), (0.0254409824396784, ['In', 'the', 'process', 'of', 'predicting', 'the', 'target', 'word,', 'we', 'learn', 'the', 'vector', 'representation', 'of', 'the', 'target', 'word']), (0.025271386954519096, ['We', 'input', 'the', 'target', 'word', 'into', 'the', 'network']), (0.02502123938710488, ['What', 'are', 'word', 'embeddings', 'exactly?', 'Loosely', 'speaking,', 'they', 'are', 'vector', 'representations', 'of', 'a', 'particular', 'word']), (0.02415527569784813, ['When', 'Wvn', 'is', 'used', 'to', 'calculate', 'hidden', 'layer', 'inputs,', 'we', 'take', 'an', 'average', 'over', 'all', 'these', 'C', 'context', 'word', 'inputs']), (0.02363122947131788, ['Now,', 'let', 'us', 'create', 'a', 'one-hot', 'encoded', 'vector', 'for', 'each', 'of', 'these', 'words', 'in', 'V']), (0.02303114876483879, ['Having', 'said', 'this,', 'what', 'follows', 'is', 'how', 'do', 'we', 'generate', 'them?', 'More', 'importantly,', 'how', 'do', 'they', 'capture', 'the', 'context?', 'Word2Vec', 'is', 'one', 'of', 'the', 'most', 'popular', 'technique', 'to', 'learn', 'word', 'embeddings', 'using', 'shallow', 'neural', 'network']), (0.02298880991615865, ['It', 'is', 'capable', 'of', 'capturing', 'context', 'of', 'a', 'word', 'in', 'a', 'document,', 'semantic', 'and', 'syntactic', 'similarity,', 'relation', 'with', 'other', 'words,', 'etc']), (0.022709717296780108, ['We', 'can', 'use', 'the', 'target', 'word', '(whose', 'representation', 'we', 'want', 'to', 'generate)', 'to', 'predict', 'the', 'context', 'and', 'in', 'the', 'process,', 'we', 'produce', 'the', 'representations']), (0.022294265741342266, ['In', 'one', 'hot', 'encoding', 'representations,', 'all', 'the', 'words', 'are', 'independent', 'of', 'each', 'other,', 'as', 'mentioned', 'earlier']), (0.021830021043666156, ['Our', 'objective', 'is', 'to', 'have', 'words', 'with', 'similar', 'context', 'occupy', 'close', 'spatial', 'positions']), (0.020984370867585245, ['Word', 'embedding', 'is', 'one', 'of', 'the', 'most', 'popular', 'representation', 'of', 'document', 'vocabulary']), (0.018459692816576147, ['It', 'just', 'gives', 'you', 'a', 'high-level', 'idea', 'of', 'what', 'word', 'embeddings', 'are', 'and', 'how', 'Word2Vec', 'works']), (0.018104858664157255, ['According', 'to', 'Mikolov,', 'Skip', 'Gram', 'works', 'well', 'with', 'small', 'amount', 'of', 'data', 'and', 'is', 'found', 'to', 'represent', 'rare', 'words', 'well']), (0.01787935946249495, ['We', 'would', 'have', 'a', 'vector', 'of', 'zeros', 'except', 'for', 'the', 'element', 'at', 'the', 'index', 'representing', 'the', 'corresponding', 'word', 'in', 'the', 'vocabulary']), (0.01773002445198023, ['On', 'the', 'other', 'hand,', 'CBOW', 'is', 'faster', 'and', 'has', 'better', 'representations', 'for', 'more', 'frequent', 'words']), (0.016129032258064516, ['Thanks', 'for', 'reading!', 'I', 'have', 'started', 'my', 'personal', 'blog', 'and', 'I', 'don’t', 'intend', 'to', 'write', 'more', 'amazing', 'articles', 'on', 'Medium']), (0.016129032258064516, ['Support', 'my', 'blog', 'by', 'subscribing', 'to', 'thenlp.space']), (0.016029043714153056, ['This', 'means', '‘good’', 'and', '‘great’', 'are', 'as', 'different', 'as', '‘day’', 'and', '‘have’,', 'which', 'is', 'not', 'true']), (0.015932529805646616, ['If', 'we', 'construct', 'an', 'exhaustive', 'vocabulary', '(let’s', 'call', 'it', 'V),', 'it', 'would', 'have', 'V', '=', '{Have,', 'a,', 'good,', 'great,', 'day}']), (0.013978519334591669, ['Have', '=', '[1,0,0,0,0]`;', 'a=[0,1,0,0,0]`', ';', 'good=[0,0,1,0,0]`', ';', 'great=[0,0,0,1,0]`', ';', 'day=[0,0,0,0,1]`', '(`', 'represents', 'transpose)', 'If', 'we', 'try', 'to', 'visualize', 'these', 'encodings,', 'we', 'can', 'think', 'of', 'a', '5', 'dimensional', 'space,', 'where', 'each', 'word', 'occupies', 'one', 'of', 'the', 'dimensions', 'and', 'has', 'nothing', 'to', 'do', 'with', 'the', 'rest', '(no', 'projection', 'along', 'the', 'other', 'dimensions)']), (0.011951655740640131, ['But', 'there’s', 'one', 'more', 'way', 'we', 'can', 'do', 'the', 'same']), (0.011888633389629897, ['Length', 'of', 'our', 'one-hot', 'encoded', 'vector', 'would', 'be', 'equal', 'to', 'the', 'size', 'of', 'V', '(=5)']), (0.01186926254253794, ['Skip-Gram', 'model:', 'This', 'looks', 'like', 'multiple-context', 'CBOW', 'model', 'just', 'got', 'flipped']), (0.011741537884786102, ['The', 'hidden', 'layer', 'contains', 'N', 'neurons', 'and', 'the', 'output', 'is', 'again', 'a', 'V', 'length', 'vector', 'with', 'the', 'elements', 'being', 'the', 'softmax', 'values']), (0.011711001477712101, ['That', 'particular', 'element', 'would', 'be', 'one']), (0.010830969596835528, ['Let’s', 'get', 'the', 'terms', 'in', 'the', 'picture', 'right:', '-', 'Wvn', 'is', 'the', 'weight', 'matrix', 'that', 'maps', 'the', 'input', 'x', 'to', 'the', 'hidden', 'layer', '(V*N', 'dimensional', 'matrix)', '-W`nv', 'is', 'the', 'weight', 'matrix', 'that', 'maps', 'the', 'hidden', 'layer', 'outputs', 'to', 'the', 'final', 'output', 'layer', '(N*V', 'dimensional', 'matrix)', 'I', 'won’t', 'get', 'into', 'the', 'mathematics']), (0.010816989721808109, ['Why', 'do', 'we', 'need', 'them?', 'Consider', 'the', 'following', 'similar', 'sentences:', 'Have', 'a', 'good', 'day', 'and', 'Have', 'a', 'great', 'day']), (0.010501655818967282, ['Consider', 'our', 'example:', 'Have', 'a', 'great', 'day']), (0.00994278857466075, ['The', 'model', 'outputs', 'C', 'probability', 'distributions']), (0.009342740857461857, ['The', 'hidden', 'layer', 'neurons', 'just', 'copy', 'the', 'weighted', 'sum', 'of', 'inputs', 'to', 'the', 'next', 'layer']), (0.008606525067777662, ['The', 'encodings', 'below', 'would', 'explain', 'this', 'better']), (0.008577192427333423, ['angle', 'close', 'to', '0']), (0.008449322265865589, ['For', 'example,', 'to', 'make', 'the', 'algorithm', 'computationally', 'more', 'efficient,', 'tricks', 'like', 'Hierarchical', 'Softmax', 'and', 'Skip-Gram', 'Negative', 'Sampling', 'are', 'used']), (0.008377996082615928, ['In', 'both', 'the', 'cases,', 'the', 'network', 'uses', 'back-propagation', 'to', 'learn']), (0.00831230181526714, ['What’s', 'ahead?', 'The', 'above', 'explanation', 'is', 'a', 'very', 'basic', 'one']), (0.008287493363545072, ['Google', 'Images', 'Here', 'comes', 'the', 'idea', 'of', 'generating', 'distributed', 'representations']), (0.008182717252577645, ['All', 'of', 'it', 'can', 'be', 'found', 'here']), (0.008061199171903469, ['The', 'only', 'non-linearity', 'is', 'the', 'softmax', 'calculations', 'in', 'the', 'output', 'layer']), (0.007943389524799015, ['Mathematically,', 'the', 'cosine', 'of', 'the', 'angle', 'between', 'such', 'vectors', 'should', 'be', 'close', 'to', '1,', 'i.e']), (0.007112008015189133, ['Detailed', 'math', 'can', 'be', 'found', 'here', 'Who', 'wins?', 'Both', 'have', 'their', 'own', 'advantages', 'and', 'disadvantages']), (0.007054681700456376, ['Another', 'variant,', 'called', 'Skip', 'Gram', 'model', 'does', 'this']), (0.006992321103285103, ['We’ll', 'just', 'get', 'an', 'idea', 'of', 'what’s', 'going', 'on']), (0.005977600029210023, ['Let', 'us', 'look', 'deeper', 'into', 'the', 'actual', 'architecture']), (0.0054025459940283405, ['They', 'hardly', 'have', 'different', 'meaning']), (0.005285667823713152, ['How', 'does', 'Word2Vec', 'work?', 'Word2Vec', 'is', 'a', 'method', 'to', 'construct', 'such', 'an', 'embedding']), (0.0048890012202425625, ['Let’s', 'tackle', 'this', 'part', 'by', 'part']), (0.004178168837038958, ['It', 'was', 'developed', 'by', 'Tomas', 'Mikolov', 'in', '2013', 'at', 'Google']), (0.004132275319732573, ['There’s', 'a', 'lot', 'more', 'to', 'it']), (0.003938041635624346, ['To', 'some', 'extent', 'that', 'is', 'true']), (0.003586759306313246, ['There', 'is', 'no', 'activation', 'like', 'sigmoid,', 'tanh', 'or', 'ReLU'])]\n","\n","Summarize Text 0: \n","Notice that here we are trying to predict a target word (day) using a single context input word great\n","\n","Summarize Text 1: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words\n","\n","Summarize Text 2: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V\n","\n","Summarize Text 3: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context\n","\n","Summarize Text 4: \n","Notice that here we are trying to predict a target word (day) using a single context input word great. So, we have seen how word representations are generated using the context words. CBOW Model The input or the context word is a one hot encoded vector of size V. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day)\n","\n","\n","\n","Article a5: =======================================================================\n","\n","Old: ------------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.019883566120336937, ['Since', 'stemmers', 'use', 'algorithmics', 'approaches,', 'the', 'result', 'of', 'the', 'stemming', 'process', 'may', 'not', 'be', 'an', 'actual', 'word', 'or', 'even', 'change', 'the', 'word', '(and', 'sentence)', 'meaning']), (0.017914003269264445, ['To', 'bring', 'a', 'short', 'example', 'I', 'took', 'the', 'first', 'sentence', 'of', 'the', 'song', '“Across', 'the', 'Universe”', 'from', 'The', 'Beatles:', 'Words', 'are', 'flowing', 'out', 'like', 'endless', 'rain', 'into', 'a', 'paper', 'cup,', 'They', 'slither', 'while', 'they', 'pass,', 'they', 'slip', 'away', 'across', 'the', 'universe', 'Now', 'let’s', 'count', 'the', 'words:', 'This', 'approach', 'may', 'reflect', 'several', 'downsides', 'like', 'the', 'absence', 'of', 'semantic', 'meaning', 'and', 'context,', 'and', 'the', 'facts', 'that', 'stop', 'words', '(like', '“the”', 'or', '“a”)', 'add', 'noise', 'to', 'the', 'analysis', 'and', 'some', 'words', 'are', 'not', 'weighted', 'accordingly', '(“universe”', 'weights', 'less', 'than', 'the', 'word', '“they”)']), (0.017488906976978195, ['Let’s', 'summarize', 'and', 'explain', 'some', 'of', 'the', 'most', 'frequently', 'used', 'algorithms', 'in', 'NLP', 'when', 'defining', 'the', 'vocabulary', 'of', 'terms:', 'Bag', 'of', 'Words', 'Is', 'a', 'commonly', 'used', 'model', 'that', 'allows', 'you', 'to', 'count', 'all', 'words', 'in', 'a', 'piece', 'of', 'text']), (0.016709102126810874, ['The', 'algorithm', 'goes', 'through', 'each', 'word', 'iteratively', 'and', 'reassigns', 'the', 'word', 'to', 'a', 'topic', 'taking', 'into', 'considerations', 'the', 'probability', 'that', 'the', 'word', 'belongs', 'to', 'a', 'topic,', 'and', 'the', 'probability', 'that', 'the', 'document', 'will', 'be', 'generated', 'by', 'a', 'topic']), (0.016003582014921407, ['A', 'potential', 'approach', 'is', 'to', 'begin', 'by', 'adopting', 'pre-defined', 'stop', 'words', 'and', 'add', 'words', 'to', 'the', 'list', 'later', 'on']), (0.01595359304159909, ['In', 'English,', 'prefixes', 'are', 'always', 'derivational', '(the', 'affix', 'creates', 'a', 'new', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'prefix', '“eco”', 'in', 'the', 'word', '“ecosystem”),', 'but', 'suffixes', 'can', 'be', 'derivational', '(the', 'affix', 'creates', 'a', 'new', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'suffix', '“ist”', 'in', 'the', 'word', '“guitarist”)', 'or', 'inflectional', '(the', 'affix', 'creates', 'a', 'new', 'form', 'of', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'suffix', '“er”', 'in', 'the', 'word', '“faster”)']), (0.015016233566468458, ['Stemming', 'Refers', 'to', 'the', 'process', 'of', 'slicing', 'the', 'end', 'or', 'the', 'beginning', 'of', 'words', 'with', 'the', 'intention', 'of', 'removing', 'affixes', '(lexical', 'additions', 'to', 'the', 'root', 'of', 'the', 'word)']), (0.014902317674095857, ['“astro”', 'in', 'the', 'word', '“astrobiology”)', 'and', 'the', 'ones', 'attached', 'at', 'the', 'end', 'of', 'the', 'word', 'are', 'called', 'suffixes', '(e.g']), (0.01472595663621011, ['Although', 'it', 'seems', 'closely', 'related', 'to', 'the', 'stemming', 'process,', 'lemmatization', 'uses', 'a', 'different', 'approach', 'to', 'reach', 'the', 'root', 'forms', 'of', 'words']), (0.014633583449291917, ['For', 'example,', 'if', 'we', 'are', 'performing', 'a', 'sentiment', 'analysis', 'we', 'might', 'throw', 'our', 'algorithm', 'off', 'track', 'if', 'we', 'remove', 'a', 'stop', 'word', 'like', '“not”']), (0.014542861338734335, ['To', 'be', 'more', 'specific,', 'LDA', 'finds', 'groups', 'of', 'related', 'words', 'by:', 'Assigning', 'each', 'word', 'to', 'a', 'random', 'topic,', 'where', 'the', 'user', 'defines', 'the', 'number', 'of', 'topics', 'it', 'wishes', 'to', 'uncover']), (0.01448240148502147, ['Lemmatization', 'also', 'takes', 'into', 'consideration', 'the', 'context', 'of', 'the', 'word', 'in', 'order', 'to', 'solve', 'other', 'problems', 'like', 'disambiguation,', 'which', 'means', 'it', 'can', 'discriminate', 'between', 'identical', 'words', 'that', 'have', 'different', 'meanings', 'depending', 'on', 'the', 'specific', 'context']), (0.014340504254405173, ['To', 'solve', 'this', 'problem,', 'one', 'approach', 'is', 'to', 'rescale', 'the', 'frequency', 'of', 'words', 'by', 'how', 'often', 'they', 'appear', 'in', 'all', 'texts', '(not', 'just', 'the', 'one', 'we', 'are', 'analyzing)', 'so', 'that', 'the', 'scores', 'for', 'frequent', 'words', 'like', '“the”,', 'that', 'are', 'also', 'frequent', 'across', 'other', 'texts,', 'get', 'penalized']), (0.014305977099723137, ['Lemmatization', 'Has', 'the', 'objective', 'of', 'reducing', 'a', 'word', 'to', 'its', 'base', 'form', 'and', 'grouping', 'together', 'different', 'forms', 'of', 'the', 'same', 'word']), (0.014128643731489988, ['Nevertheless', 'it', 'seems', 'that', 'the', 'general', 'trend', 'over', 'the', 'past', 'time', 'has', 'been', 'to', 'go', 'from', 'the', 'use', 'of', 'large', 'standard', 'stop', 'word', 'lists', 'to', 'the', 'use', 'of', 'no', 'lists', 'at', 'all']), (0.014090575482370893, ['Tokenization', 'Is', 'the', 'process', 'of', 'segmenting', 'running', 'text', 'into', 'sentences', 'and', 'words']), (0.013621345634829531, ['By', 'providing', 'a', 'part-of-speech', 'parameter', 'to', 'a', 'word', '(', 'whether', 'it', 'is', 'a', 'noun,', 'a', 'verb,', 'and', 'so', 'on)', 'it’s', 'possible', 'to', 'define', 'a', 'role', 'for', 'that', 'word', 'in', 'the', 'sentence', 'and', 'remove', 'disambiguation']), (0.013578417599937533, ['There', 'is', 'no', 'universal', 'list', 'of', 'stop', 'words']), (0.01346529992212593, ['Use', 'Cases', 'of', 'NLP', 'In', 'simple', 'terms,', 'NLP', 'represents', 'the', 'automatic', 'handling', 'of', 'natural', 'human', 'language', 'like', 'speech', 'or', 'text,', 'and', 'although', 'the', 'concept', 'itself', 'is', 'fascinating,', 'the', 'real', 'value', 'behind', 'this', 'technology', 'comes', 'from', 'the', 'use', 'cases']), (0.013201593703812369, ['Lemmatization', 'resolves', 'words', 'to', 'their', 'dictionary', 'form', '(known', 'as', 'lemma)', 'for', 'which', 'it', 'requires', 'detailed', 'dictionaries', 'in', 'which', 'the', 'algorithm', 'can', 'look', 'into', 'and', 'link', 'words', 'to', 'their', 'corresponding', 'lemmas']), (0.013154471052752146, ['In', 'this', 'process', 'some', 'very', 'common', 'words', 'that', 'appear', 'to', 'provide', 'little', 'or', 'no', 'value', 'to', 'the', 'NLP', 'objective', 'are', 'filtered', 'and', 'excluded', 'from', 'the', 'text', 'to', 'be', 'processed,', 'hence', 'removing', 'widespread', 'and', 'frequent', 'terms', 'that', 'are', 'not', 'informative', 'about', 'the', 'corresponding', 'text']), (0.012800235351484113, ['Through', 'TFIDF', 'frequent', 'terms', 'in', 'the', 'text', 'are', '“rewarded”', '(like', 'the', 'word', '“they”', 'in', 'our', 'example),', 'but', 'they', 'also', 'get', '“punished”', 'if', 'those', 'terms', 'are', 'frequent', 'in', 'other', 'texts', 'we', 'include', 'in', 'the', 'algorithm', 'too']), (0.012678823789658298, ['Affixes', 'that', 'are', 'attached', 'at', 'the', 'beginning', 'of', 'the', 'word', 'are', 'called', 'prefixes', '(e.g']), (0.012561684261743526, ['The', 'problem', 'is', 'that', 'affixes', 'can', 'create', 'or', 'expand', 'new', 'forms', 'of', 'the', 'same', 'word', '(called', 'inflectional', 'affixes),', 'or', 'even', 'create', 'new', 'words', 'themselves', '(called', 'derivational', 'affixes)']), (0.012484473027113615, ['“ful”', 'in', 'the', 'word', '“helpful”)']), (0.012201456878098541, ['For', 'example,', 'the', 'words', '“running”,', '“runs”', 'and', '“ran”', 'are', 'all', 'forms', 'of', 'the', 'word', '“run”,', 'so', '“run”', 'is', 'the', 'lemma', 'of', 'all', 'the', 'previous', 'words']), (0.011950794624410626, ['These', 'word', 'frequencies', 'or', 'occurrences', 'are', 'then', 'used', 'as', 'features', 'for', 'training', 'a', 'classifier']), (0.011801571111906335, ['This', 'technique', 'is', 'based', 'on', 'the', 'assumptions', 'that', 'each', 'document', 'consists', 'of', 'a', 'mixture', 'of', 'topics', 'and', 'that', 'each', 'topic', 'consists', 'of', 'a', 'set', 'of', 'words,', 'which', 'means', 'that', 'if', 'we', 'can', 'spot', 'these', 'hidden', 'topics', 'we', 'can', 'unlock', 'the', 'meaning', 'of', 'our', 'texts']), (0.011191449179516621, ['Stop', 'words', 'can', 'be', 'safely', 'ignored', 'by', 'carrying', 'out', 'a', 'lookup', 'in', 'a', 'pre-defined', 'list', 'of', 'keywords,', 'freeing', 'up', 'database', 'space', 'and', 'improving', 'processing', 'time']), (0.011087489525324281, ['In', 'essence', 'it', 'clusters', 'texts', 'to', 'discover', 'latent', 'topics', 'based', 'on', 'their', 'contents,', 'processing', 'individual', 'words', 'and', 'assigning', 'them', 'values', 'based', 'on', 'their', 'distribution']), (0.011004692031260243, ['The', 'thing', 'is', 'stop', 'words', 'removal', 'can', 'wipe', 'out', 'relevant', 'information', 'and', 'modify', 'the', 'context', 'in', 'a', 'given', 'sentence']), (0.010988429876675235, ['The', 'topic', 'we', 'choose,', 'our', 'tone,', 'our', 'selection', 'of', 'words,', 'everything', 'adds', 'some', 'type', 'of', 'information', 'that', 'can', 'be', 'interpreted', 'and', 'value', 'extracted', 'from', 'it']), (0.01087563504541281, ['Following', 'our', 'example,', 'the', 'result', 'of', 'tokenization', 'would', 'be:', 'Pretty', 'simple,', 'right?', 'Well,', 'although', 'it', 'may', 'seem', 'quite', 'basic', 'in', 'this', 'case', 'and', 'also', 'in', 'languages', 'like', 'English', 'that', 'separate', 'words', 'by', 'a', 'blank', 'space', '(called', 'segmented', 'languages)', 'not', 'all', 'languages', 'behave', 'the', 'same,', 'and', 'if', 'you', 'think', 'about', 'it,', 'blank', 'spaces', 'alone', 'are', 'not', 'sufficient', 'enough', 'even', 'for', 'English', 'to', 'perform', 'proper', 'tokenizations']), (0.01081852431831915, ['Under', 'these', 'conditions,', 'you', 'might', 'select', 'a', 'minimal', 'stop', 'word', 'list', 'and', 'add', 'additional', 'terms', 'depending', 'on', 'your', 'specific', 'objective']), (0.010704310671721885, ['This', 'approach', 'to', 'scoring', 'is', 'called', '“Term', 'Frequency', '—', 'Inverse', 'Document', 'Frequency”', '(TFIDF),', 'and', 'improves', 'the', 'bag', 'of', 'words', 'by', 'weights']), (0.01043804936482469, ['Stop', 'Words', 'Removal', 'Includes', 'getting', 'rid', 'of', 'common', 'language', 'articles,', 'pronouns', 'and', 'prepositions', 'such', 'as', '“and”,', '“the”', 'or', '“to”', 'in', 'English']), (0.010415398144627271, ['But', 'there', 'is', 'a', 'problem:', 'one', 'person', 'may', 'generate', 'hundreds', 'or', 'thousands', 'of', 'words', 'in', 'a', 'declaration,', 'each', 'sentence', 'with', 'its', 'corresponding', 'complexity']), (0.010374813301138104, ['“best”', 'is', 'changed', 'to', '“good”),', 'hence', 'standardizing', 'words', 'with', 'similar', 'meaning', 'to', 'their', 'root']), (0.010155824379892694, ['Think', 'about', 'words', 'like', '“bat”', '(which', 'can', 'correspond', 'to', 'the', 'animal', 'or', 'to', 'the', 'metal/wooden', 'club', 'used', 'in', 'baseball)', 'or', '“bank”', '(corresponding', 'to', 'the', 'financial', 'institution', 'or', 'to', 'the', 'land', 'alongside', 'a', 'body', 'of', 'water)']), (0.009961133071342578, ['Basically', 'it', 'creates', 'an', 'occurrence', 'matrix', 'for', 'the', 'sentence', 'or', 'document,', 'disregarding', 'grammar', 'and', 'word', 'order']), (0.009896543507670597, ['Tokenization', 'can', 'remove', 'punctuation', 'too,', 'easing', 'the', 'path', 'to', 'a', 'proper', 'word', 'segmentation', 'but', 'also', 'triggering', 'possible', 'complications']), (0.009770781041646536, ['Nowadays', 'it', 'is', 'no', 'longer', 'about', 'trying', 'to', 'interpret', 'a', 'text', 'or', 'speech', 'based', 'on', 'its', 'keywords', '(the', 'old', 'fashioned', 'mechanical', 'way),', 'but', 'about', 'understanding', 'the', 'meaning', 'behind', 'those', 'words', '(the', 'cognitive', 'way)']), (0.00955158016473466, ['Stemmers', 'are', 'simple', 'to', 'use', 'and', 'run', 'very', 'fast', '(they', 'perform', 'simple', 'operations', 'on', 'a', 'string),', 'and', 'if', 'speed', 'and', 'performance', 'are', 'important', 'in', 'the', 'NLP', 'model,', 'then', 'stemming', 'is', 'certainly', 'the', 'way', 'to', 'go']), (0.009504328648392204, ['You', 'don’t', 'define', 'the', 'topics', 'themselves', '(you', 'define', 'just', 'the', 'number', 'of', 'topics)', 'and', 'the', 'algorithm', 'will', 'map', 'all', 'documents', 'to', 'the', 'topics', 'in', 'a', 'way', 'that', 'words', 'in', 'each', 'document', 'are', 'mostly', 'captured', 'by', 'those', 'imaginary', 'topics']), (0.00940325408928404, ['Unlike', 'other', 'clustering', 'algorithms', 'like', 'K-means', 'that', 'perform', 'hard', 'clustering', '(where', 'topics', 'are', 'disjointed),', 'LDA', 'assigns', 'each', 'document', 'to', 'a', 'mixture', 'of', 'topics,', 'which', 'means', 'that', 'each', 'document', 'can', 'be', 'described', 'by', 'one', 'or', 'more', 'topics', '(e.g']), (0.009398866286571116, ['Natural', 'Language', 'Processing', 'or', 'NLP', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'gives', 'the', 'machines', 'the', 'ability', 'to', 'read,', 'understand', 'and', 'derive', 'meaning', 'from', 'human', 'languages']), (0.009133706809245228, ['How', 'does', 'the', 'future', 'look', 'like?', 'At', 'the', 'moment', 'NLP', 'is', 'battling', 'to', 'detect', 'nuances', 'in', 'language', 'meaning,', 'whether', 'due', 'to', 'lack', 'of', 'context,', 'spelling', 'errors', 'or', 'dialectal', 'differences']), (0.009094028568223153, ['NLP', 'is', 'being', 'used', 'to', 'track', 'news,', 'reports,', 'comments', 'about', 'possible', 'mergers', 'between', 'companies,', 'everything', 'can', 'be', 'then', 'incorporated', 'into', 'a', 'trading', 'algorithm', 'to', 'generate', 'massive', 'profits']), (0.009000017773411265, ['The', 'process', 'of', 'understanding', 'and', 'manipulating', 'language', 'is', 'extremely', 'complex,', 'and', 'for', 'this', 'reason', 'it', 'is', 'common', 'to', 'use', 'different', 'techniques', 'to', 'handle', 'different', 'challenges', 'before', 'binding', 'everything', 'together']), (0.008829205578567976, ['Programming', 'languages', 'like', 'Python', 'or', 'R', 'are', 'highly', 'used', 'to', 'perform', 'these', 'techniques,', 'but', 'before', 'diving', 'into', 'code', 'lines', '(that', 'will', 'be', 'the', 'topic', 'of', 'a', 'different', 'article),', 'it’s', 'important', 'to', 'understand', 'the', 'concepts', 'beneath', 'them']), (0.008565431220224075, ['So', 'if', 'stemming', 'has', 'serious', 'limitations,', 'why', 'do', 'we', 'use', 'it?', 'First', 'of', 'all,', 'it', 'can', 'be', 'used', 'to', 'correct', 'spelling', 'errors', 'from', 'the', 'tokens']), (0.008545531719504888, ['Companies', 'like', 'Yahoo', 'and', 'Google', 'filter', 'and', 'classify', 'your', 'emails', 'with', 'NLP', 'by', 'analyzing', 'text', 'in', 'emails', 'that', 'flow', 'through', 'their', 'servers', 'and', 'stopping', 'spam', 'before', 'they', 'even', 'enter', 'your', 'inbox']), (0.00850058432716491, ['In', 'theory,', 'we', 'can', 'understand', 'and', 'even', 'predict', 'human', 'behaviour', 'using', 'that', 'information']), (0.008084933269974587, ['This', 'way', 'it', 'is', 'possible', 'to', 'detect', 'figures', 'of', 'speech', 'like', 'irony,', 'or', 'even', 'perform', 'sentiment', 'analysis']), (0.007960801232873637, ['For', 'example,', 'Amazon', 'Comprehend', 'Medical', 'is', 'a', 'service', 'that', 'uses', 'NLP', 'to', 'extract', 'disease', 'conditions,', 'medications', 'and', 'treatment', 'outcomes', 'from', 'patient', 'notes,', 'clinical', 'trial', 'reports', 'and', 'other', 'electronic', 'health', 'records']), (0.00791265384558746, ['Ok,', 'so', 'how', 'can', 'we', 'tell', 'the', 'difference', 'and', 'chop', 'the', 'right', 'bit?', 'A', 'possible', 'approach', 'is', 'to', 'consider', 'a', 'list', 'of', 'common', 'affixes', 'and', 'rules', '(Python', 'and', 'R', 'languages', 'have', 'different', 'libraries', 'containing', 'affixes', 'and', 'methods)', 'and', 'perform', 'stemming', 'based', 'on', 'them,', 'but', 'of', 'course', 'this', 'approach', 'presents', 'limitations']), (0.007715688277998803, ['Topic', 'modeling', 'is', 'extremely', 'useful', 'for', 'classifying', 'texts,', 'building', 'recommender', 'systems', '(e.g']), (0.00768464680996594, ['In', 'unsupervised', 'learning', 'methods', 'like', 'this', 'one,', 'there', 'is', 'no', 'output', 'variable', 'to', 'guide', 'the', 'learning', 'process', 'and', 'data', 'is', 'explored', 'by', 'algorithms', 'to', 'find', 'patterns']), (0.007647442987664028, ['Today', 'NLP', 'is', 'booming', 'thanks', 'to', 'the', 'huge', 'improvements', 'in', 'the', 'access', 'to', 'data', 'and', 'the', 'increase', 'in', 'computational', 'power,', 'which', 'are', 'allowing', 'practitioners', 'to', 'achieve', 'meaningful', 'results', 'in', 'areas', 'like', 'healthcare,', 'media,', 'finance', 'and', 'human', 'resources,', 'among', 'others']), (0.007476915776957985, ['Nevertheless,', 'thanks', 'to', 'the', 'advances', 'in', 'disciplines', 'like', 'machine', 'learning', 'a', 'big', 'revolution', 'is', 'going', 'on', 'regarding', 'this', 'topic']), (0.007337100246294024, ['Although', 'the', 'future', 'looks', 'extremely', 'challenging', 'and', 'full', 'of', 'threats', 'for', 'NLP,', 'the', 'discipline', 'is', 'developing', 'at', 'a', 'very', 'fast', 'pace', '(probably', 'like', 'never', 'before)', 'and', 'we', 'are', 'likely', 'to', 'reach', 'a', 'level', 'of', 'advancement', 'in', 'the', 'coming', 'years', 'that', 'will', 'make', 'complex', 'applications', 'look', 'possible']), (0.007250254315317985, ['Remember,', 'we', 'use', 'it', 'with', 'the', 'objective', 'of', 'improving', 'our', 'performance,', 'not', 'as', 'a', 'grammar', 'exercise']), (0.007028334989278264, ['Basic', 'NLP', 'to', 'impress', 'your', 'non-NLP', 'friends', 'The', 'main', 'drawbacks', 'we', 'face', 'these', 'days', 'with', 'NLP', 'relate', 'to', 'the', 'fact', 'that', 'language', 'is', 'very', 'tricky']), (0.007018414571637266, ['Amazon’s', 'Alexa', 'and', 'Apple’s', 'Siri', 'are', 'examples', 'of', 'intelligent', 'voice', 'driven', 'interfaces', 'that', 'use', 'NLP', 'to', 'respond', 'to', 'vocal', 'prompts', 'and', 'do', 'everything', 'like', 'find', 'a', 'particular', 'shop,', 'tell', 'us', 'the', 'weather', 'forecast,', 'suggest', 'the', 'best', 'route', 'to', 'the', 'office', 'or', 'turn', 'on', 'the', 'lights', 'at', 'home']), (0.00700912836511447, ['From', 'the', 'universe', 'of', 'topic', 'modelling', 'techniques,', 'Latent', 'Dirichlet', 'Allocation', '(LDA)', 'is', 'probably', 'the', 'most', 'commonly', 'used']), (0.006918057026913463, ['Topic', 'Modeling', 'Is', 'as', 'a', 'method', 'for', 'uncovering', 'hidden', 'structures', 'in', 'sets', 'of', 'texts', 'or', 'documents']), (0.006770990891193275, ['At', 'the', 'same', 'time,', 'since', 'it', 'requires', 'more', 'knowledge', 'about', 'the', 'language', 'structure', 'than', 'a', 'stemming', 'approach,', 'it', 'demands', 'more', 'computational', 'power', 'than', 'setting', 'up', 'or', 'adapting', 'a', 'stemming', 'algorithm']), (0.006727647986027914, ['NLP', 'is', 'also', 'being', 'used', 'in', 'both', 'the', 'search', 'and', 'selection', 'phases', 'of', 'talent', 'recruitment,', 'identifying', 'the', 'skills', 'of', 'potential', 'hires', 'and', 'also', 'spotting', 'prospects', 'before', 'they', 'become', 'active', 'on', 'the', 'job', 'market']), (0.00664032274437667, ['This', 'relatively', 'new', 'algorithm', '(invented', 'less', 'than', '20', 'years', 'ago)', 'works', 'as', 'an', 'unsupervised', 'learning', 'method', 'that', 'discovers', 'different', 'topics', 'underlying', 'a', 'collection', 'of', 'documents']), (0.006503176931247532, ['Zo', 'uses', 'a', 'combination', 'of', 'innovative', 'approaches', 'to', 'recognize', 'and', 'generate', 'conversation,', 'and', 'other', 'companies', 'are', 'exploring', 'with', 'bots', 'that', 'can', 'remember', 'details', 'specific', 'to', 'an', 'individual', 'conversation']), (0.006440324032403459, ['As', 'you', 'might', 'already', 'pictured,', 'lemmatization', 'is', 'a', 'much', 'more', 'resource-intensive', 'task', 'than', 'performing', 'a', 'stemming', 'process']), (0.0063730945382361025, ['Let’s', 'mention', 'some', 'examples:', 'NLP', 'enables', 'the', 'recognition', 'and', 'prediction', 'of', 'diseases', 'based', 'on', 'electronic', 'health', 'records', 'and', 'patient’s', 'own', 'speech']), (0.006292602668849352, ['NLP', 'may', 'be', 'the', 'key', 'to', 'an', 'effective', 'clinical', 'support', 'in', 'the', 'future,', 'but', 'there', 'are', 'still', 'many', 'challenges', 'to', 'face', 'in', 'the', 'short', 'term']), (0.006273415996191528, ['To', 'help', 'identifying', 'fake', 'news,', 'the', 'NLP', 'Group', 'at', 'MIT', 'developed', 'a', 'new', 'system', 'to', 'determine', 'if', 'a', 'source', 'is', 'accurate', 'or', 'politically', 'biased,', 'detecting', 'if', 'a', 'news', 'source', 'can', 'be', 'trusted', 'or', 'not']), (0.006241227681946529, ['NLP', 'is', 'particularly', 'booming', 'in', 'the', 'healthcare', 'industry']), (0.006188819264965973, ['NLP', 'can', 'help', 'you', 'with', 'lots', 'of', 'tasks', 'and', 'the', 'fields', 'of', 'application', 'just', 'seem', 'to', 'increase', 'on', 'a', 'daily', 'basis']), (0.006144111076614901, ['Number', 'of', 'publications', 'containing', 'the', 'sentence', '“natural', 'language', 'processing”', 'in', 'PubMed', 'in', 'the', 'period', '1978–2018']), (0.005960199238743654, ['Splitting', 'on', 'blank', 'spaces', 'may', 'break', 'up', 'what', 'should', 'be', 'considered', 'as', 'one', 'token,', 'as', 'in', 'the', 'case', 'of', 'certain', 'names', '(e.g']), (0.005881519930463385, ['Organizations', 'can', 'determine', 'what', 'customers', 'are', 'saying', 'about', 'a', 'service', 'or', 'product', 'by', 'identifying', 'and', 'extracting', 'information', 'in', 'sources', 'like', 'social', 'media']), (0.005753695563450888, ['Document', '1', 'is', 'described', 'by', '70%', 'of', 'topic', 'A,', '20%', 'of', 'topic', 'B', 'and', '10%', 'of', 'topic', 'C)', 'and', 'reflect', 'more', 'realistic', 'results']), (0.005672119265959733, ['How', 'would', 'users', 'react', 'to', 'such', 'diagnosis?', 'And', 'what', 'would', 'happen', 'if', 'you', 'were', 'tested', 'as', 'a', 'false', 'positive?', '(meaning', 'that', 'you', 'can', 'be', 'diagnosed', 'with', 'the', 'disease', 'even', 'though', 'you', 'don’t', 'have', 'it)']), (0.0056214149865788255, ['To', 'offset', 'this', 'effect', 'you', 'can', 'edit', 'those', 'predefined', 'methods', 'by', 'adding', 'or', 'removing', 'affixes', 'and', 'rules,', 'but', 'you', 'must', 'consider', 'that', 'you', 'might', 'be', 'improving', 'the', 'performance', 'in', 'one', 'area', 'while', 'producing', 'a', 'degradation', 'in', 'another', 'one']), (0.0056208794194099725, ['Powered', 'by', 'IBM', 'Watson', 'NLP', 'technology,', 'LegalMation', 'developed', 'a', 'platform', 'to', 'automate', 'routine', 'litigation', 'tasks', 'and', 'help', 'legal', 'teams', 'save', 'time,', 'drive', 'down', 'costs', 'and', 'shift', 'strategic', 'focus']), (0.0056010277849956065, ['An', 'inventor', 'at', 'IBM', 'developed', 'a', 'cognitive', 'assistant', 'that', 'works', 'like', 'a', 'personalized', 'search', 'engine', 'by', 'learning', 'all', 'about', 'you', 'and', 'then', 'remind', 'you', 'of', 'a', 'name,', 'a', 'song,', 'or', 'anything', 'you', 'can’t', 'remember', 'the', 'moment', 'you', 'need', 'it', 'to']), (0.005533744898833156, ['In', 'the', 'case', 'of', 'periods', 'that', 'follow', 'abbreviation', '(e.g']), (0.005464696629468475, ['On', 'March', '2016', 'Microsoft', 'launched', 'Tay,', 'an', 'Artificial', 'Intelligence', '(AI)', 'chatbot', 'released', 'on', 'Twitter', 'as', 'a', 'NLP', 'experiment']), (0.005422221968794924, ['Data', 'generated', 'from', 'conversations,', 'declarations', 'or', 'even', 'tweets', 'are', 'examples', 'of', 'unstructured', 'data']), (0.005397984287282781, ['This', 'technology', 'is', 'improving', 'care', 'delivery,', 'disease', 'diagnosis', 'and', 'bringing', 'costs', 'down', 'while', 'healthcare', 'organizations', 'are', 'going', 'through', 'a', 'growing', 'adoption', 'of', 'electronic', 'health', 'records']), (0.0053229558029387405, ['“went”', 'is', 'changed', 'to', '“go”)', 'and', 'synonyms', 'are', 'unified', '(e.g']), (0.005294002215668933, ['The', 'tokenization', 'process', 'can', 'be', 'particularly', 'problematic', 'when', 'dealing', 'with', 'biomedical', 'text', 'domains', 'which', 'contain', 'lots', 'of', 'hyphens,', 'parentheses,', 'and', 'other', 'punctuation', 'marks']), (0.005168427464554376, ['As', 'of', '2018,', 'PubMed', 'comprised', 'more', 'than', '29', 'million', 'citations', 'for', 'biomedical', 'literature', 'Companies', 'like', 'Winterlight', 'Labs', 'are', 'making', 'huge', 'improvements', 'in', 'the', 'treatment', 'of', 'Alzheimer’s', 'disease', 'by', 'monitoring', 'cognitive', 'impairment', 'through', 'speech', 'and', 'they', 'can', 'also', 'support', 'clinical', 'trials', 'and', 'studies', 'for', 'a', 'wide', 'range', 'of', 'central', 'nervous', 'system', 'disorders']), (0.005149102276616732, ['It', 'is', 'a', 'discipline', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'data', 'science', 'and', 'human', 'language,', 'and', 'is', 'scaling', 'to', 'lots', 'of', 'industries']), (0.0049748575369146855, ['Following', 'a', 'similar', 'approach,', 'Stanford', 'University', 'developed', 'Woebot,', 'a', 'chatbot', 'therapist', 'with', 'the', 'aim', 'of', 'helping', 'people', 'with', 'anxiety', 'and', 'other', 'disorders']), (0.004859704015081681, ['This', 'capability', 'is', 'being', 'explored', 'in', 'health', 'conditions', 'that', 'go', 'from', 'cardiovascular', 'diseases', 'to', 'depression', 'and', 'even', 'schizophrenia']), (0.004843144042554665, ['For', 'example,', 'verbs', 'in', 'past', 'tense', 'are', 'changed', 'into', 'present', '(e.g']), (0.0048349397164760225, ['to', 'recommend', 'you', 'books', 'based', 'on', 'your', 'past', 'readings)', 'or', 'even', 'detecting', 'trends', 'in', 'online', 'publications']), (0.004646736673745004, ['The', 'fact', 'that', 'clinical', 'documentation', 'can', 'be', 'improved', 'means', 'that', 'patients', 'can', 'be', 'better', 'understood', 'and', 'benefited', 'through', 'better', 'healthcare']), (0.0046366811572113135, ['A', 'couple', 'of', 'years', 'ago', 'Microsoft', 'demonstrated', 'that', 'by', 'analyzing', 'large', 'samples', 'of', 'search', 'engine', 'queries,', 'they', 'could', 'identify', 'internet', 'users', 'who', 'were', 'suffering', 'from', 'pancreatic', 'cancer', 'even', 'before', 'they', 'have', 'received', 'a', 'diagnosis', 'of', 'the', 'disease']), (0.004603739641922415, ['In', 'essence,', 'it’s', 'the', 'task', 'of', 'cutting', 'a', 'text', 'into', 'pieces', 'called', 'tokens,', 'and', 'at', 'the', 'same', 'time', 'throwing', 'away', 'certain', 'characters,', 'such', 'as', 'punctuation']), (0.0044731062838763, ['Well,', 'the', 'result', 'was', 'that', 'after', '16', 'hours', 'Tay', 'had', 'to', 'be', 'removed', 'due', 'to', 'its', 'racist', 'and', 'abusive', 'comments:', 'Microsoft', 'learnt', 'from', 'its', 'own', 'experience', 'and', 'some', 'months', 'later', 'released', 'Zo,', 'its', 'second', 'generation', 'English-language', 'chatbot', 'that', 'won’t', 'be', 'caught', 'making', 'the', 'same', 'mistakes', 'as', 'its', 'predecessor']), (0.004384868496347363, ['dr.),', 'the', 'period', 'following', 'that', 'abbreviation', 'should', 'be', 'considered', 'as', 'part', 'of', 'the', 'same', 'token', 'and', 'not', 'be', 'removed']), (0.004176581310265914, ['Nevertheless,', 'this', 'approach', 'still', 'has', 'no', 'context', 'nor', 'semantics']), (0.004115366662294331, ['On', 'the', 'contrary,', 'this', 'method', 'highlights', 'and', '“rewards”', 'unique', 'or', 'rare', 'terms', 'considering', 'all', 'texts']), (0.0037996387654171916, ['This', 'sentiment', 'analysis', 'can', 'provide', 'a', 'lot', 'of', 'information', 'about', 'customers', 'choices', 'and', 'their', 'decision', 'drivers']), (0.0036357519320070345, ['Always', 'look', 'at', 'the', 'whole', 'picture', 'and', 'test', 'your', 'model’s', 'performance']), (0.0036064979664839876, ['Thanks', 'Jesús', 'del', 'Valle', ',', 'Jannis', 'Busch', 'and', 'Sabrina', 'Steinert', 'for', 'your', 'valuable', 'inputs', 'Interested', 'in', 'these', 'topics?', 'Follow', 'me', 'on', 'Linkedin', 'or', 'Twitter', '']), (0.003521543862758095, ['San', 'Francisco', 'or', 'New', 'York)', 'or', 'borrowed', 'foreign', 'phrases', '(e.g']), (0.0033653905819108825, ['If', 'you', 'want', 'to', 'scale', 'and', 'analyze', 'several', 'hundreds,', 'thousands', 'or', 'millions', 'of', 'people', 'or', 'declarations', 'in', 'a', 'given', 'geography,', 'then', 'the', 'situation', 'is', 'unmanageable']), (0.0032071213533660247, ['These', 'probabilities', 'are', 'calculated', 'multiple', 'times,', 'until', 'the', 'convergence', 'of', 'the', 'algorithm']), (0.003020802146768862, ['Unstructured', 'data', 'doesn’t', 'fit', 'neatly', 'into', 'the', 'traditional', 'row', 'and', 'column', 'structure', 'of', 'relational', 'databases,', 'and', 'represent', 'the', 'vast', 'majority', 'of', 'data', 'available', 'in', 'the', 'actual', 'world']), (0.0030168591459857107, ['Everything', 'we', 'express', '(either', 'verbally', 'or', 'in', 'written)', 'carries', 'huge', 'amounts', 'of', 'information']), (0.0028785628256136177, ['The', 'idea', 'was', 'that', 'as', 'more', 'users', 'conversed', 'with', 'Tay,', 'the', 'smarter', 'it', 'would', 'get']), (0.002807327197323269, ['For', 'deeper', 'details', 'on', 'tokenization,', 'you', 'can', 'find', 'a', 'great', 'explanation', 'in', 'this', 'article']), (0.0027326903629179113, ['The', 'goal', 'should', 'be', 'to', 'optimize', 'their', 'experience,', 'and', 'several', 'organizations', 'are', 'already', 'working', 'on', 'this']), (0.0026387534196875458, ['This', 'recalls', 'the', 'case', 'of', 'Google', 'Flu', 'Trends', 'which', 'in', '2009', 'was', 'announced', 'as', 'being', 'able', 'to', 'predict', 'influenza', 'but', 'later', 'on', 'vanished', 'due', 'to', 'its', 'low', 'accuracy', 'and', 'inability', 'to', 'meet', 'its', 'projected', 'rates']), (0.0025059775715685164, ['Remember:', 'buy', 'the', 'rumor,', 'sell', 'the', 'news']), (0.0023505887283621213, ['Having', 'an', 'insight', 'into', 'what', 'is', 'happening', 'and', 'what', 'people', 'are', 'talking', 'about', 'can', 'be', 'very', 'valuable', 'to', 'financial', 'traders']), (0.0016465252035380263, ['It', 'is', 'messy', 'and', 'hard', 'to', 'manipulate']), (0.0016196835473470095, ['These', 'can', 'be', 'pre-selected', 'or', 'built', 'from', 'scratch']), (0.0015124392698857958, ['But', 'serious', 'controversy', 'is', 'around', 'the', 'subject']), (0.001248439450686642, ['laissez', 'faire)'])]\n","\n","Summarize Text 0: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning\n","\n","Summarize Text 1: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”)\n","\n","Summarize Text 2: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text\n","\n","Summarize Text 3: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text. The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic\n","\n","Summarize Text 4: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text. The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on\n","\n","Updated: ------------------------------------------------------------------------\n","\n","Indexes of top ranked_sentence order are  [(0.019883566120336937, ['Since', 'stemmers', 'use', 'algorithmics', 'approaches,', 'the', 'result', 'of', 'the', 'stemming', 'process', 'may', 'not', 'be', 'an', 'actual', 'word', 'or', 'even', 'change', 'the', 'word', '(and', 'sentence)', 'meaning']), (0.017914003269264445, ['To', 'bring', 'a', 'short', 'example', 'I', 'took', 'the', 'first', 'sentence', 'of', 'the', 'song', '“Across', 'the', 'Universe”', 'from', 'The', 'Beatles:', 'Words', 'are', 'flowing', 'out', 'like', 'endless', 'rain', 'into', 'a', 'paper', 'cup,', 'They', 'slither', 'while', 'they', 'pass,', 'they', 'slip', 'away', 'across', 'the', 'universe', 'Now', 'let’s', 'count', 'the', 'words:', 'This', 'approach', 'may', 'reflect', 'several', 'downsides', 'like', 'the', 'absence', 'of', 'semantic', 'meaning', 'and', 'context,', 'and', 'the', 'facts', 'that', 'stop', 'words', '(like', '“the”', 'or', '“a”)', 'add', 'noise', 'to', 'the', 'analysis', 'and', 'some', 'words', 'are', 'not', 'weighted', 'accordingly', '(“universe”', 'weights', 'less', 'than', 'the', 'word', '“they”)']), (0.017488906976978195, ['Let’s', 'summarize', 'and', 'explain', 'some', 'of', 'the', 'most', 'frequently', 'used', 'algorithms', 'in', 'NLP', 'when', 'defining', 'the', 'vocabulary', 'of', 'terms:', 'Bag', 'of', 'Words', 'Is', 'a', 'commonly', 'used', 'model', 'that', 'allows', 'you', 'to', 'count', 'all', 'words', 'in', 'a', 'piece', 'of', 'text']), (0.016709102126810874, ['The', 'algorithm', 'goes', 'through', 'each', 'word', 'iteratively', 'and', 'reassigns', 'the', 'word', 'to', 'a', 'topic', 'taking', 'into', 'considerations', 'the', 'probability', 'that', 'the', 'word', 'belongs', 'to', 'a', 'topic,', 'and', 'the', 'probability', 'that', 'the', 'document', 'will', 'be', 'generated', 'by', 'a', 'topic']), (0.016003582014921407, ['A', 'potential', 'approach', 'is', 'to', 'begin', 'by', 'adopting', 'pre-defined', 'stop', 'words', 'and', 'add', 'words', 'to', 'the', 'list', 'later', 'on']), (0.01595359304159909, ['In', 'English,', 'prefixes', 'are', 'always', 'derivational', '(the', 'affix', 'creates', 'a', 'new', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'prefix', '“eco”', 'in', 'the', 'word', '“ecosystem”),', 'but', 'suffixes', 'can', 'be', 'derivational', '(the', 'affix', 'creates', 'a', 'new', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'suffix', '“ist”', 'in', 'the', 'word', '“guitarist”)', 'or', 'inflectional', '(the', 'affix', 'creates', 'a', 'new', 'form', 'of', 'word', 'as', 'in', 'the', 'example', 'of', 'the', 'suffix', '“er”', 'in', 'the', 'word', '“faster”)']), (0.015016233566468458, ['Stemming', 'Refers', 'to', 'the', 'process', 'of', 'slicing', 'the', 'end', 'or', 'the', 'beginning', 'of', 'words', 'with', 'the', 'intention', 'of', 'removing', 'affixes', '(lexical', 'additions', 'to', 'the', 'root', 'of', 'the', 'word)']), (0.014902317674095857, ['“astro”', 'in', 'the', 'word', '“astrobiology”)', 'and', 'the', 'ones', 'attached', 'at', 'the', 'end', 'of', 'the', 'word', 'are', 'called', 'suffixes', '(e.g']), (0.01472595663621011, ['Although', 'it', 'seems', 'closely', 'related', 'to', 'the', 'stemming', 'process,', 'lemmatization', 'uses', 'a', 'different', 'approach', 'to', 'reach', 'the', 'root', 'forms', 'of', 'words']), (0.014633583449291917, ['For', 'example,', 'if', 'we', 'are', 'performing', 'a', 'sentiment', 'analysis', 'we', 'might', 'throw', 'our', 'algorithm', 'off', 'track', 'if', 'we', 'remove', 'a', 'stop', 'word', 'like', '“not”']), (0.014542861338734335, ['To', 'be', 'more', 'specific,', 'LDA', 'finds', 'groups', 'of', 'related', 'words', 'by:', 'Assigning', 'each', 'word', 'to', 'a', 'random', 'topic,', 'where', 'the', 'user', 'defines', 'the', 'number', 'of', 'topics', 'it', 'wishes', 'to', 'uncover']), (0.01448240148502147, ['Lemmatization', 'also', 'takes', 'into', 'consideration', 'the', 'context', 'of', 'the', 'word', 'in', 'order', 'to', 'solve', 'other', 'problems', 'like', 'disambiguation,', 'which', 'means', 'it', 'can', 'discriminate', 'between', 'identical', 'words', 'that', 'have', 'different', 'meanings', 'depending', 'on', 'the', 'specific', 'context']), (0.014340504254405173, ['To', 'solve', 'this', 'problem,', 'one', 'approach', 'is', 'to', 'rescale', 'the', 'frequency', 'of', 'words', 'by', 'how', 'often', 'they', 'appear', 'in', 'all', 'texts', '(not', 'just', 'the', 'one', 'we', 'are', 'analyzing)', 'so', 'that', 'the', 'scores', 'for', 'frequent', 'words', 'like', '“the”,', 'that', 'are', 'also', 'frequent', 'across', 'other', 'texts,', 'get', 'penalized']), (0.014305977099723137, ['Lemmatization', 'Has', 'the', 'objective', 'of', 'reducing', 'a', 'word', 'to', 'its', 'base', 'form', 'and', 'grouping', 'together', 'different', 'forms', 'of', 'the', 'same', 'word']), (0.014128643731489988, ['Nevertheless', 'it', 'seems', 'that', 'the', 'general', 'trend', 'over', 'the', 'past', 'time', 'has', 'been', 'to', 'go', 'from', 'the', 'use', 'of', 'large', 'standard', 'stop', 'word', 'lists', 'to', 'the', 'use', 'of', 'no', 'lists', 'at', 'all']), (0.014090575482370893, ['Tokenization', 'Is', 'the', 'process', 'of', 'segmenting', 'running', 'text', 'into', 'sentences', 'and', 'words']), (0.013621345634829531, ['By', 'providing', 'a', 'part-of-speech', 'parameter', 'to', 'a', 'word', '(', 'whether', 'it', 'is', 'a', 'noun,', 'a', 'verb,', 'and', 'so', 'on)', 'it’s', 'possible', 'to', 'define', 'a', 'role', 'for', 'that', 'word', 'in', 'the', 'sentence', 'and', 'remove', 'disambiguation']), (0.013578417599937533, ['There', 'is', 'no', 'universal', 'list', 'of', 'stop', 'words']), (0.01346529992212593, ['Use', 'Cases', 'of', 'NLP', 'In', 'simple', 'terms,', 'NLP', 'represents', 'the', 'automatic', 'handling', 'of', 'natural', 'human', 'language', 'like', 'speech', 'or', 'text,', 'and', 'although', 'the', 'concept', 'itself', 'is', 'fascinating,', 'the', 'real', 'value', 'behind', 'this', 'technology', 'comes', 'from', 'the', 'use', 'cases']), (0.013201593703812369, ['Lemmatization', 'resolves', 'words', 'to', 'their', 'dictionary', 'form', '(known', 'as', 'lemma)', 'for', 'which', 'it', 'requires', 'detailed', 'dictionaries', 'in', 'which', 'the', 'algorithm', 'can', 'look', 'into', 'and', 'link', 'words', 'to', 'their', 'corresponding', 'lemmas']), (0.013154471052752146, ['In', 'this', 'process', 'some', 'very', 'common', 'words', 'that', 'appear', 'to', 'provide', 'little', 'or', 'no', 'value', 'to', 'the', 'NLP', 'objective', 'are', 'filtered', 'and', 'excluded', 'from', 'the', 'text', 'to', 'be', 'processed,', 'hence', 'removing', 'widespread', 'and', 'frequent', 'terms', 'that', 'are', 'not', 'informative', 'about', 'the', 'corresponding', 'text']), (0.012800235351484113, ['Through', 'TFIDF', 'frequent', 'terms', 'in', 'the', 'text', 'are', '“rewarded”', '(like', 'the', 'word', '“they”', 'in', 'our', 'example),', 'but', 'they', 'also', 'get', '“punished”', 'if', 'those', 'terms', 'are', 'frequent', 'in', 'other', 'texts', 'we', 'include', 'in', 'the', 'algorithm', 'too']), (0.012678823789658298, ['Affixes', 'that', 'are', 'attached', 'at', 'the', 'beginning', 'of', 'the', 'word', 'are', 'called', 'prefixes', '(e.g']), (0.012561684261743526, ['The', 'problem', 'is', 'that', 'affixes', 'can', 'create', 'or', 'expand', 'new', 'forms', 'of', 'the', 'same', 'word', '(called', 'inflectional', 'affixes),', 'or', 'even', 'create', 'new', 'words', 'themselves', '(called', 'derivational', 'affixes)']), (0.012484473027113615, ['“ful”', 'in', 'the', 'word', '“helpful”)']), (0.012201456878098541, ['For', 'example,', 'the', 'words', '“running”,', '“runs”', 'and', '“ran”', 'are', 'all', 'forms', 'of', 'the', 'word', '“run”,', 'so', '“run”', 'is', 'the', 'lemma', 'of', 'all', 'the', 'previous', 'words']), (0.011950794624410626, ['These', 'word', 'frequencies', 'or', 'occurrences', 'are', 'then', 'used', 'as', 'features', 'for', 'training', 'a', 'classifier']), (0.011801571111906335, ['This', 'technique', 'is', 'based', 'on', 'the', 'assumptions', 'that', 'each', 'document', 'consists', 'of', 'a', 'mixture', 'of', 'topics', 'and', 'that', 'each', 'topic', 'consists', 'of', 'a', 'set', 'of', 'words,', 'which', 'means', 'that', 'if', 'we', 'can', 'spot', 'these', 'hidden', 'topics', 'we', 'can', 'unlock', 'the', 'meaning', 'of', 'our', 'texts']), (0.011191449179516621, ['Stop', 'words', 'can', 'be', 'safely', 'ignored', 'by', 'carrying', 'out', 'a', 'lookup', 'in', 'a', 'pre-defined', 'list', 'of', 'keywords,', 'freeing', 'up', 'database', 'space', 'and', 'improving', 'processing', 'time']), (0.011087489525324281, ['In', 'essence', 'it', 'clusters', 'texts', 'to', 'discover', 'latent', 'topics', 'based', 'on', 'their', 'contents,', 'processing', 'individual', 'words', 'and', 'assigning', 'them', 'values', 'based', 'on', 'their', 'distribution']), (0.011004692031260243, ['The', 'thing', 'is', 'stop', 'words', 'removal', 'can', 'wipe', 'out', 'relevant', 'information', 'and', 'modify', 'the', 'context', 'in', 'a', 'given', 'sentence']), (0.010988429876675235, ['The', 'topic', 'we', 'choose,', 'our', 'tone,', 'our', 'selection', 'of', 'words,', 'everything', 'adds', 'some', 'type', 'of', 'information', 'that', 'can', 'be', 'interpreted', 'and', 'value', 'extracted', 'from', 'it']), (0.01087563504541281, ['Following', 'our', 'example,', 'the', 'result', 'of', 'tokenization', 'would', 'be:', 'Pretty', 'simple,', 'right?', 'Well,', 'although', 'it', 'may', 'seem', 'quite', 'basic', 'in', 'this', 'case', 'and', 'also', 'in', 'languages', 'like', 'English', 'that', 'separate', 'words', 'by', 'a', 'blank', 'space', '(called', 'segmented', 'languages)', 'not', 'all', 'languages', 'behave', 'the', 'same,', 'and', 'if', 'you', 'think', 'about', 'it,', 'blank', 'spaces', 'alone', 'are', 'not', 'sufficient', 'enough', 'even', 'for', 'English', 'to', 'perform', 'proper', 'tokenizations']), (0.01081852431831915, ['Under', 'these', 'conditions,', 'you', 'might', 'select', 'a', 'minimal', 'stop', 'word', 'list', 'and', 'add', 'additional', 'terms', 'depending', 'on', 'your', 'specific', 'objective']), (0.010704310671721885, ['This', 'approach', 'to', 'scoring', 'is', 'called', '“Term', 'Frequency', '—', 'Inverse', 'Document', 'Frequency”', '(TFIDF),', 'and', 'improves', 'the', 'bag', 'of', 'words', 'by', 'weights']), (0.01043804936482469, ['Stop', 'Words', 'Removal', 'Includes', 'getting', 'rid', 'of', 'common', 'language', 'articles,', 'pronouns', 'and', 'prepositions', 'such', 'as', '“and”,', '“the”', 'or', '“to”', 'in', 'English']), (0.010415398144627271, ['But', 'there', 'is', 'a', 'problem:', 'one', 'person', 'may', 'generate', 'hundreds', 'or', 'thousands', 'of', 'words', 'in', 'a', 'declaration,', 'each', 'sentence', 'with', 'its', 'corresponding', 'complexity']), (0.010374813301138104, ['“best”', 'is', 'changed', 'to', '“good”),', 'hence', 'standardizing', 'words', 'with', 'similar', 'meaning', 'to', 'their', 'root']), (0.010155824379892694, ['Think', 'about', 'words', 'like', '“bat”', '(which', 'can', 'correspond', 'to', 'the', 'animal', 'or', 'to', 'the', 'metal/wooden', 'club', 'used', 'in', 'baseball)', 'or', '“bank”', '(corresponding', 'to', 'the', 'financial', 'institution', 'or', 'to', 'the', 'land', 'alongside', 'a', 'body', 'of', 'water)']), (0.009961133071342578, ['Basically', 'it', 'creates', 'an', 'occurrence', 'matrix', 'for', 'the', 'sentence', 'or', 'document,', 'disregarding', 'grammar', 'and', 'word', 'order']), (0.009896543507670597, ['Tokenization', 'can', 'remove', 'punctuation', 'too,', 'easing', 'the', 'path', 'to', 'a', 'proper', 'word', 'segmentation', 'but', 'also', 'triggering', 'possible', 'complications']), (0.009770781041646536, ['Nowadays', 'it', 'is', 'no', 'longer', 'about', 'trying', 'to', 'interpret', 'a', 'text', 'or', 'speech', 'based', 'on', 'its', 'keywords', '(the', 'old', 'fashioned', 'mechanical', 'way),', 'but', 'about', 'understanding', 'the', 'meaning', 'behind', 'those', 'words', '(the', 'cognitive', 'way)']), (0.00955158016473466, ['Stemmers', 'are', 'simple', 'to', 'use', 'and', 'run', 'very', 'fast', '(they', 'perform', 'simple', 'operations', 'on', 'a', 'string),', 'and', 'if', 'speed', 'and', 'performance', 'are', 'important', 'in', 'the', 'NLP', 'model,', 'then', 'stemming', 'is', 'certainly', 'the', 'way', 'to', 'go']), (0.009504328648392204, ['You', 'don’t', 'define', 'the', 'topics', 'themselves', '(you', 'define', 'just', 'the', 'number', 'of', 'topics)', 'and', 'the', 'algorithm', 'will', 'map', 'all', 'documents', 'to', 'the', 'topics', 'in', 'a', 'way', 'that', 'words', 'in', 'each', 'document', 'are', 'mostly', 'captured', 'by', 'those', 'imaginary', 'topics']), (0.00940325408928404, ['Unlike', 'other', 'clustering', 'algorithms', 'like', 'K-means', 'that', 'perform', 'hard', 'clustering', '(where', 'topics', 'are', 'disjointed),', 'LDA', 'assigns', 'each', 'document', 'to', 'a', 'mixture', 'of', 'topics,', 'which', 'means', 'that', 'each', 'document', 'can', 'be', 'described', 'by', 'one', 'or', 'more', 'topics', '(e.g']), (0.009398866286571116, ['Natural', 'Language', 'Processing', 'or', 'NLP', 'is', 'a', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'gives', 'the', 'machines', 'the', 'ability', 'to', 'read,', 'understand', 'and', 'derive', 'meaning', 'from', 'human', 'languages']), (0.009133706809245228, ['How', 'does', 'the', 'future', 'look', 'like?', 'At', 'the', 'moment', 'NLP', 'is', 'battling', 'to', 'detect', 'nuances', 'in', 'language', 'meaning,', 'whether', 'due', 'to', 'lack', 'of', 'context,', 'spelling', 'errors', 'or', 'dialectal', 'differences']), (0.009094028568223153, ['NLP', 'is', 'being', 'used', 'to', 'track', 'news,', 'reports,', 'comments', 'about', 'possible', 'mergers', 'between', 'companies,', 'everything', 'can', 'be', 'then', 'incorporated', 'into', 'a', 'trading', 'algorithm', 'to', 'generate', 'massive', 'profits']), (0.009000017773411265, ['The', 'process', 'of', 'understanding', 'and', 'manipulating', 'language', 'is', 'extremely', 'complex,', 'and', 'for', 'this', 'reason', 'it', 'is', 'common', 'to', 'use', 'different', 'techniques', 'to', 'handle', 'different', 'challenges', 'before', 'binding', 'everything', 'together']), (0.008829205578567976, ['Programming', 'languages', 'like', 'Python', 'or', 'R', 'are', 'highly', 'used', 'to', 'perform', 'these', 'techniques,', 'but', 'before', 'diving', 'into', 'code', 'lines', '(that', 'will', 'be', 'the', 'topic', 'of', 'a', 'different', 'article),', 'it’s', 'important', 'to', 'understand', 'the', 'concepts', 'beneath', 'them']), (0.008565431220224075, ['So', 'if', 'stemming', 'has', 'serious', 'limitations,', 'why', 'do', 'we', 'use', 'it?', 'First', 'of', 'all,', 'it', 'can', 'be', 'used', 'to', 'correct', 'spelling', 'errors', 'from', 'the', 'tokens']), (0.008545531719504888, ['Companies', 'like', 'Yahoo', 'and', 'Google', 'filter', 'and', 'classify', 'your', 'emails', 'with', 'NLP', 'by', 'analyzing', 'text', 'in', 'emails', 'that', 'flow', 'through', 'their', 'servers', 'and', 'stopping', 'spam', 'before', 'they', 'even', 'enter', 'your', 'inbox']), (0.00850058432716491, ['In', 'theory,', 'we', 'can', 'understand', 'and', 'even', 'predict', 'human', 'behaviour', 'using', 'that', 'information']), (0.008084933269974587, ['This', 'way', 'it', 'is', 'possible', 'to', 'detect', 'figures', 'of', 'speech', 'like', 'irony,', 'or', 'even', 'perform', 'sentiment', 'analysis']), (0.007960801232873637, ['For', 'example,', 'Amazon', 'Comprehend', 'Medical', 'is', 'a', 'service', 'that', 'uses', 'NLP', 'to', 'extract', 'disease', 'conditions,', 'medications', 'and', 'treatment', 'outcomes', 'from', 'patient', 'notes,', 'clinical', 'trial', 'reports', 'and', 'other', 'electronic', 'health', 'records']), (0.00791265384558746, ['Ok,', 'so', 'how', 'can', 'we', 'tell', 'the', 'difference', 'and', 'chop', 'the', 'right', 'bit?', 'A', 'possible', 'approach', 'is', 'to', 'consider', 'a', 'list', 'of', 'common', 'affixes', 'and', 'rules', '(Python', 'and', 'R', 'languages', 'have', 'different', 'libraries', 'containing', 'affixes', 'and', 'methods)', 'and', 'perform', 'stemming', 'based', 'on', 'them,', 'but', 'of', 'course', 'this', 'approach', 'presents', 'limitations']), (0.007715688277998803, ['Topic', 'modeling', 'is', 'extremely', 'useful', 'for', 'classifying', 'texts,', 'building', 'recommender', 'systems', '(e.g']), (0.00768464680996594, ['In', 'unsupervised', 'learning', 'methods', 'like', 'this', 'one,', 'there', 'is', 'no', 'output', 'variable', 'to', 'guide', 'the', 'learning', 'process', 'and', 'data', 'is', 'explored', 'by', 'algorithms', 'to', 'find', 'patterns']), (0.007647442987664028, ['Today', 'NLP', 'is', 'booming', 'thanks', 'to', 'the', 'huge', 'improvements', 'in', 'the', 'access', 'to', 'data', 'and', 'the', 'increase', 'in', 'computational', 'power,', 'which', 'are', 'allowing', 'practitioners', 'to', 'achieve', 'meaningful', 'results', 'in', 'areas', 'like', 'healthcare,', 'media,', 'finance', 'and', 'human', 'resources,', 'among', 'others']), (0.007476915776957985, ['Nevertheless,', 'thanks', 'to', 'the', 'advances', 'in', 'disciplines', 'like', 'machine', 'learning', 'a', 'big', 'revolution', 'is', 'going', 'on', 'regarding', 'this', 'topic']), (0.007337100246294024, ['Although', 'the', 'future', 'looks', 'extremely', 'challenging', 'and', 'full', 'of', 'threats', 'for', 'NLP,', 'the', 'discipline', 'is', 'developing', 'at', 'a', 'very', 'fast', 'pace', '(probably', 'like', 'never', 'before)', 'and', 'we', 'are', 'likely', 'to', 'reach', 'a', 'level', 'of', 'advancement', 'in', 'the', 'coming', 'years', 'that', 'will', 'make', 'complex', 'applications', 'look', 'possible']), (0.007250254315317985, ['Remember,', 'we', 'use', 'it', 'with', 'the', 'objective', 'of', 'improving', 'our', 'performance,', 'not', 'as', 'a', 'grammar', 'exercise']), (0.007028334989278264, ['Basic', 'NLP', 'to', 'impress', 'your', 'non-NLP', 'friends', 'The', 'main', 'drawbacks', 'we', 'face', 'these', 'days', 'with', 'NLP', 'relate', 'to', 'the', 'fact', 'that', 'language', 'is', 'very', 'tricky']), (0.007018414571637266, ['Amazon’s', 'Alexa', 'and', 'Apple’s', 'Siri', 'are', 'examples', 'of', 'intelligent', 'voice', 'driven', 'interfaces', 'that', 'use', 'NLP', 'to', 'respond', 'to', 'vocal', 'prompts', 'and', 'do', 'everything', 'like', 'find', 'a', 'particular', 'shop,', 'tell', 'us', 'the', 'weather', 'forecast,', 'suggest', 'the', 'best', 'route', 'to', 'the', 'office', 'or', 'turn', 'on', 'the', 'lights', 'at', 'home']), (0.00700912836511447, ['From', 'the', 'universe', 'of', 'topic', 'modelling', 'techniques,', 'Latent', 'Dirichlet', 'Allocation', '(LDA)', 'is', 'probably', 'the', 'most', 'commonly', 'used']), (0.006918057026913463, ['Topic', 'Modeling', 'Is', 'as', 'a', 'method', 'for', 'uncovering', 'hidden', 'structures', 'in', 'sets', 'of', 'texts', 'or', 'documents']), (0.006770990891193275, ['At', 'the', 'same', 'time,', 'since', 'it', 'requires', 'more', 'knowledge', 'about', 'the', 'language', 'structure', 'than', 'a', 'stemming', 'approach,', 'it', 'demands', 'more', 'computational', 'power', 'than', 'setting', 'up', 'or', 'adapting', 'a', 'stemming', 'algorithm']), (0.006727647986027914, ['NLP', 'is', 'also', 'being', 'used', 'in', 'both', 'the', 'search', 'and', 'selection', 'phases', 'of', 'talent', 'recruitment,', 'identifying', 'the', 'skills', 'of', 'potential', 'hires', 'and', 'also', 'spotting', 'prospects', 'before', 'they', 'become', 'active', 'on', 'the', 'job', 'market']), (0.00664032274437667, ['This', 'relatively', 'new', 'algorithm', '(invented', 'less', 'than', '20', 'years', 'ago)', 'works', 'as', 'an', 'unsupervised', 'learning', 'method', 'that', 'discovers', 'different', 'topics', 'underlying', 'a', 'collection', 'of', 'documents']), (0.006503176931247532, ['Zo', 'uses', 'a', 'combination', 'of', 'innovative', 'approaches', 'to', 'recognize', 'and', 'generate', 'conversation,', 'and', 'other', 'companies', 'are', 'exploring', 'with', 'bots', 'that', 'can', 'remember', 'details', 'specific', 'to', 'an', 'individual', 'conversation']), (0.006440324032403459, ['As', 'you', 'might', 'already', 'pictured,', 'lemmatization', 'is', 'a', 'much', 'more', 'resource-intensive', 'task', 'than', 'performing', 'a', 'stemming', 'process']), (0.0063730945382361025, ['Let’s', 'mention', 'some', 'examples:', 'NLP', 'enables', 'the', 'recognition', 'and', 'prediction', 'of', 'diseases', 'based', 'on', 'electronic', 'health', 'records', 'and', 'patient’s', 'own', 'speech']), (0.006292602668849352, ['NLP', 'may', 'be', 'the', 'key', 'to', 'an', 'effective', 'clinical', 'support', 'in', 'the', 'future,', 'but', 'there', 'are', 'still', 'many', 'challenges', 'to', 'face', 'in', 'the', 'short', 'term']), (0.006273415996191528, ['To', 'help', 'identifying', 'fake', 'news,', 'the', 'NLP', 'Group', 'at', 'MIT', 'developed', 'a', 'new', 'system', 'to', 'determine', 'if', 'a', 'source', 'is', 'accurate', 'or', 'politically', 'biased,', 'detecting', 'if', 'a', 'news', 'source', 'can', 'be', 'trusted', 'or', 'not']), (0.006241227681946529, ['NLP', 'is', 'particularly', 'booming', 'in', 'the', 'healthcare', 'industry']), (0.006188819264965973, ['NLP', 'can', 'help', 'you', 'with', 'lots', 'of', 'tasks', 'and', 'the', 'fields', 'of', 'application', 'just', 'seem', 'to', 'increase', 'on', 'a', 'daily', 'basis']), (0.006144111076614901, ['Number', 'of', 'publications', 'containing', 'the', 'sentence', '“natural', 'language', 'processing”', 'in', 'PubMed', 'in', 'the', 'period', '1978–2018']), (0.005960199238743654, ['Splitting', 'on', 'blank', 'spaces', 'may', 'break', 'up', 'what', 'should', 'be', 'considered', 'as', 'one', 'token,', 'as', 'in', 'the', 'case', 'of', 'certain', 'names', '(e.g']), (0.005881519930463385, ['Organizations', 'can', 'determine', 'what', 'customers', 'are', 'saying', 'about', 'a', 'service', 'or', 'product', 'by', 'identifying', 'and', 'extracting', 'information', 'in', 'sources', 'like', 'social', 'media']), (0.005753695563450888, ['Document', '1', 'is', 'described', 'by', '70%', 'of', 'topic', 'A,', '20%', 'of', 'topic', 'B', 'and', '10%', 'of', 'topic', 'C)', 'and', 'reflect', 'more', 'realistic', 'results']), (0.005672119265959733, ['How', 'would', 'users', 'react', 'to', 'such', 'diagnosis?', 'And', 'what', 'would', 'happen', 'if', 'you', 'were', 'tested', 'as', 'a', 'false', 'positive?', '(meaning', 'that', 'you', 'can', 'be', 'diagnosed', 'with', 'the', 'disease', 'even', 'though', 'you', 'don’t', 'have', 'it)']), (0.0056214149865788255, ['To', 'offset', 'this', 'effect', 'you', 'can', 'edit', 'those', 'predefined', 'methods', 'by', 'adding', 'or', 'removing', 'affixes', 'and', 'rules,', 'but', 'you', 'must', 'consider', 'that', 'you', 'might', 'be', 'improving', 'the', 'performance', 'in', 'one', 'area', 'while', 'producing', 'a', 'degradation', 'in', 'another', 'one']), (0.0056208794194099725, ['Powered', 'by', 'IBM', 'Watson', 'NLP', 'technology,', 'LegalMation', 'developed', 'a', 'platform', 'to', 'automate', 'routine', 'litigation', 'tasks', 'and', 'help', 'legal', 'teams', 'save', 'time,', 'drive', 'down', 'costs', 'and', 'shift', 'strategic', 'focus']), (0.0056010277849956065, ['An', 'inventor', 'at', 'IBM', 'developed', 'a', 'cognitive', 'assistant', 'that', 'works', 'like', 'a', 'personalized', 'search', 'engine', 'by', 'learning', 'all', 'about', 'you', 'and', 'then', 'remind', 'you', 'of', 'a', 'name,', 'a', 'song,', 'or', 'anything', 'you', 'can’t', 'remember', 'the', 'moment', 'you', 'need', 'it', 'to']), (0.005533744898833156, ['In', 'the', 'case', 'of', 'periods', 'that', 'follow', 'abbreviation', '(e.g']), (0.005464696629468475, ['On', 'March', '2016', 'Microsoft', 'launched', 'Tay,', 'an', 'Artificial', 'Intelligence', '(AI)', 'chatbot', 'released', 'on', 'Twitter', 'as', 'a', 'NLP', 'experiment']), (0.005422221968794924, ['Data', 'generated', 'from', 'conversations,', 'declarations', 'or', 'even', 'tweets', 'are', 'examples', 'of', 'unstructured', 'data']), (0.005397984287282781, ['This', 'technology', 'is', 'improving', 'care', 'delivery,', 'disease', 'diagnosis', 'and', 'bringing', 'costs', 'down', 'while', 'healthcare', 'organizations', 'are', 'going', 'through', 'a', 'growing', 'adoption', 'of', 'electronic', 'health', 'records']), (0.0053229558029387405, ['“went”', 'is', 'changed', 'to', '“go”)', 'and', 'synonyms', 'are', 'unified', '(e.g']), (0.005294002215668933, ['The', 'tokenization', 'process', 'can', 'be', 'particularly', 'problematic', 'when', 'dealing', 'with', 'biomedical', 'text', 'domains', 'which', 'contain', 'lots', 'of', 'hyphens,', 'parentheses,', 'and', 'other', 'punctuation', 'marks']), (0.005168427464554376, ['As', 'of', '2018,', 'PubMed', 'comprised', 'more', 'than', '29', 'million', 'citations', 'for', 'biomedical', 'literature', 'Companies', 'like', 'Winterlight', 'Labs', 'are', 'making', 'huge', 'improvements', 'in', 'the', 'treatment', 'of', 'Alzheimer’s', 'disease', 'by', 'monitoring', 'cognitive', 'impairment', 'through', 'speech', 'and', 'they', 'can', 'also', 'support', 'clinical', 'trials', 'and', 'studies', 'for', 'a', 'wide', 'range', 'of', 'central', 'nervous', 'system', 'disorders']), (0.005149102276616732, ['It', 'is', 'a', 'discipline', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'data', 'science', 'and', 'human', 'language,', 'and', 'is', 'scaling', 'to', 'lots', 'of', 'industries']), (0.0049748575369146855, ['Following', 'a', 'similar', 'approach,', 'Stanford', 'University', 'developed', 'Woebot,', 'a', 'chatbot', 'therapist', 'with', 'the', 'aim', 'of', 'helping', 'people', 'with', 'anxiety', 'and', 'other', 'disorders']), (0.004859704015081681, ['This', 'capability', 'is', 'being', 'explored', 'in', 'health', 'conditions', 'that', 'go', 'from', 'cardiovascular', 'diseases', 'to', 'depression', 'and', 'even', 'schizophrenia']), (0.004843144042554665, ['For', 'example,', 'verbs', 'in', 'past', 'tense', 'are', 'changed', 'into', 'present', '(e.g']), (0.0048349397164760225, ['to', 'recommend', 'you', 'books', 'based', 'on', 'your', 'past', 'readings)', 'or', 'even', 'detecting', 'trends', 'in', 'online', 'publications']), (0.004646736673745004, ['The', 'fact', 'that', 'clinical', 'documentation', 'can', 'be', 'improved', 'means', 'that', 'patients', 'can', 'be', 'better', 'understood', 'and', 'benefited', 'through', 'better', 'healthcare']), (0.0046366811572113135, ['A', 'couple', 'of', 'years', 'ago', 'Microsoft', 'demonstrated', 'that', 'by', 'analyzing', 'large', 'samples', 'of', 'search', 'engine', 'queries,', 'they', 'could', 'identify', 'internet', 'users', 'who', 'were', 'suffering', 'from', 'pancreatic', 'cancer', 'even', 'before', 'they', 'have', 'received', 'a', 'diagnosis', 'of', 'the', 'disease']), (0.004603739641922415, ['In', 'essence,', 'it’s', 'the', 'task', 'of', 'cutting', 'a', 'text', 'into', 'pieces', 'called', 'tokens,', 'and', 'at', 'the', 'same', 'time', 'throwing', 'away', 'certain', 'characters,', 'such', 'as', 'punctuation']), (0.0044731062838763, ['Well,', 'the', 'result', 'was', 'that', 'after', '16', 'hours', 'Tay', 'had', 'to', 'be', 'removed', 'due', 'to', 'its', 'racist', 'and', 'abusive', 'comments:', 'Microsoft', 'learnt', 'from', 'its', 'own', 'experience', 'and', 'some', 'months', 'later', 'released', 'Zo,', 'its', 'second', 'generation', 'English-language', 'chatbot', 'that', 'won’t', 'be', 'caught', 'making', 'the', 'same', 'mistakes', 'as', 'its', 'predecessor']), (0.004384868496347363, ['dr.),', 'the', 'period', 'following', 'that', 'abbreviation', 'should', 'be', 'considered', 'as', 'part', 'of', 'the', 'same', 'token', 'and', 'not', 'be', 'removed']), (0.004176581310265914, ['Nevertheless,', 'this', 'approach', 'still', 'has', 'no', 'context', 'nor', 'semantics']), (0.004115366662294331, ['On', 'the', 'contrary,', 'this', 'method', 'highlights', 'and', '“rewards”', 'unique', 'or', 'rare', 'terms', 'considering', 'all', 'texts']), (0.0037996387654171916, ['This', 'sentiment', 'analysis', 'can', 'provide', 'a', 'lot', 'of', 'information', 'about', 'customers', 'choices', 'and', 'their', 'decision', 'drivers']), (0.0036357519320070345, ['Always', 'look', 'at', 'the', 'whole', 'picture', 'and', 'test', 'your', 'model’s', 'performance']), (0.0036064979664839876, ['Thanks', 'Jesús', 'del', 'Valle', ',', 'Jannis', 'Busch', 'and', 'Sabrina', 'Steinert', 'for', 'your', 'valuable', 'inputs', 'Interested', 'in', 'these', 'topics?', 'Follow', 'me', 'on', 'Linkedin', 'or', 'Twitter', '']), (0.003521543862758095, ['San', 'Francisco', 'or', 'New', 'York)', 'or', 'borrowed', 'foreign', 'phrases', '(e.g']), (0.0033653905819108825, ['If', 'you', 'want', 'to', 'scale', 'and', 'analyze', 'several', 'hundreds,', 'thousands', 'or', 'millions', 'of', 'people', 'or', 'declarations', 'in', 'a', 'given', 'geography,', 'then', 'the', 'situation', 'is', 'unmanageable']), (0.0032071213533660247, ['These', 'probabilities', 'are', 'calculated', 'multiple', 'times,', 'until', 'the', 'convergence', 'of', 'the', 'algorithm']), (0.003020802146768862, ['Unstructured', 'data', 'doesn’t', 'fit', 'neatly', 'into', 'the', 'traditional', 'row', 'and', 'column', 'structure', 'of', 'relational', 'databases,', 'and', 'represent', 'the', 'vast', 'majority', 'of', 'data', 'available', 'in', 'the', 'actual', 'world']), (0.0030168591459857107, ['Everything', 'we', 'express', '(either', 'verbally', 'or', 'in', 'written)', 'carries', 'huge', 'amounts', 'of', 'information']), (0.0028785628256136177, ['The', 'idea', 'was', 'that', 'as', 'more', 'users', 'conversed', 'with', 'Tay,', 'the', 'smarter', 'it', 'would', 'get']), (0.002807327197323269, ['For', 'deeper', 'details', 'on', 'tokenization,', 'you', 'can', 'find', 'a', 'great', 'explanation', 'in', 'this', 'article']), (0.0027326903629179113, ['The', 'goal', 'should', 'be', 'to', 'optimize', 'their', 'experience,', 'and', 'several', 'organizations', 'are', 'already', 'working', 'on', 'this']), (0.0026387534196875458, ['This', 'recalls', 'the', 'case', 'of', 'Google', 'Flu', 'Trends', 'which', 'in', '2009', 'was', 'announced', 'as', 'being', 'able', 'to', 'predict', 'influenza', 'but', 'later', 'on', 'vanished', 'due', 'to', 'its', 'low', 'accuracy', 'and', 'inability', 'to', 'meet', 'its', 'projected', 'rates']), (0.0025059775715685164, ['Remember:', 'buy', 'the', 'rumor,', 'sell', 'the', 'news']), (0.0023505887283621213, ['Having', 'an', 'insight', 'into', 'what', 'is', 'happening', 'and', 'what', 'people', 'are', 'talking', 'about', 'can', 'be', 'very', 'valuable', 'to', 'financial', 'traders']), (0.0016465252035380263, ['It', 'is', 'messy', 'and', 'hard', 'to', 'manipulate']), (0.0016196835473470095, ['These', 'can', 'be', 'pre-selected', 'or', 'built', 'from', 'scratch']), (0.0015124392698857958, ['But', 'serious', 'controversy', 'is', 'around', 'the', 'subject']), (0.001248439450686642, ['laissez', 'faire)'])]\n","\n","Summarize Text 0: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning\n","\n","Summarize Text 1: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”)\n","\n","Summarize Text 2: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text\n","\n","Summarize Text 3: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text. The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic\n","\n","Summarize Text 4: \n","Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles: Words are flowing out like endless rain into a paper cup, They slither while they pass, they slip away across the universe Now let’s count the words: This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”). Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms: Bag of Words Is a commonly used model that allows you to count all words in a piece of text. The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on\n","\n","\n","\n"]}]},{"cell_type":"markdown","source":["# Answered Questions"],"metadata":{"id":"nYzx3GLsMkyK"}},{"cell_type":"markdown","source":["## 1. What are the two main strategies used in text summarization?\n","\n","Extractive and abstractive summarization. \n","\n","Extractive summarization calculates and selects the top representative segments of a given document/corpus and uses the segments as a summary. This usually produces \"summaries\" that don't function well as summaries, but do make structural sense linguistically and are consistent with the information in the document.\n","\n","Abstractive summarization generates new text based on the document/corpus given. Conceptually this is what we consider a proper summary to be, however this is a much more difficult research problem that so far has had significant difficulty generating significant strings of text that don't devolve into gibberish. "],"metadata":{"id":"B3o0Ihv0Mm3a"}},{"cell_type":"markdown","source":["## 2. Which feature is used in the text summarization code? Explain how to calculate it.\n","\n","The feature being used is a count vector. This is calculated by first creating a vector that is as long as the entire dictionary for the document/corpus, with each value being a 0. Then the string is examined, and for each time a token is encountered 1 is added to that token's place in the vector. The end result is a vector that contains the count of all the tokens used in the string out of all of the tokens that could have possibly been used."],"metadata":{"id":"e4IQyzifMx7J"}},{"cell_type":"markdown","source":["## 3. What is the similarity measurement method used in this code?\n","\n","Cosine similarity."],"metadata":{"id":"4xQ4eLFFMzyJ"}},{"cell_type":"markdown","source":["## 4. We know in ICE-1, TF-IDF is used as the text feature. Can we use it in this code? \n","\n","Technically yes, but we shouldn't. Since TF-IDF features scale based on the document in question, we would only be looking at a dictionary of tokens for each document individually, not for each corpus. Since in this example our corpus consists of two strings, and each document is only one string, then the TF-IDF would amount to a dictionary and counts of only the words in each string, set most of the vectors to a flat shape with low values. This would interfere with the cosine similarity calculations, and is more work than necessary."],"metadata":{"id":"YnoEqqp8M1wZ"}},{"cell_type":"markdown","source":["## 5. Compare the outputs above. Are they the same or not? Please analyze the comparison result.\n","\n","I got results that were identical to the unmodified code. Given the features involved and the articles I chose, this makes sense. This code uses token counts to calculate a similarity score. The only thing that could significantly change the similarity scores would be a change in the token counts Given the nature of the text preprocessing methods used, the only thing likely to change the counts would be the lemmatization. This could have potentially collapsed some counts, thereby reducing the vocabulary and upping some token's counts. However, I chose some technical machine learning/NLP articles to work with. Since the terms used in these are usually fairly exacting and these kinds of articles don't expound a lot or include fluff, it would make sense that the lemmatization collapse very few vocabulary terms, which would in turn have next to no effect on the similarity scores."],"metadata":{"id":"TUb6ddLEM3sG"}}]}