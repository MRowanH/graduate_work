{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1_z6SUA8tQoD_udqNOptDEc_pbVkrgJHA","authorship_tag":"ABX9TyO7Srw1GJNDe98JhkDU1ptR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["try:\n","    import speech_recognition as sr\n","except:\n","    ! pip install SpeechRecognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6uJwzc3lk0Y","executionInfo":{"status":"ok","timestamp":1668873124935,"user_tz":360,"elapsed":7118,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"24d6239c-d5b8-42ae-9e44-5a08dc6124fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SpeechRecognition\n","  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n","\u001b[K     |████████████████████████████████| 32.8 MB 1.5 MB/s \n","\u001b[?25hInstalling collected packages: SpeechRecognition\n","Successfully installed SpeechRecognition-3.8.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFZ0B0C7lC7O"},"outputs":[],"source":["import itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import pylab\n","import speech_recognition as sr\n","import tensorflow as tf\n","import wave\n","\n","from pathlib import Path\n","from scipy import signal\n","from scipy.io import wavfile\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","source":["directory = \"/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/\"\n","INPUT_DIR = directory + \"kaggle_data/free-spoken-digit-dataset-master/recordings/\"\n","OUTPUT_DIR = directory + \"kaggle_data/preprocessed/\"\n","folder = INPUT_DIR + \"0_jackson_0.wav\""],"metadata":{"id":"teAwUNLhlNLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r = sr.Recognizer()\n","\n","with sr.AudioFile(folder) as source:\n","    # listen for the data (load audio to memory)\n","    audio_data = r.record(source)\n","    # recognize (convert from speech to text)\n","    text = r.recognize_google(audio_data)\n","    print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3QsjdzTlPNy","executionInfo":{"status":"ok","timestamp":1668873615396,"user_tz":360,"elapsed":623,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"1aca8f86-f91c-435c-8d9e-f6b38b7dba0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","source":["r = sr.Recognizer()\n","c = 0\n","for filename in os.listdir(INPUT_DIR):\n","    \n","    f = os.path.join(INPUT_DIR, filename)\n","\n","    with sr.AudioFile(f) as source:\n","        try:\n","            audio_data = r.record(source)\n","            text = r.recognize_google(audio_data)\n","            print(text)\n","        \n","        except:\n","            continue\n","    \n","    print(f)\n","    if c == 9:\n","        break\n","    c += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AOUrwQ0rlQP9","executionInfo":{"status":"ok","timestamp":1668874135289,"user_tz":360,"elapsed":146474,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"a5bac022-d667-4219-aeda-0a7eb958e7ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_7.wav\n","3\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_36.wav\n","free\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_35.wav\n","free\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_31.wav\n","3\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_25.wav\n","free\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/3_lucas_22.wav\n","2\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/2_jackson_15.wav\n","2\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/2_jackson_11.wav\n","2\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/2_jackson_10.wav\n","2\n","/content/drive/MyDrive/Classes/CSCE 5222 Feature Engineering/Group Project/kaggle_data/free-spoken-digit-dataset-master/recordings/2_jackson_29.wav\n"]}]},{"cell_type":"code","source":["parent_list = os.listdir(INPUT_DIR)\n","for i in range(10):\n","    print(parent_list[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqlKeDDZlQ7Y","executionInfo":{"status":"ok","timestamp":1668874139002,"user_tz":360,"elapsed":146,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"09c7756a-9896-4975-8fa7-85f19df90b8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3_george_3.wav\n","3_george_30.wav\n","3_george_29.wav\n","3_george_28.wav\n","3_george_31.wav\n","3_george_27.wav\n","3_george_26.wav\n","3_george_24.wav\n","3_george_23.wav\n","3_george_25.wav\n"]}]},{"cell_type":"code","source":["# Utility function to get sound and frame rate info\n","def get_wav_info(wav_file):\n","    wav = wave.open(wav_file, 'r')\n","    frames = wav.readframes(-1)\n","    sound_info = pylab.frombuffer(frames, 'int16')\n","    frame_rate = wav.getframerate()\n","    wav.close()\n","    return sound_info, frame_rate\n","\n","# For every recording, make a spectogram and save it as label_speaker_no.png\n","if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-images')):\n","    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n","    \n","l = str(len(os.listdir(INPUT_DIR)))\n","for i, filename in enumerate(os.listdir(INPUT_DIR)):\n","    if i % 100 == 0:\n","        print(str(i) + \"/\" + l)\n","    if \"wav\" in filename:\n","        file_path = os.path.join(INPUT_DIR, filename)\n","        file_stem = Path(file_path).stem\n","        target_dir = f'class_{file_stem[0]}'\n","        dist_dir = os.path.join(os.path.join(OUTPUT_DIR, 'audio-images'), target_dir)\n","        file_dist_path = os.path.join(dist_dir, file_stem)\n","        if not os.path.exists(file_dist_path + '.png'):\n","            if not os.path.exists(dist_dir):\n","                os.mkdir(dist_dir)\n","            file_stem = Path(file_path).stem\n","            sound_info, frame_rate = get_wav_info(file_path)\n","            pylab.specgram(sound_info, Fs=frame_rate)\n","            pylab.savefig(f'{file_dist_path}.png')\n","            pylab.close()\n","\n","# Print the ten classes in our dataset\n","path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images'))\n","print(\"Classes: \\n\")\n","for i in range(10):\n","    print(path_list[i])\n","    \n","# File names for class 1\n","path_list = os.listdir(os.path.join(OUTPUT_DIR, 'audio-images/class_1'))\n","print(\"\\nA few example files: \\n\")\n","for i in range(10):\n","    print(path_list[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YX_ct7rvlSpk","executionInfo":{"status":"ok","timestamp":1668874870413,"user_tz":360,"elapsed":538370,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"b4b70a34-7933-4946-f79a-ba31517f5c9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/3000\n","100/3000\n","200/3000\n","300/3000\n","400/3000\n","500/3000\n","600/3000\n","700/3000\n","800/3000\n","900/3000\n","1000/3000\n","1100/3000\n","1200/3000\n","1300/3000\n","1400/3000\n","1500/3000\n","1600/3000\n","1700/3000\n","1800/3000\n","1900/3000\n","2000/3000\n","2100/3000\n","2200/3000\n","2300/3000\n","2400/3000\n","2500/3000\n","2600/3000\n","2700/3000\n","2800/3000\n","2900/3000\n","Classes: \n","\n","class_3\n","class_2\n","class_4\n","class_1\n","class_5\n","class_0\n","class_6\n","class_7\n","class_8\n","class_9\n","\n","A few example files: \n","\n","1_theo_42.png\n","1_theo_4.png\n","1_theo_34.png\n","1_theo_36.png\n","1_theo_49.png\n","1_theo_46.png\n","1_theo_43.png\n","1_theo_37.png\n","1_yweweler_12.png\n","1_yweweler_19.png\n"]}]},{"cell_type":"code","source":["class Trainer():\n","\n","    def __init__(self, name):\n","        self.name = name\n","\n","    def build_model(self, img_height, img_width, n_channels, n_classes):\n","        self.model = tf.keras.models.Sequential()\n","        self.model.add(tf.keras.layers.Input(shape=(img_height, img_width, n_channels)))\n","        self.model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.Flatten())\n","        self.model.add(tf.keras.layers.Dense(256, activation='relu'))\n","        self.model.add(tf.keras.layers.BatchNormalization())\n","        self.model.add(tf.keras.layers.Dropout(0.5))\n","        self.model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n","\n","        # Compile model\n","        self.model.compile(\n","            loss='sparse_categorical_crossentropy',\n","            optimizer=tf.keras.optimizers.RMSprop(),\n","            metrics=['accuracy'],\n","        )\n","\n","    def train_model(self, train_dataset, val_dataset, epochs):\n","        self.history = self.model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n","\n","    def plot_loss(self):\n","        # Plot the loss curves for training and validation.\n","        history_dict = self.history.history\n","        loss_values = history_dict['loss']\n","        val_loss_values = history_dict['val_loss']\n","        epochs = range(1, len(loss_values)+1)\n","\n","        plt.figure(figsize=(8,6))\n","        plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","        plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","        plt.title('Training and validation loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.show()\n","\n","    def plot_accuracy(self):\n","        # Plot the accuracy curves for training and validation.\n","        history_dict = self.history.history\n","        acc_values = history_dict['accuracy']\n","        val_acc_values = history_dict['val_accuracy']\n","        epochs = range(1, len(acc_values)+1)\n","\n","        plt.figure(figsize=(8,6))\n","        plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n","        plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n","        plt.title('Training and validation accuracy')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Accuracy')\n","        plt.legend()\n","        plt.show()"],"metadata":{"id":"Iz6KlARnqj1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Declare constants\n","IMAGE_HEIGHT = 256\n","IMAGE_WIDTH = 256\n","BATCH_SIZE = 32\n","N_CHANNELS = 3\n","N_CLASSES = 10\n","\n","# Make a dataset containing the training spectrograms\n","train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","                                             batch_size=BATCH_SIZE,\n","                                             validation_split=0.2,\n","                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n","                                             shuffle=True,\n","                                             color_mode='rgb',\n","                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n","                                             subset=\"training\",\n","                                             seed=0)\n","\n","# Make a dataset containing the validation spectrogram\n","valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","                                             batch_size=BATCH_SIZE,\n","                                             validation_split=0.2,\n","                                             directory=os.path.join(OUTPUT_DIR, 'audio-images'),\n","                                             shuffle=True,\n","                                             color_mode='rgb',\n","                                             image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n","                                             subset=\"validation\",\n","                                             seed=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8-NLWoulTv-","executionInfo":{"status":"ok","timestamp":1668875324611,"user_tz":360,"elapsed":3443,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"a7298b5f-aa62-41fa-b0dc-caceed626362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3000 files belonging to 10 classes.\n","Using 2400 files for training.\n","Found 3000 files belonging to 10 classes.\n","Using 600 files for validation.\n"]}]},{"cell_type":"code","source":["# Function to prepare our datasets for modelling\n","def prepare(ds, augment=False):\n","    # Define our one transformation\n","    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n","    flip_and_rotate = tf.keras.Sequential([\n","        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n","        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n","    ])\n","    \n","    # Apply rescale to both datasets and augmentation only to training\n","    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n","    if augment: ds = ds.map(lambda x, y: (flip_and_rotate(x, training=True), y))\n","    return ds\n","\n","train_dataset = prepare(train_dataset, augment=False)\n","valid_dataset = prepare(valid_dataset, augment=False)"],"metadata":{"id":"RUF971mTlVKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, N_CHANNELS)))\n","model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(256, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dropout(0.5))\n","model.add(tf.keras.layers.Dense(N_CLASSES, activation='softmax'))\n","\n","# Compile model\n","model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    optimizer=tf.keras.optimizers.RMSprop(),\n","    metrics=['accuracy'],\n",")"],"metadata":{"id":"piGYECA_lWV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model for 10 epochs, capture the history\n","history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"b7O_DmtmlXp8","executionInfo":{"status":"error","timestamp":1668876795730,"user_tz":360,"elapsed":142423,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"32fcee16-bd6c-41d2-cd96-67d0aa224e00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","75/75 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.9508"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-41f9968e6e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model for 10 epochs, capture the history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_test_function_3058]"]}]},{"cell_type":"code","source":["# Plot the loss curves for training and validation.\n","history_dict = history.history\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","epochs = range(1, len(loss_values)+1)\n","\n","plt.figure(figsize=(8,6))\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"sadxh9fLlYtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the accuracy curves for training and validation.\n","acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","epochs = range(1, len(acc_values)+1)\n","\n","plt.figure(figsize=(8,6))\n","plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"fsWgSedolZpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the final loss and accuracy\n","final_loss, final_acc = model.evaluate(valid_dataset, verbose=0)\n","print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))"],"metadata":{"id":"Dw8gFyRzla6u"},"execution_count":null,"outputs":[]}]}