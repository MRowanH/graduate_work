{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xba1Cefb0EcD","colab_type":"code","colab":{}},"source":["# Imports ---------------------------------------------------------------------\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import tree, metrics\n","from sklearn.model_selection import GridSearchCV\n","import numpy as np\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","# Model Class -----------------------------------------------------------------\n","class Model_SVM():\n","    # Class variables.\n","    training_data = None\n","    training_labels = None\n","    model_name = None\n","    model = None\n","    for_csv = []\n","    \n","    def __init__(self, data):\n","        # Initilzie class variables.\n","        self.training_data = []\n","        self.training_labels = []\n","        self.grid_results = [];\n","\n","        x = []\n","        y = []\n","        for doc in data:\n","            for row in doc.get_features():\n","                self.training_data.append(row[1:])\n","                x.append(row[1])\n","                y.append(row[2])\n","            self.training_labels.extend(doc.get_gold_labels())\n","    \n","    def train_model(self, parameters):\n","        clf = SVC(**parameters)\n","        svm_log_range = [10**x for x in range(-2, 2+1)]\n","        grid_params = {'gamma': svm_log_range, 'C': svm_log_range}\n","        self.sv_grid = GridSearchCV(clf, grid_params,cv=3, verbose=3, n_jobs=4)\n","        print(\"Training SVM...2\")\n","        self.sv_grid.fit(self.training_data, self.training_labels)\n","#        clf.fit(self.training_data, self.training_labels)\n","        \n","        self.grid_results = {\"best_score\": self.sv_grid.best_score_,\n","                                 \"best_params\": self.sv_grid.best_params_,\n","                                 \"best_estimator\": self.sv_grid.best_estimator_,\n","                                 \"final_test_accuracy\": 0}\n","        self.model = self.sv_grid.best_estimator_\n","    \n","    def predict(self, doc):\n","        best_p = [None, 0]\n","        best_a = [None, 0]\n","        a = []\n","        test_data = []\n","        test_labels = []\n","        for row in doc:\n","            for b in row.get_features():\n","                test_data.append(b[1:])\n","            test_labels.extend(row.get_gold_labels())\n","            \n","            # p = self.model.predict_proba([row[1:]])[0]\n","            # if best_p[1] < p[2]:\n","            #     best_p = [row[0], p[2]]\n","            # if best_a[1] < p[0]:\n","            #     best_a = [row[0], p[0]]\n","                \n","            # a.append([p,row[0]])\n","        y_pred = self.model.predict(test_data)\n","        print(\"SVM Confusion Matrix\")\n","        print(metrics.confusion_matrix(y_true=test_labels, y_pred=y_pred))\n","        print()\n","        print(\"SVM Classification Report\")\n","        print(metrics.classification_report(test_labels, y_pred))\n","#        sorted_list_a = sorted(a,key= lambda x: x[0][0])\n","#        sorted_list_p = sorted(a,key=lambda x: x[0][2])\n","        return [best_p, best_a]\n","    \n","    def test(self, test_data):\n","        acc = 0.0\n","        data = []\n","        labels = []\n","        for doc in test_data:\n","            for row in doc.get_features():\n","                data.append(row[1:])\n","            labels.extend(doc.get_gold_labels())\n","        return self.model.score(data, labels)\n","\n","class Model_NB():\n","    # Class variables.\n","    training_data = None\n","    training_labels = None\n","    model_name = None\n","    model = None\n","    for_csv = []\n","    \n","    def __init__(self, data):\n","        # Initilzie class variables.\n","        self.training_data = []\n","        self.training_labels = []\n","        x = []\n","        y = []\n","        for doc in data:\n","            for row in doc.get_features():\n","                self.training_data.append(row[1:])\n","                x.append(row[1])\n","                y.append(row[2])\n","            self.training_labels.extend(doc.get_gold_labels())\n","    \n","    def train_model(self, parameters):\n","        clf = GaussianNB(**parameters)\n","        clf.fit(self.training_data, self.training_labels)\n","        self.model = clf\n","    \n","    def predict(self, doc):\n","        best_p = [None, 0]\n","        best_a = [None, 0]\n","        a = []\n","        test_data = []\n","        test_labels = []\n","        for row in doc:\n","            for b in row.get_features():\n","                test_data.append(b[1:])\n","            test_labels.extend(row.get_gold_labels())\n","            \n","            # p = self.model.predict_proba([row[1:]])[0]\n","            # if best_p[1] < p[2]:\n","            #     best_p = [row[0], p[2]]\n","            # if best_a[1] < p[0]:\n","            #     best_a = [row[0], p[0]]\n","                \n","            # a.append([p,row[0]])\n","        y_pred = self.model.predict(test_data)\n","        print(\"NB Confusion Matrix\")\n","        print(metrics.confusion_matrix(y_true=test_labels, y_pred=y_pred))\n","        print()\n","        print(\"NB Classification Report\")\n","        print(metrics.classification_report(test_labels, y_pred))\n","#        sorted_list_a = sorted(a,key= lambda x: x[0][0])\n","#        sorted_list_p = sorted(a,key=lambda x: x[0][2])\n","        return [best_p, best_a]\n","    \n","    def test(self, test_data):\n","        acc = 0.0\n","        data = []\n","        labels = []\n","        for doc in test_data:\n","            for row in doc.get_features():\n","                data.append(row[1:])\n","            labels.extend(doc.get_gold_labels())\n","        return self.model.score(data, labels)\n","\n","class Model_MLP():\n","    # Class variables.\n","    training_data = None\n","    training_labels = None\n","    model_name = None\n","    model = None\n","    for_csv = []\n","    \n","    def __init__(self, data):\n","        # Initilzie class variables.\n","        self.training_data = []\n","        self.training_labels = []\n","        self.grid_results = []\n","        x = []\n","        y = []\n","        for doc in data:\n","            for row in doc.get_features():\n","                self.training_data.append(row[1:])\n","                x.append(row[1])\n","                y.append(row[2])\n","            self.training_labels.extend(doc.get_gold_labels())\n","    \n","    def train_model(self, parameters):\n","        clf = MLPClassifier(**parameters)\n","        grid_params = {'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15)}\n","        mlp_grid = GridSearchCV(clf, grid_params,cv=3,verbose=3, n_jobs=4)\n","        print(\"Training MLP...\")\n","        mlp_grid.fit(self.training_data, self.training_labels)\n","#        clf.fit(self.training_data, self.training_labels)\n","        \n","        self.grid_results = {\"best_score\": mlp_grid.best_score_,\n","                                 \"best_params\": mlp_grid.best_params_,\n","                                 \"best_estimator\": mlp_grid.best_estimator_,\n","                                 \"final_test_accuracy\": 0}\n","        self.model = mlp_grid.best_estimator_\n","    \n","    def predict(self, doc):\n","        best_p = [None, 0]\n","        best_a = [None, 0]\n","        a = []\n","        test_data = []\n","        test_labels = []\n","        for row in doc:\n","            for b in row.get_features():\n","                test_data.append(b[1:])\n","            test_labels.extend(row.get_gold_labels())\n","            \n","            # p = self.model.predict_proba([row[1:]])[0]\n","            # if best_p[1] < p[2]:\n","            #     best_p = [row[0], p[2]]\n","            # if best_a[1] < p[0]:\n","            #     best_a = [row[0], p[0]]\n","                \n","            # a.append([p,row[0]])\n","        y_pred = self.model.predict(test_data)\n","        print(\"MLP Confusion Matrix\")\n","        print(metrics.confusion_matrix(y_true=test_labels, y_pred=y_pred))\n","        print()\n","        print(\"MLP Classification Report\")\n","        print(metrics.classification_report(test_labels, y_pred))\n","    \n","    def test(self, test_data):\n","        acc = 0.0\n","        data = []\n","        labels = []\n","        for doc in test_data:\n","            for row in doc.get_features():\n","                data.append(row[1:])\n","            labels.extend(doc.get_gold_labels())\n","        return self.model.score(data, labels)\n","\n","class Model_Tree():\n","    # Class variables.\n","    training_data = None\n","    training_labels = None\n","    model_name = None\n","    model = None\n","    for_csv = []\n","    \n","    def __init__(self, data):\n","        # Initilzie class variables.\n","        self.training_data = []\n","        self.training_labels = []\n","        x = []\n","        y = []\n","        for doc in data:\n","            for row in doc.get_features():\n","                self.training_data.append(row[1:])\n","                x.append(row[1])\n","                y.append(row[2])\n","            self.training_labels.extend(doc.get_gold_labels())\n","    \n","    def train_model(self, parameters):\n","        clf = tree.AdaBoostClassifier(**parameters)\n","        clf.fit(self.training_data, self.training_labels)\n","        self.model = clf\n","    \n","    def predict(self, doc):\n","        best_p = [None, 0]\n","        best_a = [None, 0]\n","        a = []\n","        test_data = []\n","        test_labels = []\n","        for row in doc:\n","            for b in row.get_features():\n","                test_data.append(b[1:])\n","            test_labels.extend(row.get_gold_labels())\n","            \n","            # p = self.model.predict_proba([row[1:]])[0]\n","            # if best_p[1] < p[2]:\n","            #     best_p = [row[0], p[2]]\n","            # if best_a[1] < p[0]:\n","            #     best_a = [row[0], p[0]]\n","                \n","            # a.append([p,row[0]])\n","        y_pred = self.model.predict(test_data)\n","        print(\"Tree Confusion Matrix\")\n","        print(metrics.confusion_matrix(y_true=test_labels, y_pred=y_pred))\n","        print()\n","        print(\"Tree Classification Report\")\n","        print(metrics.classification_report(test_labels, y_pred))\n","    \n","    def test(self, test_data):\n","        acc = 0.0\n","        data = []\n","        labels = []\n","        for doc in test_data:\n","            for row in doc.get_features():\n","                data.append(row[1:])\n","            labels.extend(doc.get_gold_labels())\n","        return self.model.score(data, labels)\n","    \n","    \n","class Model_AdaBoost():\n","    # Class variables.\n","    training_data = None\n","    training_labels = None\n","    model_name = None\n","    model = None\n","    for_csv = []\n","    \n","    def __init__(self, data):\n","        # Initilzie class variables.\n","        self.training_data = []\n","        self.training_labels = []\n","        self.grid_results = []\n","        x = []\n","        y = []\n","        for doc in data:\n","            for row in doc.get_features():\n","                self.training_data.append(row[1:])\n","                x.append(row[1])\n","                y.append(row[2])\n","            self.training_labels.extend(doc.get_gold_labels())\n","    \n","    def train_model(self, parameters):\n","        clf = AdaBoostClassifier(**parameters)\n","        print(\"Training Adaboost...\")\n","        clf.fit(self.training_data, self.training_labels)\n","        self.model = clf\n","    \n","    def predict(self, doc):\n","        best_p = [None, 0]\n","        best_a = [None, 0]\n","        a = []\n","        test_data = []\n","        test_labels = []\n","        for row in doc:\n","            for b in row.get_features():\n","                test_data.append(b[1:])\n","            test_labels.extend(row.get_gold_labels())\n","            \n","            # p = self.model.predict_proba([row[1:]])[0]\n","            # if best_p[1] < p[2]:\n","            #     best_p = [row[0], p[2]]\n","            # if best_a[1] < p[0]:\n","            #     best_a = [row[0], p[0]]\n","                \n","            # a.append([p,row[0]])\n","        y_pred = self.model.predict(test_data)\n","        print(\"AdaBoost Confusion Matrix\")\n","        print(metrics.confusion_matrix(y_true=test_labels, y_pred=y_pred))\n","        print()\n","        print(\"AdaBoost Classification Report\")\n","        print(metrics.classification_report(test_labels, y_pred))\n","    \n","    def test(self, test_data):\n","        acc = 0.0\n","        data = []\n","        labels = []\n","        for doc in test_data:\n","            for row in doc.get_features():\n","                data.append(row[1:])\n","            labels.extend(doc.get_gold_labels())\n","        return self.model.score(data, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLABu9TZ0ObB","colab_type":"code","colab":{}},"source":["# Installs and Downloads ------------------------------------------------------\n","# Note: spaCy installation is version 2.1.0.\n","import nltk\n","# import spacy\n","# import neuralcoref\n","import re\n","from collections import defaultdict\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from tabulate import tabulate\n","import numpy as np\n","\n","#nltk.download(\"popular\")\n","#nltk.download(\"vader_lexicon\")\n","\n","# Document Class --------------------------------------------------------------\n","class Document():\n","    # Class variables.\n","    title = None\n","    text = None\n","    protagonist_g = None\n","    antagonist_g = None\n","    ner_tags = None\n","    sentiments = None\n","    corefs = None\n","    \n","    def __init__(self, title, file, protagonist, antagonist):\n","        # Initilize class variables.\n","        self.title = title\n","        self.protagonist_g = self.entitiy_cleaner(protagonist)\n","        self.antagonist_g = self.entitiy_cleaner(antagonist)\n","        self.open_file(file)\n","        self.generate_features()\n","    \n","    def open_file(self, file):\n","        # Open file.\n","        try:\n","            all_lines = []\n","            line = \"\"\n","            with open(file, encoding=\"utf8\") as file:\n","                header_end = False\n","                for l in file.readlines():\n","                    if \"*** END OF THIS PROJECT GUTENBERG EBOOK\" in l:\n","                        break\n","                    if \"*** START OF THIS PROJECT GUTENBERG EBOOK\" in l:\n","                        header_end = True\n","                    if header_end == False:\n","                        continue\n","                    if re.search(\"\\w\", l) is None:\n","                        all_lines.append(re.sub(\"\\s+\", \" \", line.strip()))\n","                        line = \"\"\n","                    else:\n","                        line += l.strip() + \" \"\n","            self.text = []\n","            for line in all_lines:\n","                self.text.append(list(nltk.sent_tokenize(line)))\n","        except Exception as e:\n","            print(\"Error: Error opening file.\")\n","            print(e)\n","            \n","    def generate_features(self):\n","        # Generate all features from text.\n","        self.ner_tags = set()\n","        self.sentiments = {}\n","        self.corefs = defaultdict(int)\n","        nlp = spacy.load(\"en_core_web_sm\")\n","        neuralcoref.add_to_pipe(nlp)\n","        \n","        # Iterate through sentences.\n","        for par in self.text:\n","            doc = \"\"\n","            for sent in par:\n","                if len(doc) > 0:\n","                    doc += \" \"\n","                doc += sent\n","                \n","                # Get pos and ner tags.\n","                tokens = nltk.word_tokenize(sent)\n","                tokens = nltk.pos_tag(tokens)\n","                tokens = nltk.chunk.ne_chunk(tokens)\n","                for chunk in tokens:\n","                    if hasattr(chunk, \"label\"):\n","                        if chunk.label() == \"PERSON\" or chunk.label() == \"ORGANIZATION\":\n","                            ent = self.entitiy_cleaner(\"\".join(c[0] for c in chunk))\n","                            self.ner_tags.add(ent)\n","                            if ent not in self.sentiments:\n","                                self.sentiments.update({ent: []})\n","                            self.sentiments[ent].append(sent)\n","            \n","            # Do coreference resolution.\n","            crefs = defaultdict(int)\n","            doc = nlp(doc)\n","            for ent in doc.ents:\n","                key = self.entitiy_cleaner(ent)\n","                if ent._.is_coref:\n","                    crefs[key] += len(list(ent._.coref_cluster))\n","                else:\n","                    crefs[key] += 1\n","            for k in sorted(crefs, key = len, reverse = True):\n","                in_c = False\n","                for n in self.ner_tags:\n","                    if k in n or n in k:\n","                        in_c = True\n","                if in_c == False:\n","                    continue\n","                in_c = False\n","                for k_m in crefs:\n","                    if k in k_m:\n","                        self.corefs[k_m] += crefs[k]/2\n","                        in_c = True\n","                        break\n","                    if k_m in k:\n","                        self.corefs[k] += crefs[k_m]/2\n","                        in_c = True\n","                        break\n","                if in_c == False:\n","                    self.corefs[k] += 1\n","        total = 0.0\n","        for ent in self.corefs:\n","            total += self.corefs[ent]\n","        for ent in self.corefs:\n","            self.corefs[ent] = self.corefs[ent]/total\n","        \n","        # Do sentiment analysis.\n","        sid = SentimentIntensityAnalyzer()\n","        for ent in self.sentiments:\n","            s = 0\n","            for sent in self.sentiments[ent]:\n","#                print(sid.polarity_scores(sent))\n","                s += sid.polarity_scores(sent)[\"compound\"]\n","            self.sentiments[ent] = s/len(self.sentiments[ent])\n","    \n","    def entitiy_cleaner(self, ent):\n","        return re.sub(\"\\s+\", \" \", str(ent).strip()).lower()\n","    \n","    def get_features(self):\n","        features = []\n","        for ent in self.ner_tags:\n","            if ent not in self.sentiments or ent not in self.corefs:\n","                continue\n","            features.append([str(ent), self.corefs[ent], self.sentiments[ent]])\n","        return features\n","    \n","    def get_gold_labels(self):\n","        labels = []\n","        for ent in self.ner_tags:\n","            if ent not in self.sentiments or ent not in self.corefs:\n","                continue\n","            label = \"\"\n","            if ent in self.protagonist_g or self.protagonist_g in ent:\n","                label = \"p\"\n","            elif ent in self.antagonist_g or self.antagonist_g in ent:\n","                label = \"a\"\n","            else:\n","                label = \"n\"\n","            labels.append(label)\n","        return labels\n","    \n","    def preds(self):\n","        cref_keys = sorted(self.corefs, key = lambda k: len(self.corefs[k]), reverse = False)\n","        for k in self.corefs:\n","            if \"gutenberg\" in k:\n","                del cref_keys[cref_keys.index(k)]\n","        \n","        p = cref_keys[-1]\n","        a = cref_keys[-2]\n","        pg = self.protagonist_g\n","        if pg in p or p in pg:\n","            pg = True\n","        else:\n","            pg = False\n","        ag = self.antagonist_g\n","        if ag in a or a in ag:\n","            ag = True\n","        else:\n","            ag = False\n","        values = [\n","                    [\"Protagonist\", p, self.protagonist_g, pg], \n","                    [\"Antagonist\", a, self.antagonist_g, ag]\n","                    ]\n","        print(self.title)\n","\n","        return [pg, ag]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7yGOhyc0W9P","colab_type":"code","outputId":"65dd0739-ef3f-4b03-84cb-55cf0af63f74","executionInfo":{"status":"ok","timestamp":1574358350364,"user_tz":360,"elapsed":68065,"user":{"displayName":"Derrion Thompson","photoUrl":"","userId":"03974428895377299514"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T7crxm7A7edA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"8c371a61-6090-40a0-937b-e5fb7537a54b","executionInfo":{"status":"error","timestamp":1574358392137,"user_tz":360,"elapsed":563,"user":{"displayName":"Derrion Thompson","photoUrl":"","userId":"03974428895377299514"}}},"source":["import pickle\n","\n","model_ab = None\n","model_mlp = None\n","model_nb = None\n","model_svm = None\n","model_tree = None\n","\n","with open(\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/ab.pickle\", \"rb\") as file:\n","  model_ab = pickle.load(file)\n","with open(\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/mlp.pickle\", \"rb\") as file:\n","  model_mlp = pickle.load(file)\n","with open(\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/nb.pickle\", \"rb\") as file:\n","  model_nb = pickle.load(file)\n","with open(\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/svm.pickle\", \"rb\") as file:\n","  model_svm = pickle.load(file)\n","with open(\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/tree.pickle\", \"rb\") as file:\n","  model_tree = pickle.load(file)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-ee1998b4be9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/ab.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mmodel_ab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/CSCE_5215_ML/Final_Project_Files/Models/mlp.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'class_model'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"iYyxpkeJ0ASs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"e00bc52d-17a4-4cc1-d764-3b3bc7fdd074","executionInfo":{"status":"error","timestamp":1574358356324,"user_tz":360,"elapsed":841,"user":{"displayName":"Derrion Thompson","photoUrl":"","userId":"03974428895377299514"}}},"source":["# Installs and Downloads ------------------------------------------------------\n","import pandas as pd\n","from class_document import Document\n","# from class_model import Model_SVM, Model_NB, Model_MLP, Model_Tree, Model_AdaBoost\n","import pickle\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","# Get Data --------------------------------------------------------------------\n","all_docs = []\n","\n","# with open(\"data_labels.csv\", \"rb\") as file:\n","# #with open(\"data_labels_2.csv\", \"rb\") as file:\n","#     labels = pd.read_csv(file)\n","#     for i, row in labels.iterrows():\n","#         print(i)\n","#         try:\n","#             all_docs.append(Document(row[\"Book\"], \"Files/\"+row[\"file\"], row[\"Protagonist\"], row[\"Antagonist\"]))\n","# #            with open(\"Dataset/%s.pickle\"%(row[\"Book\"]), \"wb\") as file:\n","#             with open(\"Dataset/%s.pickle\"%(row[\"Book\"]), \"wb\") as file:\n","#                 pickle.dump(all_docs[-1], file)\n","#         except Exception as e:\n","#             print(\"Error: No file found \"+row[\"file\"]+\".\")\n","#             print(e)\n","#             print()\n","#         print(\"File %d Complete\"%(i))\n","\n","for file in os.listdir(\"Dataset\"):\n","    filename = os.fsdecode(file)\n","    if filename.endswith(\".pickle\"):\n","        print(filename)\n","#        print(str(os.path.getsize(\"Dataset/\" + filename)))\n","        try:\n","            with open(\"Dataset/\" + filename,\"rb\") as picklefile:\n","                doc = pickle.load(picklefile)\n","                all_docs.append(doc)\n","        except Exception as e:\n","            print(e)\n","\n","train_data = []\n","test_data = []\n","for i, doc in enumerate(all_docs):\n","    if i < int(len(all_docs)*0.8)+1:\n","        train_data.append(doc)\n","    else:\n","        test_data.append(doc)\n","\n","# Train Models ---------------------------------------------------------------\n","# params_svm = {\n","#             \"probability\": True,\n","#             \"kernel\": \"rbf\",\n","#             \"class_weight\": \"balanced\",\n","#             \"gamma\": \"auto\",\n","#             # \"verbose\": True\n","#             }\n","# model_svm = Model_SVM(train_data)\n","# model_svm.train_model(params_svm)\n","# with open(\"Models/svm.pickle\", \"wb\") as file:\n","#     pickle.dump(model_svm, file)\n","    \n","# params_nb = {\n","#             }\n","# model_nb = Model_NB(train_data)\n","# model_nb.train_model(params_nb)\n","# with open(\"Models/nb.pickle\", \"wb\") as file:\n","#     pickle.dump(model_nb, file)\n","    \n","# params_mlp = {\n","#         \"verbose\": True\n","#             }\n","# model_mlp = Model_MLP(train_data)\n","# model_mlp.train_model(params_mlp)\n","# with open(\"Models/mlp.pickle\", \"wb\") as file:\n","#     pickle.dump(model_mlp, file)\n","    \n","# params_tree = {\n","#             }\n","# model_tree = Model_Tree(train_data)\n","# model_tree.train_model(params_tree)\n","# with open(\"Models/tree.pickle\", \"wb\") as file:\n","#     pickle.dump(model_tree, file)\n","    \n","print(\"SVM:\")\n","# results_svm = model_svm.test(test_data)\n","\n","predict_svm = model_svm.predict(test_data)\n","print()\n","print(\"NB:\")\n","# results_nb = model_nb.test(test_data)\n","predict_nb = model_nb.predict(test_data)\n","print()\n","print(\"MLP:\")\n","# results_mlp = model_mlp.test(test_data)\n","predict_mlp = model_mlp.predict(test_data)\n","\n","print(\"Tree:\")\n","# print(model_tree.test(test_data))\n","predict_tree = model_tree.predict(test_data)\n","\n","# params_ab = { \"n_estimators\":10\n","#             }\n","# model_ab = Model_AdaBoost(train_data)\n","# model_ab.train_model(params_ab)\n","# with open(\"Models/ab.pickle\", \"wb\") as file:\n","#     pickle.dump(model_ab, file)\n","    \n","print(\"Adaboost:\")\n","# results_ab = model_ab.test(test_data)\n","predict_ab = model_ab.predict(test_data)\n","print()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a75e04d4603b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclass_document\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# from class_model import Model_SVM, Model_NB, Model_MLP, Model_Tree, Model_AdaBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'class_document'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}