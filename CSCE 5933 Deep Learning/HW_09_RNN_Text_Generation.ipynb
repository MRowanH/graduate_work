{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW_09_RNN_Text_Generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN23tIaxEe+58fCd1G8uRn5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"53673922c5ed4178a32637837cac42eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_acba3911064942b0854b091110956df4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3d307babcb624b2fa05510762ced2edb","IPY_MODEL_dec67484eadb48a28b45da54673f971a"]}},"acba3911064942b0854b091110956df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d307babcb624b2fa05510762ced2edb":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a0c516cd539f4d32bb00852af4f3f9c7","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"IntProgressModel","bar_style":"success","max":258,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":258,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_963faa1486f5442489574d73fa1974c0"}},"dec67484eadb48a28b45da54673f971a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_05e7e0cd3fd04ab9914958bcd6500b7c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 258/258 [06:51&lt;00:00,  1.60s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2317b9a9f8c45bbad99a13644d93073"}},"a0c516cd539f4d32bb00852af4f3f9c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"963faa1486f5442489574d73fa1974c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05e7e0cd3fd04ab9914958bcd6500b7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d2317b9a9f8c45bbad99a13644d93073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"KpZOw8HTCYBR","colab_type":"code","colab":{}},"source":["# Imports ----------------------------------------------------------------------\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import os\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnxvJVl2DkKw","colab_type":"code","colab":{}},"source":["# Functions --------------------------------------------------------------------\n","\n","def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","def build_model_GRU(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.GRU(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model\n","\n","def build_model_LSTM(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.LSTM(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model\n","\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 1000\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 1.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the character returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted character as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv5nzDMLCeCg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1585854335825,"user_tz":300,"elapsed":2817,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"665a4b54-a2fa-46df-b05e-06778e6f6948"},"source":["# Global Variables -------------------------------------------------------------\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","BATCH_SIZE = None\n","BUFFER_SIZE = None\n","vocab_size = None\n","embedding_dim = None\n","rnn_units = None\n","EPOCHS = None"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jxscYgOCj2q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"status":"ok","timestamp":1585854336160,"user_tz":300,"elapsed":3101,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"ea3f7386-1006-4266-a80e-1ed4f3caf28d"},"source":["# Evaluate Data ----------------------------------------------------------------\n","\n","# Read, then decode for py2 compat.\n","# length of text is the number of characters in it\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","print ('Length of text: {} characters'.format(len(text)))\n","print()\n","\n","# Take a look at the first 250 characters in text\n","print(text[:250])\n","print()\n","\n","# The unique characters in the file\n","vocab = sorted(set(text))\n","print ('{} unique characters'.format(len(vocab)))\n","print()\n","\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","text_as_int = np.array([char2idx[c] for c in text])\n","print('{')\n","for char,_ in zip(char2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')\n","print()\n","\n","# Show how the first 13 characters from the text are mapped to integers\n","print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n","print()\n","\n","# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length of text: 1115394 characters\n","\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","\n","65 unique characters\n","\n","{\n","  '\\n':   0,\n","  ' ' :   1,\n","  '!' :   2,\n","  '$' :   3,\n","  '&' :   4,\n","  \"'\" :   5,\n","  ',' :   6,\n","  '-' :   7,\n","  '.' :   8,\n","  '3' :   9,\n","  ':' :  10,\n","  ';' :  11,\n","  '?' :  12,\n","  'A' :  13,\n","  'B' :  14,\n","  'C' :  15,\n","  'D' :  16,\n","  'E' :  17,\n","  'F' :  18,\n","  'G' :  19,\n","  ...\n","}\n","\n","'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SA06s3TtCVCW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"executionInfo":{"status":"ok","timestamp":1585854342452,"user_tz":300,"elapsed":9352,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"99a717a8-2e09-486f-960a-7027189e4752"},"source":["# Create Dataset ---------------------------------------------------------------\n","\n","# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])\n","print()\n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))\n","print()\n","\n","dataset = sequences.map(split_input_target)\n","\n","for input_example, target_example in  dataset.take(1):\n","  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n","print()\n","\n","for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","  print(\"Step {:4d}\".format(i))\n","  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","  print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n","print()\n","\n","# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)\n","print()\n","\n","# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F\n","i\n","r\n","s\n","t\n","\n","'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","\n","Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","\n","Step    0\n","  input: 18 ('F')\n","  expected output: 47 ('i')\n","Step    1\n","  input: 47 ('i')\n","  expected output: 56 ('r')\n","Step    2\n","  input: 56 ('r')\n","  expected output: 57 ('s')\n","Step    3\n","  input: 57 ('s')\n","  expected output: 58 ('t')\n","Step    4\n","  input: 58 ('t')\n","  expected output: 1 (' ')\n","\n","<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dWg8we5ZSu5g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1585854604506,"user_tz":300,"elapsed":271370,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"8b7680d3-64b3-4ac3-f899-00793aa0c321"},"source":["# GRU --------------------------------------------------------------------------\n","model = build_model_GRU(\n","                        vocab_size = len(vocab),\n","                        embedding_dim=embedding_dim,\n","                        rnn_units=rnn_units,\n","                        batch_size=BATCH_SIZE)\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size) - GRU\")\n","print()\n","\n","model.summary()\n","print()\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","print(sampled_indices)\n","print()\n","\n","print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n","print()\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","print()\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints_GRU'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","EPOCHS=10\n","\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n","print(tf.train.latest_checkpoint(checkpoint_dir))\n","print()\n","\n","model = build_model_GRU(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))\n","model.summary()\n","print()\n","\n","print(\"Generated text for GRU\")\n","print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size) - GRU\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (64, None, 256)           16640     \n","_________________________________________________________________\n","gru (GRU)                    (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense (Dense)                (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","[54 42 28  4 23 59 56 21 53 16 19 51 45 35 14 40 25 55 11 61 37 37 42 13\n"," 43  0 39 57 29 23 60 26 58 57 63 55 56 20 56  0 50  2 11 59 12 50 29 64\n"," 27 63  7 22 51 14 28 62 55 39 32  5 18 58 35  6 61 40 51 24 60 27 53 12\n"," 29 59  1 17 42 28  5  1 48 29  7 61 51 27 52 47 22 37 30 32 29 24 22 18\n"," 47 21 38 34]\n","\n","Input: \n"," ' have a word anon. Lay hold on him.\\n\\nLUCIO:\\nThis may prove worse than hanging.\\n\\nDUKE VINCENTIO:\\n\\nANG'\n","\n","Next Char Predictions: \n"," \"pdP&KurIoDGmgWBbMq;wYYdAe\\nasQKvNtsyqrHr\\nl!;u?lQzOy-JmBPxqaT'FtW,wbmLvOo?Qu EdP' jQ-wmOniJYRTQLJFiIZV\"\n","\n","Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.172571\n","\n","Epoch 1/10\n","172/172 [==============================] - 24s 137ms/step - loss: 2.6793\n","Epoch 2/10\n","172/172 [==============================] - 23s 136ms/step - loss: 1.9541\n","Epoch 3/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.6912\n","Epoch 4/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.5433\n","Epoch 5/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.4555\n","Epoch 6/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.3941\n","Epoch 7/10\n","172/172 [==============================] - 23s 134ms/step - loss: 1.3503\n","Epoch 8/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.3125\n","Epoch 9/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.2770\n","Epoch 10/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.2448\n","./training_checkpoints_GRU/ckpt_10\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","gru_1 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_1 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Generated text for GRU\n","ROMEO: then,\n","For when it serren out\n","Many man that he diest the duke.\n","\n","DUKE VINCENTIO:\n","And six you now?\n","\n","EDWARD:\n","O, lay itor look now? love none immodeshal\n","Dispirity's bole of thine a halk of my sovereign. You that there now?\n","Swert dare his thence, madhather,\n","You home time hence with election\n","As in a larg's by a dogrous tears: and for all it with fearful lage!\n","Our luining Rosations, what news me to His the year old,\n","But likewill else speak from even to this way to hold\n","When she shall sent for me the leads as one.\n","\n","BRUTUS:\n","My Lord Northumbeart, and so it is leanting that you are not\n","This words pups time satisfied and heir agoin Herbior:\n","For I cannot wert how it, then? then why leving dance\n","Why they are bounted of grief, or express his gentleman.\n","Betwern the last of noble as charge?\n","\n","PETRUCHIO:\n","Look, after thou, an art thou rohal day?\n","\n","DORSET:\n","The prince there in is! and could his brother, breathe but\n","Pait is my son was glad to bring these restrite\n","Than my mother doth we serve it;\n","For foul my fa\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U2IQ8ouRTAOd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1585854918213,"user_tz":300,"elapsed":585003,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"55177f57-d188-482b-f836-2b915b3cadf8"},"source":["# LSTM -------------------------------------------------------------------------\n","model = build_model_LSTM(\n","                        vocab_size = len(vocab),\n","                        embedding_dim=embedding_dim,\n","                        rnn_units=rnn_units,\n","                        batch_size=BATCH_SIZE)\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size) - LSTM\")\n","print()\n","\n","model.summary()\n","print()\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","print(sampled_indices)\n","print()\n","\n","print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n","print()\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","print()\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints_LSTM'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","EPOCHS=10\n","\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n","print(tf.train.latest_checkpoint(checkpoint_dir))\n","print()\n","\n","model = build_model_LSTM(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))\n","model.summary()\n","print()\n","\n","print(\"Generated text for LSTM\")\n","print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size) - LSTM\n","\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (64, None, 256)           16640     \n","_________________________________________________________________\n","lstm (LSTM)                  (64, None, 1024)          5246976   \n","_________________________________________________________________\n","dense_2 (Dense)              (64, None, 65)            66625     \n","=================================================================\n","Total params: 5,330,241\n","Trainable params: 5,330,241\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","[17 56  0 63 28 57 54  7 43 32 22 39  1 44  9 25  7  3 23 38  2  9 48 14\n"," 30 22 19  1 51 24 62 25 22 37 57 30 32  6 57 36 64 53 39 28 54 18  1 47\n"," 38 28 16 49 64 25 54 60 19  9 52 12 36  7  3 58 38  3 63 26 55 31 60 30\n"," 24 15  1 55 57 42 40 14 33 11  5  7 26 57 62 16 59 27  1 51 13 31 40  0\n"," 57 47 27 50]\n","\n","Input: \n"," \"denied our course;\\nThe sun that sear'd the wings of my sweet boy\\nThy brother Edward, and thyself the\"\n","\n","Next Char Predictions: \n"," \"Er\\nyPsp-eTJa f3M-$KZ!3jBRJG mLxMJYsRT,sXzoaPpF iZPDkzMpvG3n?X-$tZ$yNqSvRLC qsdbBU;'-NsxDuO mASb\\nsiOl\"\n","\n","Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.1734743\n","\n","Epoch 1/10\n","172/172 [==============================] - 29s 168ms/step - loss: 2.5498\n","Epoch 2/10\n","172/172 [==============================] - 29s 167ms/step - loss: 1.8603\n","Epoch 3/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.6141\n","Epoch 4/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.4845\n","Epoch 5/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.4084\n","Epoch 6/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.3523\n","Epoch 7/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.3073\n","Epoch 8/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.2677\n","Epoch 9/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.2302\n","Epoch 10/10\n","172/172 [==============================] - 29s 168ms/step - loss: 1.1933\n","./training_checkpoints_LSTM/ckpt_10\n","\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (1, None, 1024)           5246976   \n","_________________________________________________________________\n","dense_3 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 5,330,241\n","Trainable params: 5,330,241\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Generated text for LSTM\n","ROMEO: ETRANFEMERNYZDUKMz:XBISUHCY:ZUSSSTIANLAZASET:\n","Aim, like my husband's reason to you?\n","\n","BIANCA:\n","Alas? I dare never mercy stanlest thou\n","And livening in the Duke of Gloucester will cause think you;\n","How many your friends as feel!\n","I think he did outs suffer it.\n","I think I'll rest they of their truth?\n","That it inclusing on the breed, that executes\n","yet here to lay against all brights in sorrow sent\n","From Piorance: our execution of a flagmer\n","Of the beastly languis of himself, and rather\n","\n","KING RICHARD III:\n","Thy paraness have\n","full for him! you cannot come to wover your youth?\n","Therefore the world most become my mother you and into\n","Mischark, sir, if a subject plop'd hell,\n","Your good sister, the ways dog, her cries 'God s:\n","We'll all king on the world is rush'd with order,\n","For that o' the stop of the earth anger.\n","\n","First Senator:\n","More wager, now how angelious mates\n","Instand get thee the people.\n","\n","Ciress?\n","\n","Provost:\n","For. 3 GEUMENCE:\n","And him thou hadst deserved a leanties,\n","From me; nor the irst end you no lest.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zyp5YkWmFUN2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["53673922c5ed4178a32637837cac42eb","acba3911064942b0854b091110956df4","3d307babcb624b2fa05510762ced2edb","dec67484eadb48a28b45da54673f971a","a0c516cd539f4d32bb00852af4f3f9c7","963faa1486f5442489574d73fa1974c0","05e7e0cd3fd04ab9914958bcd6500b7c","d2317b9a9f8c45bbad99a13644d93073"]},"executionInfo":{"status":"ok","timestamp":1585855332683,"user_tz":300,"elapsed":999426,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"c35e113e-0f09-47f6-8df4-d648aa61ec23"},"source":["# Load Alternate Dataset -------------------------------------------------------\n","dataset = tfds.load(\"wikipedia/20190301.en\", split=\"train\")\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset wikipedia/20190301.en/1.0.0 (download: 15.72 GiB, generated: Unknown size, total: 15.72 GiB) to /root/tensorflow_datasets/wikipedia/20190301.en/1.0.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Dataset wikipedia is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead set\n","data_dir=gs://tfds-data/datasets.\n","\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53673922c5ed4178a32637837cac42eb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Dl Completed...', max=258, style=ProgressStyle(description_wi…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\u001b[1mDataset wikipedia downloaded and prepared to /root/tensorflow_datasets/wikipedia/20190301.en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8D7gXqmhOftc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"status":"ok","timestamp":1585855335965,"user_tz":300,"elapsed":1002659,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"4b0f8c4e-63f2-49b1-ca19-f5ba125c59bf"},"source":["# Evaluate Data ----------------------------------------------------------------\n","\n","# Read, then decode for py2 compat.\n","# length of text is the number of characters in it\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","print ('Length of text: {} characters'.format(len(text)))\n","print()\n","\n","# Take a look at the first 250 characters in text\n","print(text[:250])\n","print()\n","\n","# The unique characters in the file\n","vocab = sorted(set(text))\n","print ('{} unique characters'.format(len(vocab)))\n","print()\n","\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","text_as_int = np.array([char2idx[c] for c in text])\n","print('{')\n","for char,_ in zip(char2idx, range(20)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ...\\n}')\n","print()\n","\n","# Show how the first 13 characters from the text are mapped to integers\n","print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n","print()\n","\n","# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length of text: 1115394 characters\n","\n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","\n","65 unique characters\n","\n","{\n","  '\\n':   0,\n","  ' ' :   1,\n","  '!' :   2,\n","  '$' :   3,\n","  '&' :   4,\n","  \"'\" :   5,\n","  ',' :   6,\n","  '-' :   7,\n","  '.' :   8,\n","  '3' :   9,\n","  ':' :  10,\n","  ';' :  11,\n","  '?' :  12,\n","  'A' :  13,\n","  'B' :  14,\n","  'C' :  15,\n","  'D' :  16,\n","  'E' :  17,\n","  'F' :  18,\n","  'G' :  19,\n","  ...\n","}\n","\n","'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eKRzvweOOgRX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"executionInfo":{"status":"ok","timestamp":1585855335968,"user_tz":300,"elapsed":1002602,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"28601671-fb4d-4608-8391-316681652004"},"source":["# Create Dataset ---------------------------------------------------------------\n","\n","# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])\n","print()\n","\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))\n","print()\n","\n","dataset = sequences.map(split_input_target)\n","\n","for input_example, target_example in  dataset.take(1):\n","  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n","print()\n","\n","for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","  print(\"Step {:4d}\".format(i))\n","  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","  print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n","print()\n","\n","# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)\n","print()\n","\n","# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F\n","i\n","r\n","s\n","t\n","\n","'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","\n","Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","\n","Step    0\n","  input: 18 ('F')\n","  expected output: 47 ('i')\n","Step    1\n","  input: 47 ('i')\n","  expected output: 56 ('r')\n","Step    2\n","  input: 56 ('r')\n","  expected output: 57 ('s')\n","Step    3\n","  input: 57 ('s')\n","  expected output: 58 ('t')\n","Step    4\n","  input: 58 ('t')\n","  expected output: 1 (' ')\n","\n","<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ekagRPl_C843","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1585855593888,"user_tz":300,"elapsed":1260467,"user":{"displayName":"Mica Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKXHcqzSUcoMDkbPtJrWlpdkkU2LO3kJyPfSrM=s64","userId":"02567847060328523003"}},"outputId":"bbefe4f4-a1c0-4a3c-ec58-cfa6b4a3406b"},"source":["# Alternate Dataset - GRU ------------------------------------------------------\n","\n","model = build_model_GRU(\n","                        vocab_size = len(vocab),\n","                        embedding_dim=embedding_dim,\n","                        rnn_units=rnn_units,\n","                        batch_size=BATCH_SIZE)\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size) - GRU alternate dataset\")\n","print()\n","\n","model.summary()\n","print()\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","print(sampled_indices)\n","print()\n","\n","print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n","print()\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","print()\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints_GRU_alt_dataset'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","EPOCHS=10\n","\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n","print(tf.train.latest_checkpoint(checkpoint_dir))\n","print()\n","\n","model = build_model_GRU(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))\n","model.summary()\n","print()\n","\n","print(\"Generated text for GRU alternate dataset\")\n","print(generate_text(model, start_string=u\"ROMEO: \"))\n","print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size) - GRU alternate dataset\n","\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (64, None, 256)           16640     \n","_________________________________________________________________\n","gru_2 (GRU)                  (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense_4 (Dense)              (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","[28 55 39 27 52 47 44 14  3 31 13 44 60  0 42 64  2 12  9 49 26 51 46 36\n"," 40 25 42 12 42  5 11 36  6 32 32 12 26  9 41 17 51  8 44 40 54  1  6 24\n"," 37  7 15 22 63  2 21 46 43 61 32  7 16 51 14 34 53 51 28 58  7 26 20 32\n"," 43 60 54  8 55 37 54 49 61 47 38 55 38 64 22  7 22 59 35 30 20 44 45 63\n"," 24  1 55 43]\n","\n","Input: \n"," 'en they rode from London,\\nWere jocund, and supposed their state was sure,\\nAnd they indeed had no cau'\n","\n","Next Char Predictions: \n"," \"PqaOnifB$SAfv\\ndz!?3kNmhXbMd?d';X,TT?N3cEm.fbp ,LY-CJy!IhewT-DmBVomPt-NHTevp.qYpkwiZqZzJ-JuWRHfgyL qe\"\n","\n","Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.1761723\n","\n","Epoch 1/10\n","172/172 [==============================] - 24s 137ms/step - loss: 2.6322\n","Epoch 2/10\n","172/172 [==============================] - 23s 136ms/step - loss: 1.9363\n","Epoch 3/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.6761\n","Epoch 4/10\n","172/172 [==============================] - 23s 136ms/step - loss: 1.5336\n","Epoch 5/10\n","172/172 [==============================] - 23s 136ms/step - loss: 1.4481\n","Epoch 6/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.3893\n","Epoch 7/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.3434\n","Epoch 8/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.3051\n","Epoch 9/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.2712\n","Epoch 10/10\n","172/172 [==============================] - 23s 135ms/step - loss: 1.2383\n","./training_checkpoints_GRU_alt_dataset/ckpt_10\n","\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","gru_3 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_5 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Generated text for GRU alternate dataset\n","ROMEO: It stay awhile:\n","A bary in thy to be sleep of his?\n","Go fet in mine own. Was cry\n","'ge is give me to the feeling? out of them,\n","As much t fall eyes.\n","\n","LEONTES:\n","Once more, they leave this comfort it the first,\n","I twent thereof.\n","I will resolve none.\n","\n","PETRUCHIO:\n","Every have with yrum of the evile them?\n","\n","PETRUCHIO:\n","Sir, hold of knively nor hard to tell them.\n","\n","COMINIUS:\n","Canisel!\n","Thou wert been so corruption famous by the king which extellow'd up, I hate the worst ride us, wherein!\n","\n","Messenger:\n","Stay: if had he 'sea: if you cannot be yours.\n","\n","KING RICHARD II:\n","Gor! what's thy head to tell the help of hopp some cold those that\n","I am grieved up, fathers;\n","Beside no triumph of that rarion appoldom with the mercy,--O, trumpet thou give me take his brdied,\n","And for an impity\n","forget; uncle ke are troupand more:\n","Thou'rt to be some courtees, he will not look thee:\n","If you say him: I'll give thee; for it may can abod\n","This present percy makes my sense against the way;\n","Where never from the earld Claudio in all the king\n","\n"],"name":"stdout"}]}]}