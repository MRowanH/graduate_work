{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M7ZQSLkvn9u4"},"outputs":[],"source":["!pip install keras-tuner &> /dev/null\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","import keras_tuner as kt\n","from keras_tuner import HyperModel\n","from keras_tuner.tuners import RandomSearch,BayesianOptimization\n","import keras.backend as K\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,f1_score,roc_auc_score,precision_score, recall_score\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QHYvr6LFkZPp"},"outputs":[],"source":["#train test split\n","X_train_brty, X_test_brty, y_train_brty, y_test_brty = train_test_split(X_brty, y_brty, test_size = 0.3,stratify=y_brty)\n","\n","#-----------------------------------------------------------------------------------------------\n","#--------------------------------HYPER-TUNE MODEL-----------------------------------------------\n","#-----------------------------------------------------------------------------------------------\n","#same like RandomSearch in Machine learning, we need to setup different combination of hyperparameter. I am using Keras Tuner API for this\n","\n","def model_tune(hyperparameter,**kwargs):\n","  model = keras.Sequential() #initialize seq model\n","  #tune epoch and batch_size so i am giving a range\n","  kwargs['batch_size'] = hyperparameter.Int('batch_size', 8, 64, step=4)\n","  kwargs['epochs'] = hyperparameter.Int('epochs', 100, 500, step=5)\n","  drop_outs = hyperparameter.Float('drop_outs',0, 0.5, step=0.1, default=0)\n","\n","  model.add(tf.keras.layers.InputLayer(input_shape=702)) #I donot want change anything on my input layer so it is outside the tune loop\n","  # Tune the number of dense layers \n","  for i in range(hyperparameter.Int('num_layers', 1, 5)):\n","    hyperparameter_nodes = hyperparameter.Int('units_'+str(i), min_value=8, max_value=128, step=8) #for every layer I used number of nodes ranging from 8 - 1024\n","    model.add(tf.keras.layers.Dense(units=hyperparameter_nodes, activation='relu'))\n","    model.add(Dropout(drop_outs))\n","\n","  model.add(tf.keras.layers.Dense(1,activation='sigmoid')) #this is output layer \n","\n","  hyperparameter_learning_rate = hyperparameter.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\") # lets see what learning rate is the best for optimizer\n","  \n","  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hyperparameter_learning_rate),loss='binary_crossentropy',metrics=[keras.metrics.BinaryAccuracy()]) \n","\n","  return model\n","\n","tuned = RandomSearch(model_tune, objective='val_binary_accuracy',max_trials=50) # I am trying 300 different combination of neural networks\n","tuned.search(X_train_brty, y_train_brty, verbose = 1,validation_split = 0.2) #let call randomsearch and run hypertuning\n","\n","epochs = (tuned.get_best_hyperparameters(num_trials=1)[0]).get(\"epochs\")\n","batch_size = (tuned.get_best_hyperparameters(num_trials=1)[0]).get(\"batch_size\")\n","\n","tuned_model = tuned.hypermodel.build(tuned.get_best_hyperparameters(num_trials=1)[0])\n","\n","#we do not want to keep on training if the model is not doing good. so early stopping will stop model from training further\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy')\n","history = tuned_model.fit(X_train_brty, y_train_brty, verbose = 1,validation_split = 0.2,epochs = epochs,batch_size = batch_size)\n","pred_test = tuned_model.predict(X_test_brty)\n","y_pred_brty = np.where(pred_test > 0.5, 1, 0)"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"12qmKd7DdiK63QPwKo7yD5ilWk2eP01Np","timestamp":1663620839820}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}