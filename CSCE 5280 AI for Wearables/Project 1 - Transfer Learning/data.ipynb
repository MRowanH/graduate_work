{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1frRx7HysJrapppPNwAfDD1dlQAzUTbda","authorship_tag":"ABX9TyMVjtFk9FiZhTZg4Fh8TZ7v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xxbX7rWRGLy","executionInfo":{"status":"ok","timestamp":1664254588051,"user_tz":300,"elapsed":23134,"user":{"displayName":"Mica Haney","userId":"02567847060328523003"}},"outputId":"8a63a8d9-1dcb-4ab1-a3a9-6a007f52a617"},"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieving folder list\n","Processing file 138srRsR00ALdY5C1C09tLFO_bWGMOkoZ activity_Downstairs_df\n","Processing file 1QJTINMLihyyjljNEk7DBRQyMjGXVjPcf activity_Jogging_df\n","Processing file 1U_lzf-yOBC96IupG-9wS324Lyu78u916 activity_Sitting_df\n","Processing file 1wZitN2BWyzh_dW2OGSIy03YxVdKI3P07 activity_Standing_df\n","Processing file 1C5GpQ7RhX_t_h-9XGV9x7I3Leufs5_Sl activity_Upstairs_df\n","Processing file 1_hPbQ1MtEqqEI3KQ7ttIYGIdYH7yi36d activity_Walking_df\n","Retrieving folder list completed\n","Building directory structure\n","Building directory structure completed\n","Downloading...\n","From: https://drive.google.com/uc?id=138srRsR00ALdY5C1C09tLFO_bWGMOkoZ\n","To: /content/activity_clusters_df/activity_Downstairs_df\n","100% 6.45M/6.45M [00:00<00:00, 87.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QJTINMLihyyjljNEk7DBRQyMjGXVjPcf\n","To: /content/activity_clusters_df/activity_Jogging_df\n","100% 5.84M/5.84M [00:00<00:00, 156MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1U_lzf-yOBC96IupG-9wS324Lyu78u916\n","To: /content/activity_clusters_df/activity_Sitting_df\n","100% 978k/978k [00:00<00:00, 96.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1wZitN2BWyzh_dW2OGSIy03YxVdKI3P07\n","To: /content/activity_clusters_df/activity_Standing_df\n","100% 742k/742k [00:00<00:00, 90.1MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1C5GpQ7RhX_t_h-9XGV9x7I3Leufs5_Sl\n","To: /content/activity_clusters_df/activity_Upstairs_df\n","100% 2.13M/2.13M [00:00<00:00, 182MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1_hPbQ1MtEqqEI3KQ7ttIYGIdYH7yi36d\n","To: /content/activity_clusters_df/activity_Walking_df\n","100% 1.70M/1.70M [00:00<00:00, 169MB/s]\n","Download completed\n","freq train df\n","freq train x\n","freq train y\n","freq train x_shaped\n","freq val df\n","freq val x\n","freq val y\n","freq val x_shaped\n","freq test df\n","freq test x\n","freq test y\n","freq test x_shaped\n","infreq train df\n","infreq train x\n","infreq train y\n","infreq train x_shaped\n","infreq val df\n","infreq val x\n","infreq val y\n","infreq val x_shaped\n","infreq test df\n","infreq test x\n","infreq test y\n","infreq test x_shaped\n","comb train df\n","comb train x\n","comb train y\n","comb train x_shaped\n","comb val df\n","comb val x\n","comb val y\n","comb val x_shaped\n","comb test df\n","comb test x\n","comb test y\n","comb test x_shaped\n"]}],"source":["import pandas as pd\n","import tensorflow as tf\n","import numpy as np\n","import os\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, plot_confusion_matrix\n","import matplotlib.pyplot as plt\n","import pickle\n","\n","# Download data to runtime environment\n","if not os.path.exists(\"activity_clusters_df/\"):\n","    ! gdown --folder https://drive.google.com/drive/folders/1CqsUGqC6-8EIY7wOEL58LgpRYxRcnR_4?usp=sharing\n","\n","# Load files\n","a = {}\n","a.update({\"downstairs\": pd.read_csv(\"activity_clusters_df/activity_Downstairs_df\")})\n","a.update({\"jogging\": pd.read_csv(\"activity_clusters_df/activity_Jogging_df\")})\n","a.update({\"sitting\": pd.read_csv(\"activity_clusters_df/activity_Sitting_df\")})\n","a.update({\"standing\": pd.read_csv(\"activity_clusters_df/activity_Standing_df\")})\n","a.update({\"upstairs\": pd.read_csv(\"activity_clusters_df/activity_Upstairs_df\")})\n","a.update({\"walking\": pd.read_csv(\"activity_clusters_df/activity_Walking_df\")})\n","\n","# View shapes\n","sorter = []\n","for k in a:\n","    sorter.append([k, a[k].shape])\n","sorter = sorted(sorter, key = lambda x: x[1], reverse=True)\n","\n","# Numeric encode tasks\n","label_map = {}\n","for i, row in enumerate(sorter):\n","    label_map.update({row[0].capitalize(): i})\n","label_values = list(label_map.keys())\n","for k in a:\n","    a[k].replace({\"label\": label_map}, inplace=True)\n","\n","# Drop column \"Unnamed: 0\", was previous index\n","for k in a:\n","    a[k].drop(\"Unnamed: 0\", axis=1, inplace=True)\n","\n","# Shuffle and split datasets\n","for k in a:\n","    a[k] = a[k].sample(frac=1, random_state=42)\n","\n","    train = a[k].sample(frac=0.8, random_state=42)\n","    \n","    test = a[k].drop(train.index)\n","    val = test.sample(frac=0.5, random_state=42)\n","    test = test.drop(val.index)\n","\n","    a[k] = {\"train\": train, \"val\": val, \"test\": test}\n","\n","# Combine frequent and infrequent activites\n","data = {\"freq\": {\"train\": None, \"val\": None, \"test\": None}, \"infreq\": {\"train\": None, \"val\": None, \"test\": None}, \"comb\": {\"train\": None, \"val\": None, \"test\": None}}\n","\n","data[\"freq\"][\"train\"] = pd.concat([a[\"downstairs\"][\"train\"], a[\"jogging\"][\"train\"], a[\"upstairs\"][\"train\"], a[\"walking\"][\"train\"]], axis=0, ignore_index=True)\n","data[\"freq\"][\"val\"] = pd.concat([a[\"downstairs\"][\"val\"], a[\"jogging\"][\"val\"], a[\"upstairs\"][\"val\"], a[\"walking\"][\"val\"]], axis=0, ignore_index=True)\n","data[\"freq\"][\"test\"] = pd.concat([a[\"downstairs\"][\"test\"], a[\"jogging\"][\"test\"], a[\"upstairs\"][\"test\"], a[\"walking\"][\"test\"]], axis=0, ignore_index=True)\n","\n","data[\"infreq\"][\"train\"] = pd.concat([a[\"sitting\"][\"train\"], a[\"standing\"][\"train\"]], axis=0, ignore_index=True)\n","data[\"infreq\"][\"val\"] = pd.concat([a[\"sitting\"][\"val\"], a[\"standing\"][\"val\"]], axis=0, ignore_index=True)\n","data[\"infreq\"][\"test\"] = pd.concat([a[\"sitting\"][\"test\"], a[\"standing\"][\"test\"]], axis=0, ignore_index=True)\n","\n","data[\"comb\"][\"train\"] = pd.concat([a[\"downstairs\"][\"train\"], a[\"jogging\"][\"train\"], a[\"upstairs\"][\"train\"], a[\"walking\"][\"train\"], a[\"sitting\"][\"train\"], a[\"standing\"][\"train\"]], axis=0, ignore_index=True)\n","data[\"comb\"][\"val\"] = pd.concat([a[\"downstairs\"][\"val\"], a[\"jogging\"][\"val\"], a[\"upstairs\"][\"val\"], a[\"walking\"][\"val\"], a[\"sitting\"][\"val\"], a[\"standing\"][\"val\"]], axis=0, ignore_index=True)\n","data[\"comb\"][\"test\"] = pd.concat([a[\"downstairs\"][\"test\"], a[\"jogging\"][\"test\"], a[\"upstairs\"][\"test\"], a[\"walking\"][\"test\"], a[\"sitting\"][\"test\"], a[\"standing\"][\"test\"]], axis=0, ignore_index=True)\n","\n","# Shuffle\n","for k1 in data:\n","    for k2 in data[k1]:\n","        data[k1][k2] = data[k1][k2].sample(frac=1, random_state=42)\n","\n","# Split\n","img_rows = 27\n","img_cols = 15\n","for k1 in data:\n","    for k2 in data[k1]:\n","\n","        # Flat data\n","        x = data[k1][k2][data[k1][k2].columns[:-1]]\n","        y = data[k1][k2][[\"label\"]]\n","\n","        # Image data\n","        shaped = x.to_numpy(copy=True)\n","        shaped = np.reshape(shaped, (shaped.shape[0], img_rows, img_cols))\n","\n","        data[k1][k2] = {\"df\": data[k1][k2], \"x\": x, \"y\": y, \"x_shaped\": shaped}\n","\n","# Write to .csv files\n","dir = \"/content/drive/MyDrive/Classes/CSCE 5280 AI for Wearables/\"\n","for k1 in data:\n","    for k2 in data[k1]:\n","        for k3 in data[k1][k2]:\n","            print(k1, k2, k3)\n","            if k3 == \"x_shaped\":\n","                with open(\"%s%s_%s_%s.pickle\"%(dir, k1, k2, k3), \"wb\") as file:\n","                    pickle.dump(data[k1][k2][k3], file)\n","            else:\n","                data[k1][k2][k3].to_csv(\"%s%s_%s_%s.csv\"%(dir, k1, k2, k3), header=True, index=False)"]}]}